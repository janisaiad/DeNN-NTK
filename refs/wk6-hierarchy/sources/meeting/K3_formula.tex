\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{abstract}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=red,
    pdftitle={The Neural Tangent Kernel and its First-Order Corrections},
    pdfauthor={Research Report},
    pdfsubject={Neural Networks, NTK Theory},
    pdfkeywords={Neural Tangent Kernel, First-order corrections, ReLU networks}
}

% Page style
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\fancyfoot[C]{\footnotesize Neural Tangent Kernel Theory}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

% Title formatting
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Abstract formatting
\renewcommand{\abstractname}{Abstract}
\renewcommand{\absnamepos}{center}

\title{\LARGE\textbf{The Neural Tangent Kernel and its First-Order Corrections}\\
\vspace{0.5cm}
\large A Mathematical Analysis of Finite-Width Networks}

\author{
\textbf{Research Report}\\
\vspace{0.3cm}
\textit{Neural Network Theory Group}\\
\vspace{0.2cm}
\today
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a comprehensive mathematical analysis of the Neural Tangent Kernel (NTK) and its first-order corrections for finite-width neural networks. We provide a detailed derivation showing how the corrections $O_3$ and $O_4$ emerge from the gradient of the NTK formula with respect to network parameters. Our main contribution is a rigorous proof demonstrating that for ReLU activation functions, the backpropagation terms containing second derivatives vanish almost everywhere, leading to a natural truncation of the correction series. We establish a spectral decomposition of the NTK eigenvalues, showing that the minimal eigenvalue admits the form $\lambda_{\min}(K_\theta) \geq al + \frac{b}{N} + o(\frac{1}{N})$, where the linear term $al$ represents the infinite-width limit and $\frac{b}{N}$ captures the finite-width corrections. This analysis provides crucial insights into the convergence properties of gradient descent in finite-width neural networks.
\end{abstract}

\vspace{1cm}

\tableofcontents

\newpage


\section{Other proof}

In the framework of the Neural Tangent Hierarchy (NTH), the kernel $K^{(3)}$ arises as the operator governing the time evolution of the Neural Tangent Kernel $K^{(2)}$. The dynamics of $K^{(2)}$ under gradient flow are given by:
\begin{equation}
\frac{d}{dt} K^{(2)}_t(x_\alpha, x_\beta) = -\frac{1}{n} \sum_{\gamma=1}^n K^{(3)}_t(x_\alpha, x_\beta, x_\gamma) (f_t(x_\gamma) - y_\gamma)
\end{equation}
where $K^{(3)}_t(x_\alpha, x_\beta, x_\gamma)$ is defined as the contraction of the gradient of $K^{(2)}_t(x_\alpha, x_\beta)$ with respect to the network parameters $\theta$ with the gradient of the network output $f_t(x_\gamma)$:
\begin{equation}
K^{(3)}_t(x_\alpha, x_\beta, x_\gamma) := \langle \nabla_\theta K^{(2)}_t(x_\alpha, x_\beta), \nabla_\theta f_t(x_\gamma) \rangle.
\end{equation}

Let's derive the explicit formula for $K^{(3)}_t$. We start from the expression for $K^{(2)}_t(x_\alpha, x_\beta)$:
\begin{equation}
K^{(2)}_t(x_\alpha, x_\beta) = \langle x^{(H)}_\alpha, x^{(H)}_\beta \rangle + \sum_{\ell=1}^{H} \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle
\end{equation}
where we use the notation:
\begin{itemize}
    \item $x^{(p)}_\mu = \frac{1}{\sqrt{m}}\sigma(W^{(p)} x^{(p-1)}_\mu)$ for $p \ge 1$, and $x^{(0)}_\mu = x_\mu$.
    \item $G^{(\ell)}_\mu = \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) (W^{(\ell+1)})^T \cdots \frac{1}{\sqrt{m}}\sigma'_H(x_\mu) a_t$, with $\sigma'_\ell(x_\mu) = \text{diag}(\sigma'(W^{(\ell)}x^{(\ell-1)}_\mu))$.
\end{itemize}

Let $\delta_\gamma(\cdot) := \langle \nabla_\theta (\cdot), \nabla_\theta f_t(x_\gamma) \rangle$ denote the directional derivative along $\nabla_\theta f_t(x_\gamma)$. Then $K^{(3)}_{\alpha\beta\gamma} = \delta_\gamma(K^{(2)}_{\alpha\beta})$. Applying the product rule, we get:
\begin{align}
K^{(3)}_{\alpha\beta\gamma} = \delta_\gamma \langle x^{(H)}_\alpha, x^{(H)}_\beta \rangle + \sum_{\ell=1}^{H} \delta_\gamma \left( \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle \right)
\end{align}
Each term expands further, following $\delta_\gamma \langle A, B \rangle = \langle \delta_\gamma A, B \rangle + \langle A, \delta_\gamma B \rangle$. This results in a sum over all components of $K^{(2)}_{\alpha\beta}$, where each component is acted upon by $\delta_\gamma$. The core of the derivation lies in computing $\delta_\gamma$ for the building blocks $x^{(p)}_\mu$ and $G^{(p)}_\mu$.

The recursive formula for the derivative of the activations is:
\begin{equation}
\delta_\gamma x^{(p)}_\mu = \frac{1}{\sqrt{m}} \sigma'_{p}(x_\mu) \left( W^{(p)} (\delta_\gamma x^{(p-1)}_\mu) + \langle x^{(p-1)}_\gamma, x^{(p-1)}_\mu \rangle G^{(p)}_\gamma \right)
\end{equation}
with the base case $\delta_\gamma x^{(0)}_\mu = \delta_\gamma x_\mu = 0$.

The derivative of the backpropagated vectors $G^{(\ell)}_\mu$ is given by a sum of terms, where $\delta_\gamma$ acts on one factor at a time:
\begin{align}
\delta_\gamma G^{(\ell)}_\mu = \sum_{p=\ell}^{H} & \left( \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) \cdots \delta_\gamma(\frac{1}{\sqrt{m}}(W^{(p+1)})^T) \cdots \sigma'_H(x_\mu) a_t \right) \\
+ \sum_{p=\ell}^{H} & \left( \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) \cdots \delta_\gamma(\sigma'_{p}(x_\mu)) \cdots \sigma'_H(x_\mu) a_t \right) \\
+ & \left( \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) \cdots \sigma'_H(x_\mu) (\delta_\gamma a_t) \right)
\end{align}
where
\begin{itemize}
    \item $\delta_\gamma a_t = x^{(H)}_\gamma$
    \item The term $\delta_\gamma((W^{(p+1)})^T)$ corresponds to a rank-1 update which, when contracted in context, yields terms involving $G^{(p+1)}_\gamma$ and inner products of activations.
    \item $\delta_\gamma(\sigma'_{p}(x_\mu)) = \sigma''_{p}(x_\mu) \text{diag}(\delta_\gamma(W^{(p)}x^{(p-1)}_\mu/\sqrt{m}))$. This term vanishes for ReLU activation functions as $\sigma''=0$ almost everywhere.
\end{itemize}

Combining these rules gives the complete formula for $K^{(3)}_t(x_\alpha, x_\beta, x_\gamma)$. It is a sum of many terms, each corresponding to differentiating one part of $K^{(2)}_t(x_\alpha, x_\beta)$. For clarity, we write it as a sum of contributions:
\begin{equation}
K^{(3)}_{\alpha\beta\gamma} = K^{(3, \text{out})}_{\alpha\beta\gamma} + \sum_{\ell=1}^H \left( K^{(3, G)}_{\alpha\beta\gamma, \ell} + K^{(3, x)}_{\alpha\beta\gamma, \ell} \right)
\end{equation}
where
\begin{align}
K^{(3, \text{out})}_{\alpha\beta\gamma} &= \langle \delta_\gamma x^{(H)}_\alpha, x^{(H)}_\beta \rangle + \langle x^{(H)}_\alpha, \delta_\gamma x^{(H)}_\beta \rangle \\
K^{(3, G)}_{\alpha\beta\gamma, \ell} &= \left( \langle \delta_\gamma G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle + \langle G^{(\ell)}_\alpha, \delta_\gamma G^{(\ell)}_\beta \rangle \right) \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle \\
K^{(3, x)}_{\alpha\beta\gamma, \ell} &= \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \left( \langle \delta_\gamma x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle + \langle x^{(\ell-1)}_\alpha, \delta_\gamma x^{(\ell-1)}_\beta \rangle \right)
\end{align}
The terms $\delta_\gamma x^{(p)}_\mu$ and $\delta_\gamma G^{(p)}_\mu$ are expanded using the recursive rules above, yielding a full, albeit lengthy, expression. This hierarchical structure is characteristic of the NTH framework.


\section{Extended Formula for $K^{(3)}$}

To obtain a complete expression for $K^{(3)}_{\alpha\beta\gamma}$, we expand the terms $\delta_\gamma x^{(p)}_\mu$ and $\delta_\gamma G^{(\ell)}_\mu$ using their recursive definitions. We assume a ReLU activation function, so $\sigma''=0$ almost everywhere.

\subsection{Development of Recursive Terms}

The term $\delta_\gamma x^{(p)}_\mu$, which represents the variation of activation $x^{(p)}_\mu$ along the gradient of output $f_t(x_\gamma)$, can be expanded as a sum over previous layers:
\begin{equation}
\delta_\gamma x^{(p)}_\mu = \sum_{j=1}^{p} \left( \prod_{k=j+1}^{p} \frac{\sigma'_{k}(x_\mu) W^{(k)}}{\sqrt{m}} \right) \frac{\sigma'_{j}(x_\mu)}{\sqrt{m}} \langle x^{(j-1)}_\gamma, x^{(j-1)}_\mu \rangle G^{(j)}_\gamma
\end{equation}
where the product $\prod_{k=j+1}^{p}$ is an ordered product of Jacobian matrices.

Similarly, the term $\delta_\gamma G^{(\ell)}_\mu$, the variation of the backpropagated gradient vector, decomposes into a sum over subsequent layers:
\begin{align}
\delta_\gamma G^{(\ell)}_\mu = & \sum_{p=\ell}^{H-1} \left( \prod_{k=\ell}^{p} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\mu)}{\sqrt{m}} \right) \frac{1}{m} G^{(p+1)}_\gamma \langle x^{(p)}_\gamma, x^{(p)}_\mu \rangle \\
& + \left( \prod_{k=\ell}^{H-1} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\mu)}{\sqrt{m}} \right) \frac{x^{(H)}_\gamma}{\sqrt{m}}
\end{align}
The first term captures the effect of varying weights $W^{(\ell+1)}, \dots, W^{(H)}$, and the second term that of varying the output vector $a_t$.

\subsection{Complete Expression}

Substituting these expressions into equations (18), (19) and (20) yields the complete formula for $K^{(3)}_{\alpha\beta\gamma}$. It is a sum of $2H^2 + 2H$ terms.
\begin{align}
K^{(3)}_{\alpha\beta\gamma} = & \sum_{j=1}^{H} \left( \langle \mathcal{J}^{(H \to j)}_{\alpha} \mathcal{T}^{(j)}_{\gamma\alpha}, x^{(H)}_\beta \rangle + \langle x^{(H)}_\alpha, \mathcal{J}^{(H \to j)}_{\beta} \mathcal{T}^{(j)}_{\gamma\beta} \rangle \right) \nonumber \\
& + \sum_{\ell=1}^{H} \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle \sum_{p=\ell}^{H-1} \left( \langle \mathcal{B}^{(\ell \to p)}_{\alpha} \mathcal{U}^{(p)}_{\gamma\alpha}, G^{(\ell)}_\beta \rangle + \langle G^{(\ell)}_\alpha, \mathcal{B}^{(\ell \to p)}_{\beta} \mathcal{U}^{(p)}_{\gamma\beta} \rangle \right) \nonumber \\
& + \sum_{\ell=1}^{H} \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle \left( \langle \mathcal{B}^{(\ell \to H-1)}_{\alpha} \frac{x^{(H)}_\gamma}{\sqrt{m}}, G^{(\ell)}_\beta \rangle + \langle G^{(\ell)}_\alpha, \mathcal{B}^{(\ell \to H-1)}_{\beta} \frac{x^{(H)}_\gamma}{\sqrt{m}} \rangle \right) \nonumber \\
& + \sum_{\ell=1}^{H} \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \sum_{j=1}^{\ell-1} \left( \langle \mathcal{J}^{(\ell-1 \to j)}_{\alpha} \mathcal{T}^{(j)}_{\gamma\alpha}, x^{(\ell-1)}_\beta \rangle + \langle x^{(\ell-1)}_\alpha, \mathcal{J}^{(\ell-1 \to j)}_{\beta} \mathcal{T}^{(j)}_{\gamma\beta} \rangle \right)
\end{align}
where we have defined for readability:
\begin{itemize}
    \item Forward propagators (Jacobian): $\mathcal{J}^{(p \to j)}_{\mu} = \prod_{k=j+1}^{p} \frac{\sigma'_{k}(x_\mu) W^{(k)}}{\sqrt{m}}$
    \item Forward source terms: $\mathcal{T}^{(j)}_{\gamma\mu} = \frac{\sigma'_{j}(x_\mu)}{\sqrt{m}} \langle x^{(j-1)}_\gamma, x^{(j-1)}_\mu \rangle G^{(j)}_\gamma$
    \item Backward propagators: $\mathcal{B}^{(\ell \to p)}_{\mu} = \prod_{k=\ell}^{p} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\mu)}{\sqrt{m}}$
    \item Backward source terms: $\mathcal{U}^{(p)}_{\gamma\mu} = \frac{1}{m} G^{(p+1)}_\gamma \langle x^{(p)}_\gamma, x^{(p)}_\mu \rangle$
\end{itemize}
This formula, while complex, represents the complete hierarchical structure of the first-order correction to the NTK dynamics.

\section{Fully Expanded Non-Recursive Formula for $K^{(3)}$}

Here, we present the complete, non-recursive expression for $K^{(3)}_{\alpha\beta\gamma}$. This formula is obtained by substituting the expanded expressions for the directional derivatives $\delta_\gamma x^{(p)}_\mu$ and $\delta_\gamma G^{(\ell)}_\mu$ into the decomposed formula for $K^{(3)}$. This makes the dependency on the network parameters (weights $W^{(k)}$ and output vectors $a_t$) and activations at each layer fully explicit. We continue to assume a ReLU activation function, which sets second derivatives of $\sigma$ to zero almost everywhere.

The final expression is a sum of four main components, corresponding to the differentiation of the output layer activations, the backward vectors, and the hidden layer activations:

\begin{align}
K^{(3)}_{\alpha\beta\gamma} = & \nonumber \\
% K^(3,out) term
& \sum_{j=1}^{H} \Biggl( \left\langle \left( \prod_{k=j+1}^{H} \frac{\sigma'_{k}(x_\alpha) W^{(k)}}{\sqrt{m}} \right) \frac{\sigma'_{j}(x_\alpha)}{\sqrt{m}} \langle x^{(j-1)}_\gamma, x^{(j-1)}_\alpha \rangle G^{(j)}_\gamma, x^{(H)}_\beta \right\rangle \nonumber \\
& \quad + \left\langle x^{(H)}_\alpha, \left( \prod_{k=j+1}^{H} \frac{\sigma'_{k}(x_\beta) W^{(k)}}{\sqrt{m}} \right) \frac{\sigma'_{j}(x_\beta)}{\sqrt{m}} \langle x^{(j-1)}_\gamma, x^{(j-1)}_\beta \rangle G^{(j)}_\gamma \right\rangle \Biggr) \nonumber \\
% K^(3,G) term
& + \sum_{\ell=1}^{H} \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle \Biggl[ \nonumber \\
& \quad \sum_{p=\ell}^{H-1} \Biggl( \left\langle \left( \prod_{k=\ell}^{p} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\alpha)}{\sqrt{m}} \right) \frac{G^{(p+1)}_\gamma \langle x^{(p)}_\gamma, x^{(p)}_\alpha \rangle}{m}, G^{(\ell)}_\beta \right\rangle \nonumber \\
& \qquad + \left\langle G^{(\ell)}_\alpha, \left( \prod_{k=\ell}^{p} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\beta)}{\sqrt{m}} \right) \frac{G^{(p+1)}_\gamma \langle x^{(p)}_\gamma, x^{(p)}_\beta \rangle}{m} \right\rangle \Biggr) \nonumber \\
& \quad + \left\langle \left( \prod_{k=\ell}^{H-1} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\alpha)}{\sqrt{m}} \right) \frac{x^{(H)}_\gamma}{\sqrt{m}}, G^{(\ell)}_\beta \right\rangle \nonumber \\
& \quad + \left\langle G^{(\ell)}_\alpha, \left( \prod_{k=\ell}^{H-1} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\beta)}{\sqrt{m}} \right) \frac{x^{(H)}_\gamma}{\sqrt{m}} \right\rangle \Biggr] \nonumber \\
% K^(3,x) term
& + \sum_{\ell=1}^{H} \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \sum_{j=1}^{\ell-1} \Biggl( \left\langle \left( \prod_{k=j+1}^{\ell-1} \frac{\sigma'_{k}(x_\alpha) W^{(k)}}{\sqrt{m}} \right) \frac{\sigma'_{j}(x_\alpha)}{\sqrt{m}} \langle x^{(j-1)}_\gamma, x^{(j-1)}_\alpha \rangle G^{(j)}_\gamma, x^{(\ell-1)}_\beta \right\rangle \nonumber \\
& \quad + \left\langle x^{(\ell-1)}_\alpha, \left( \prod_{k=j+1}^{\ell-1} \frac{\sigma'_{k}(x_\beta) W^{(k)}}{\sqrt{m}} \right) \frac{\sigma'_{j}(x_\beta)}{\sqrt{m}} \langle x^{(j-1)}_\gamma, x^{(j-1)}_\beta \rangle G^{(j)}_\gamma \right\rangle \Biggr)
\end{align}
In this expression, the products $\prod$ represent ordered matrix products. The empty product $\prod_{k=p+1}^p$ is defined as the identity matrix. This detailed formula lays bare the intricate dependencies of the NTK dynamics on the network's architecture and parameters at finite width.


\newpage


\bibliographystyle{plain}
\bibliography{references}

\appendix
\section{References}

\bibitem{dyer2019asymptoticswidenetworksfeynman}
E. Dyer and G. Gur-Ari,
``Asymptotics of Wide Networks from Feynman Diagrams,''
arXiv:1909.11304 [cs.LG], 2019.

\bibitem{terjek2025mlpseocconcentrationntk}
D. Terjék and D. González-Sánchez,
``MLPs at the EOC: Concentration of the NTK,''
arXiv:2501.14724 [cs.LG], 2025.

\end{document}