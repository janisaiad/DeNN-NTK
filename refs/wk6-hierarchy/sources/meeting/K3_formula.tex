\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{abstract}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=red,
    pdftitle={The Neural Tangent Kernel and its First-Order Corrections},
    pdfauthor={Research Report},
    pdfsubject={Neural Networks, NTK Theory},
    pdfkeywords={Neural Tangent Kernel, First-order corrections, ReLU networks}
}

% Page style
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\fancyfoot[C]{\footnotesize Neural Tangent Kernel Theory}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

% Title formatting
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Abstract formatting
\renewcommand{\abstractname}{Abstract}
\renewcommand{\absnamepos}{center}

\title{\LARGE\textbf{The Neural Tangent Kernel and its First-Order Corrections}\\
\vspace{0.5cm}
\large A Mathematical Analysis of Finite-Width Networks}

\author{
\textbf{Research Report}\\
\vspace{0.3cm}
\textit{Neural Network Theory Group}\\
\vspace{0.2cm}
\today
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a comprehensive mathematical analysis of the Neural Tangent Kernel (NTK) and its first-order corrections for finite-width neural networks. We provide a detailed derivation showing how the corrections $O_3$ and $O_4$ emerge from the gradient of the NTK formula with respect to network parameters. Our main contribution is a rigorous proof demonstrating that for ReLU activation functions, the backpropagation terms containing second derivatives vanish almost everywhere, leading to a natural truncation of the correction series. We establish a spectral decomposition of the NTK eigenvalues, showing that the minimal eigenvalue admits the form $\lambda_{\min}(K_\theta) \geq al + \frac{b}{N} + o(\frac{1}{N})$, where the linear term $al$ represents the infinite-width limit and $\frac{b}{N}$ captures the finite-width corrections. This analysis provides crucial insights into the convergence properties of gradient descent in finite-width neural networks.
\end{abstract}

\vspace{1cm}

\tableofcontents

\newpage


\section{Other proof}

In the framework of the Neural Tangent Hierarchy (NTH), the kernel $K^{(3)}$ arises as the operator governing the time evolution of the Neural Tangent Kernel $K^{(2)}$. The dynamics of $K^{(2)}$ under gradient flow are given by:
\begin{equation}
\frac{d}{dt} K^{(2)}_t(x_\alpha, x_\beta) = -\frac{1}{n} \sum_{\gamma=1}^n K^{(3)}_t(x_\alpha, x_\beta, x_\gamma) (f_t(x_\gamma) - y_\gamma)
\end{equation}
where $K^{(3)}_t(x_\alpha, x_\beta, x_\gamma)$ is defined as the contraction of the gradient of $K^{(2)}_t(x_\alpha, x_\beta)$ with respect to the network parameters $\theta$ with the gradient of the network output $f_t(x_\gamma)$:
\begin{equation}
K^{(3)}_t(x_\alpha, x_\beta, x_\gamma) := \langle \nabla_\theta K^{(2)}_t(x_\alpha, x_\beta), \nabla_\theta f_t(x_\gamma) \rangle.
\end{equation}

Let's derive the explicit formula for $K^{(3)}_t$. We start from the expression for $K^{(2)}_t(x_\alpha, x_\beta)$:
\begin{equation}
K^{(2)}_t(x_\alpha, x_\beta) = \langle x^{(H)}_\alpha, x^{(H)}_\beta \rangle + \sum_{\ell=1}^{H} \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle
\end{equation}
where we use the notation:
\begin{itemize}
    \item $x^{(p)}_\mu = \frac{1}{\sqrt{m}}\sigma(W^{(p)} x^{(p-1)}_\mu)$ for $p \ge 1$, and $x^{(0)}_\mu = x_\mu$.
    \item $G^{(\ell)}_\mu = \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) (W^{(\ell+1)})^T \cdots \frac{1}{\sqrt{m}}\sigma'_H(x_\mu) a_t$, with $\sigma'_\ell(x_\mu) = \text{diag}(\sigma'(W^{(\ell)}x^{(\ell-1)}_\mu))$.
\end{itemize}

Let $\delta_\gamma(\cdot) := \langle \nabla_\theta (\cdot), \nabla_\theta f_t(x_\gamma) \rangle$ denote the directional derivative along $\nabla_\theta f_t(x_\gamma)$. Then $K^{(3)}_{\alpha\beta\gamma} = \delta_\gamma(K^{(2)}_{\alpha\beta})$. Applying the product rule, we get:
\begin{align}
K^{(3)}_{\alpha\beta\gamma} = \delta_\gamma \langle x^{(H)}_\alpha, x^{(H)}_\beta \rangle + \sum_{\ell=1}^{H} \delta_\gamma \left( \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle \right)
\end{align}
Each term expands further, following $\delta_\gamma \langle A, B \rangle = \langle \delta_\gamma A, B \rangle + \langle A, \delta_\gamma B \rangle$. This results in a sum over all components of $K^{(2)}_{\alpha\beta}$, where each component is acted upon by $\delta_\gamma$. The core of the derivation lies in computing $\delta_\gamma$ for the building blocks $x^{(p)}_\mu$ and $G^{(p)}_\mu$.

The recursive formula for the derivative of the activations is:
\begin{equation}
\delta_\gamma x^{(p)}_\mu = \frac{1}{\sqrt{m}} \sigma'_{p}(x_\mu) \left( W^{(p)} (\delta_\gamma x^{(p-1)}_\mu) + \langle x^{(p-1)}_\gamma, x^{(p-1)}_\mu \rangle G^{(p)}_\gamma \right)
\end{equation}
with the base case $\delta_\gamma x^{(0)}_\mu = \delta_\gamma x_\mu = 0$.

The derivative of the backpropagated vectors $G^{(\ell)}_\mu$ is given by a sum of terms, where $\delta_\gamma$ acts on one factor at a time:
\begin{align}
\delta_\gamma G^{(\ell)}_\mu = \sum_{p=\ell}^{H} & \left( \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) \cdots \delta_\gamma(\frac{1}{\sqrt{m}}(W^{(p+1)})^T) \cdots \sigma'_H(x_\mu) a_t \right) \\
+ \sum_{p=\ell}^{H} & \left( \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) \cdots \delta_\gamma(\sigma'_{p}(x_\mu)) \cdots \sigma'_H(x_\mu) a_t \right) \\
+ & \left( \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) \cdots \sigma'_H(x_\mu) (\delta_\gamma a_t) \right)
\end{align}
where
\begin{itemize}
    \item $\delta_\gamma a_t = x^{(H)}_\gamma$
    \item The term $\delta_\gamma((W^{(p+1)})^T)$ corresponds to a rank-1 update which, when contracted in context, yields terms involving $G^{(p+1)}_\gamma$ and inner products of activations.
    \item $\delta_\gamma(\sigma'_{p}(x_\mu)) = \sigma''_{p}(x_\mu) \text{diag}(\delta_\gamma(W^{(p)}x^{(p-1)}_\mu/\sqrt{m}))$. This term vanishes for ReLU activation functions as $\sigma''=0$ almost everywhere.
\end{itemize}

Combining these rules gives the complete formula for $K^{(3)}_t(x_\alpha, x_\beta, x_\gamma)$. It is a sum of many terms, each corresponding to differentiating one part of $K^{(2)}_t(x_\alpha, x_\beta)$. For clarity, we write it as a sum of contributions:
\begin{equation}
K^{(3)}_{\alpha\beta\gamma} = K^{(3, \text{out})}_{\alpha\beta\gamma} + \sum_{\ell=1}^H \left( K^{(3, G)}_{\alpha\beta\gamma, \ell} + K^{(3, x)}_{\alpha\beta\gamma, \ell} \right)
\end{equation}
where
\begin{align}
K^{(3, \text{out})}_{\alpha\beta\gamma} &= \langle \delta_\gamma x^{(H)}_\alpha, x^{(H)}_\beta \rangle + \langle x^{(H)}_\alpha, \delta_\gamma x^{(H)}_\beta \rangle \\
K^{(3, G)}_{\alpha\beta\gamma, \ell} &= \left( \langle \delta_\gamma G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle + \langle G^{(\ell)}_\alpha, \delta_\gamma G^{(\ell)}_\beta \rangle \right) \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle \\
K^{(3, x)}_{\alpha\beta\gamma, \ell} &= \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \left( \langle \delta_\gamma x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle + \langle x^{(\ell-1)}_\alpha, \delta_\gamma x^{(\ell-1)}_\beta \rangle \right)
\end{align}
The terms $\delta_\gamma x^{(p)}_\mu$ and $\delta_\gamma G^{(p)}_\mu$ are expanded using the recursive rules above, yielding a full, albeit lengthy, expression. This hierarchical structure is characteristic of the NTH framework.

\newpage


\bibliographystyle{plain}
\bibliography{references}

\appendix
\section{References}

\bibitem{dyer2019asymptoticswidenetworksfeynman}
E. Dyer and G. Gur-Ari,
``Asymptotics of Wide Networks from Feynman Diagrams,''
arXiv:1909.11304 [cs.LG], 2019.

\bibitem{terjek2025mlpseocconcentrationntk}
D. Terjék and D. González-Sánchez,
``MLPs at the EOC: Concentration of the NTK,''
arXiv:2501.14724 [cs.LG], 2025.

\end{document}
