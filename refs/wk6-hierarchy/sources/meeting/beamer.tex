\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{lmodern} % For better font rendering

\usetheme{Antibes}
\usecolortheme{default}

\title{Analysis of the Neural Tangent Hierarchy and the First-Order Correction Kernel $K^{(3)}$}
\subtitle{Theory, Scaling, and Experiments}
\author{Janis Aiazi}
\institute{UMD, Ecole Polytechnique}
\date{\today}

% Math commands from report
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\evmin}[1]{\lambda_{\min}\left(#1\right)}
\newcommand{\evmax}[1]{\lambda_{\max}\left(#1\right)}
\newcommand{\svmin}[1]{\sigma_{\min}\left(#1\right)}
\newcommand{\tr}{\text{tr}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\Ktwo}{K^{(2)}}
\newcommand{\Kthree}{K^{(3)}}
\newcommand{\Order}{\mathcal{O}}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\section{Introduction: Beyond Infinite Width}

\begin{frame}{The Success and Limits of the NTK}
\textbf{The Neural Tangent Kernel (NTK):}
\begin{itemize}
    \item A major breakthrough for understanding training dynamics of neural networks.
    \item In the infinite-width limit, the NTK ($K^{(2)}$) becomes a fixed kernel.
    \item Training dynamics simplify to kernel regression, a convex optimization problem.
\end{itemize}
\vspace{1cm}
\textbf{The Finite-Width Gap:}
\begin{itemize}
    \item Real-world, finite-width networks often outperform their infinite-width counterparts.
    \item This suggests that finite-width effects, like the \textit{evolution of the NTK during training}, are crucial for performance.
\end{itemize}
\end{frame}

\begin{frame}{Our Goal: Understanding the First Correction}
To study finite-width effects, we turn to the \textbf{Neural Tangent Hierarchy (NTH)}.
\begin{itemize}
    \item The NTH describes dynamics as a hierarchy of coupled ODEs.
    \item The evolution of the network output $f$ is governed by $K^{(2)}$.
    \item The evolution of $K^{(2)}$ is governed by a third-order kernel, $K^{(3)}$.
\end{itemize}
\vspace{1cm}
\textbf{In this presentation, we will:}
\begin{enumerate}
    \item Formally derive the expression for the first-order correction kernel, $K^{(3)}$.
    \item Analyze how its magnitude scales with network depth ($H$) and width ($m$).
    \item Present numerical experiments that validate our theoretical findings.
\end{enumerate}
\end{frame}

\begin{frame}{The NTK Spectrum at Infinite Width}
\textbf{The spectrum of the infinite-width NTK has a distinct structure:}
\begin{itemize}
    \item There is one large, isolated eigenvalue that remains constant with depth.
    \item The rest of the eigenvalues form a "bulk" that scales linearly with depth $L$ (as $\sim 0.75 L/N$).
    \item This linear scaling of the small eigenvalues is what amplifies the finite-width corrections in deep networks.
\end{itemize}

\begin{columns}[T] % Aligns top of columns
\column{0.5\textwidth}
    \includegraphics[width=\textwidth]{largest_eigenvalue_vs_L_infinite.png}
    \captionof{figure}{\tiny Largest eigenvalue vs. Depth L.}

\column{0.5\textwidth}
    \includegraphics[width=\textwidth]{kth_eigenvalue_vs_L_infinite.png}
    \captionof{figure}{\tiny Smallest eigenvalues vs. Depth L.}
\end{columns}

\end{frame}

\section{The Neural Tangent Hierarchy}

\begin{frame}{From NTK to the Hierarchy}
The dynamics of the network output $f_\alpha(t) := f(x_\alpha, \theta_t)$ are given by:
\[
\frac{d f_\alpha(t)}{dt} = -\frac{1}{n} \sum_{\beta=1}^n K^{(2)}_t(x_\alpha, x_\beta) (f_\beta(t) - y_\beta)
\]
\begin{definition}[Neural Tangent Kernel]
$K^{(2)}_t(x_\alpha, x_\beta) := \langle \nabla_\theta f_t(x_\alpha), \nabla_\theta f_t(x_\beta) \rangle$
\end{definition}
\vspace{0.5cm}

\begin{definition}[Third-Order Kernel]
$K^{(3)}_t(x_\alpha, x_\beta, x_\gamma) := \langle \nabla_\theta K^{(2)}_t(x_\alpha, x_\beta), \nabla_\theta f_t(x_\gamma) \rangle$
\end{definition}
This defines an infinite hierarchy where the dynamics of $K^{(r)}$ are governed by $K^{(r+1)}$.
\end{frame}

\section{Derivation of the $K^{(3)}$ Formula}

\begin{frame}{$K^{(3)}$: Structure and Components}
Let $\delta_\gamma(\cdot) := \langle \nabla_\theta (\cdot), \nabla_\theta f_t(x_\gamma) \rangle$. Then $K^{(3)}_{\alpha\beta\gamma} = \delta_\gamma(K^{(2)}_{\alpha\beta})$.
\vspace{0.2cm}
The NTK for an $H$-layer MLP has the form:
\[
K^{(2)}_{\alpha\beta} = \langle x^{(H)}_\alpha, x^{(H)}_\beta \rangle + \sum_{\ell=1}^{H} \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle
\]
Applying the product rule for $\delta_\gamma$ yields a decomposition into four main parts:
\begin{align*}
K^{(3)}_{\alpha\beta\gamma} = K^{(3,\text{out})} + K^{(3,G,W)} + K^{(3,G,a)} + K^{(3,x)}
\end{align*}
\begin{itemize}
    \item $K^{(3,\text{out})}$: from derivatives of final layer activations $x^{(H)}$.
    \item $K^{(3,G,W)}$: from derivatives of backward vectors $G^{(\ell)}$ w.r.t. weights $W$.
    \item $K^{(3,G,a)}$: from derivatives of $G^{(\ell)}$ w.r.t. output weights $a$.
    \item $K^{(3,x)}$: from derivatives of hidden layer activations $x^{(\ell-1)}$.
\end{itemize}
\end{frame}

\begin{frame}{Recursive Derivatives}
\begin{proposition}[Recursive Derivatives]
The core of the derivation is to compute the directional derivatives of the activations and backward vectors. They follow these recursions:
\begin{itemize}
    \item \textbf{Forward derivative $\delta_\gamma x^{(p)}$:}
    \[
    \delta_\gamma x^{(p)}_\mu = \frac{\sigma'_{p}}{\sqrt{m}} \left( W^{(p)} (\delta_\gamma x^{(p-1)}_\mu) + \langle x^{(p-1)}_\gamma, x^{(p-1)}_\mu \rangle G^{(p)}_\gamma \right)
    \]
    This recursion accumulates terms at each layer.
    
    \item \textbf{Backward derivative $\delta_\gamma G^{(\ell)}$:}
    \[
    \delta_\gamma G^{(\ell)}_\mu = \text{sum of terms from replacing } W^{(p+1)} \text{ and } a_t
    \]
    This involves summing contributions from all subsequent layers $p > \ell$.
\end{itemize}
\end{proposition}
\end{frame}

\begin{frame}{The Final Formula}
\begin{theorem}[Fully Expanded Formula for $K^{(3)}$ with ReLU]
By unrolling the recursions, we obtain a complete, non-recursive formula for $K^{(3)}$. The full expression is complex, but it is a sum of the four components identified earlier:
\begin{align*}
K^{(3)}_{\alpha\beta\gamma} = K^{(3,\text{out})}_{\alpha\beta\gamma} + K^{(3,G,W)}_{\alpha\beta\gamma} + K^{(3,G,a)}_{\alpha\beta\gamma} + K^{(3,x)}_{\alpha\beta\gamma}
\end{align*}
Each component is an explicit (but lengthy) sum over layers involving activations, backward vectors, and their inner products.
\end{theorem}
\end{frame}

\begin{frame}{$K^{(3)}$: Structure and Components}
\begin{columns}
\column{0.5\textwidth}
\textbf{1. Why linear scaling with L?}
\begin{itemize}
    \item Our experiments showed that the $K^{(3)} \equiv O_3$ kernel's norm diminishes with width.
    \item Therefore, the $O_3$ term's contribution is suppressed.
    \item The observed linear growth of the correction with depth $L$ must be primarily driven by the \textbf{$O_4$ term}.
    \item \textit{Note: The exact scaling of the $O_3$ term is complex and under active experimental investigation.}
\end{itemize}

\column{0.5\textwidth}
\textbf{2. Why scaling with L and N?}
\begin{itemize}
    \item The correction is amplified by small eigenvalues ($\lambda_i$), which we've just seen scale as $\sim L/N$.
    \item A larger $L$ or $N$ means a smaller $\lambda_{\min}$, which \textbf{boosts the correction}, explaining the scaling.
\end{itemize}
\end{columns}
\end{frame}

\section{Experimental Validation}

\begin{frame}{Experimental Setup}
\textbf{Goal:} Empirically compute $K^{(3)}$ and verify the scaling law.
\begin{itemize}
    \item \textbf{Architecture:} MLP with ReLU, width $M=100$.
    \item \textbf{Data:} $N=8$ random, normalized data points in $D_{in}=20$.
    \item \textbf{Variable:} We vary the network depth $L$ (same as $H$).
    \item \textbf{Methodology:}
        \begin{enumerate}
            \item Initialize network for a given depth $L$.
            \item Perform forward pass, storing all activations and derivatives.
            \item Compute the full $K^{(3)}$ tensor using the derived formulas.
            \item Calculate its infinity norm $\|K^{(3)}\|_\infty = \max |K^{(3)}_{\alpha\beta\gamma}|$.
        \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}{Results and Discussion}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../../../experiments/plots/k3_inf_norm_vs_L_finite_M100_N8_D20.png}
    \caption{Infinity norm of empirical $K^{(3)}$ vs. network depth $L$ (M=100).}
    \label{fig:k3_norm_vs_l}
\end{figure}
\begin{itemize}
    \item The plot clearly shows that the magnitude of $K^{(3)}$ \textbf{grows} with depth.
    \item The trend is strongly consistent with our theoretical prediction $K^{(3)} \sim \Order(L^2/\sqrt{m})$.
    \item This validates that finite-width effects become more pronounced in deeper networks.
\end{itemize}
\end{frame}

\section{Deeper Dive: NTK Correction Scaling}

\begin{frame}{Precise Scaling of the NTK Correction}
A multivariate regression in log-space ($R^2=0.994$), confirmed by extensive individual experiments, on the spectral radius of the correction $\|K - K_\infty\|$ yields a precise scaling law:

\begin{alertblock}{Empirical Scaling Law}
\[
\|K - K_\infty\| \propto L^{1.171} \cdot D_{\text{in}}^{0.047} \cdot N^{0.900} \cdot M^{-1.009}
\]
\end{alertblock}

\textbf{Interpretation:}
\begin{itemize}
    \item \textbf{Depth ($L$):} Super-linear growth.
    \item \textbf{Data Size ($N$):} Near-linear growth.
    \item \textbf{Input Dimension ($D_{in}$):} The dependence remains bounded and small. This is great for our setup in PDE solving in high dimension.
    \item \textbf{Width ($M$):} Decays as $\sim 1/M$, as expected from theory.
\end{itemize}

\vfill
\tiny{\textit{Disclaimer: These results are based on extensive computations ($\sim$100h+) and are fully reproducible via the provided codebase.}}

\end{frame}

\begin{frame}{Theoretical Interpretation: A Deeper Look}
The formula for the late-time NTK correction, $\Theta^{(1)}_\infty$, connects our findings.
\begin{theorem}[Late-Time Correction \cite{large-width-feynman}]
\begin{align*}
\Theta^{(1)}_\infty = & -\sum_{i}\frac{1}{\lambda_{i}}(O_{3}(\vec{x};0)\cdot\hat{e}_{i})(\Delta f_{0}\cdot\hat{e}_{i}) \\
& + \sum_{i,j}\frac{1}{\lambda_{i}(\lambda_{i}+\lambda_{j})}(\hat{e}_{i}^{T}O_{4}(\vec{x};0)\hat{e}_{j})(\Delta f_{0}\cdot \hat{e}_{i})(\Delta f_{0}\cdot \hat{e}_{j})
\end{align*}
\end{theorem}

\begin{columns}
\column{0.5\textwidth}
\textbf{1. Why linear scaling with L?}
\begin{itemize}
    \item Our experiments showed that the $K^{(3)} \equiv O_3$ kernel's norm diminishes with width.
    \item Therefore, the $O_3$ term's contribution is suppressed.
    \item The observed linear growth of the correction with depth $L$ must be primarily driven by the \textbf{$O_4$ term}.
    \item \textit{Note: The exact scaling of the $O_3$ term is complex and under active experimental investigation.}
\end{itemize}

\column{0.5\textwidth}
\textbf{2. Why scaling with L and N?}
\begin{itemize}
    \item The correction is amplified by small eigenvalues ($\lambda_i$), which we've just seen scale as $\sim L/N$.
    \item A larger $L$ or $N$ means a smaller $\lambda_{\min}$, which \textbf{boosts the correction}, explaining the scaling.
\end{itemize}
\end{columns}

\end{frame}

\section{Future Work}
\begin{frame}{Future Work}
\begin{itemize}
    \item \textbf{NTH}: Compute $\lambda_H$ and verify its linear scaling.
    \item \textbf{Feynman}: Perform eigendecomposition of the $O_4$ matrix. Implement the full framework if not found in literature.
    \item \textbf{Concentration of the NTK}: Investigate the parabola shape and other emerging patterns.
    \item \textbf{Numerics on Manifolds}: Extend experiments to the torus and sphere (using Kronecker decomposition / spherical harmonics) and analyze dependency on input dimension.
    \item \textbf{NTK in Narrow Networks}: Study NTK for narrow networks with varying initializations, architectures (e.g., skip connections/ResNets).
\end{itemize}
\end{frame}

\section{Conclusion}

\begin{frame}{Conclusion}
\textbf{Summary:}
\begin{itemize}
    \item We moved beyond the infinite-width NTK by analyzing the NTH.
    \item We derived a complete formula for the first-order correction kernel, $K^{(3)}$.
    \item We showed theoretically and empirically that its magnitude scales as $\Order(H^2/\sqrt{m})$.
\end{itemize}
\vspace{1cm}
\textbf{Key Implication:}
\begin{alertblock}
The training dynamics of deep networks are inherently richer than shallow ones. The learning geometry itself (the NTK) evolves more significantly, meaning deep networks are less "lazy". This provides a theoretical basis for their powerful feature learning capabilities.
\end{alertblock}
\end{frame}

\section{Scaling Analysis of $K^{(3)}$}
Warning : This part is unsure now because I'm doing the calculations by hand on my own.
\begin{frame}{Scaling of Core Components (Not sure of the validity)}
\begin{itemize}
    \item \textbf{Activations and G-vectors:}
    With proper initialization (dynamical isometry), norms are stable across layers:
    \[ \|x^{(p)}_\mu\| \sim \Order(1) \quad \text{and} \quad \|G^{(\ell)}_\mu\| \sim \Order(1) \]
    
    \item \textbf{Numerical Challenge: Lyapunov Exponents}
    For any \textit{finite} width, the norm of products of random matrices is governed by Lyapunov exponents. Disentangling potential small exponential trends from the polynomial scaling we want to measure is hard.
    
    \item \textbf{Derivative Terms:}
    We could imagine by making some simplifying assumptions:
    \begin{align*}
    \|\delta_\gamma x^{(p)}_\mu\| &\sim \Order\left(\frac{p}{\sqrt{m}}\right) \quad (\text{linear growth with layer } p) \\
    \|\delta_\gamma G^{(\ell)}_\mu\| &\sim \Order\left(\frac{H-\ell}{m}\right) \quad (\text{grows with number of subsequent layers})
    \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Overall Scaling of $K^{(3)}$}
We combine the scaling of the components to find the scaling of the full tensor.

\begin{columns}
\column{0.6\textwidth}
\textbf{Term-by-term analysis when H is large and M bounded:}
\begin{itemize}
    \item $K^{(3, \text{out})} \sim \Order(H/\sqrt{m})$
    \item $K^{(3, G, W)} \sim \Order(H^2/m)$
    \item $K^{(3, G, a)} \sim \Order(H/\sqrt{m})$
    \item $K^{(3, x)} \sim \Order(H^2/\sqrt{m})$
\end{itemize}
\column{0.4\textwidth}
For $H \ll \sqrt{m}$, the dominant term is the one with the slowest decay in $m$ and fastest growth in $H$.
\end{columns}

\begin{alertblock}{Dominant Term}
The forward term $K^{(3,x)}$ dominates the scaling.
\end{alertblock}

\begin{theorem}[Scaling of $K^{(3)}$]
The magnitude of the entries of the third-order kernel $K^{(3)}$ scales as:
\begin{equation}
K^{(3)}_{\alpha\beta\gamma} \sim \Order\left(\frac{H^2}{\sqrt{m}}\right)
\end{equation*}
\end{theorem}
\end{frame}
\end{document}
