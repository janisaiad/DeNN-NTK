\documentclass{article}


\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{tocloft}
\usepackage{tikz}

% Page setup
\geometry{a4paper, margin=2.5cm}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}

% Mathematical commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\evmin}[1]{\lambda_{\min}\left(#1\right)}
\newcommand{\evmax}[1]{\lambda_{\max}\left(#1\right)}
\newcommand{\svmin}[1]{\sigma_{\min}\left(#1\right)}
\newcommand{\tr}{\text{tr}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\Ktwo}{K^{(2)}}
\newcommand{\Kthree}{K^{(3)}}
\newcommand{\Order}{\mathcal{O}}


\title{Analysis of the Neural Tangent Hierarchy and the First-Order Correction Kernel $K^{(3)}$}

\author{Janis AIAD}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report provides a detailed analysis of the Neural Tangent Hierarchy (NTH), a framework designed to study the training dynamics of finite-width neural networks beyond the infinite-width approximation. While the standard Neural Tangent Kernel (NTK) provides a powerful description of infinitely wide networks, it remains constant during training, failing to capture the full learning dynamics observed in practice. The NTH addresses this by introducing a hierarchy of higher-order kernels that describe the evolution of the NTK. We focus on the first-order correction, encapsulated by the $K^{(3)}$ kernel, which governs the dynamics of the NTK itself. We present a complete derivation of the formula for $K^{(3)}$ and conduct a scaling analysis of its magnitude with respect to network depth ($H$) and width ($m$). Our key finding is that the $K^{(3)}$ kernel's magnitude scales as $\mathcal{O}(H^2/\sqrt{m})$, indicating that finite-width effects become more pronounced in deeper networks. This provides a theoretical basis for understanding why deeper networks can learn features beyond the fixed infinite-width kernel.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

Deep neural networks have achieved remarkable success across various domains, yet their theoretical understanding, particularly concerning their training dynamics, remains an active area of research. A significant breakthrough in this direction was the introduction of the Neural Tangent Kernel (NTK) by Jacot et al. \cite{jacot2018neural}. The NTK describes the evolution of a neural network's output during training via gradient descent. In the limit of infinite network width, the NTK converges to a deterministic kernel that remains constant throughout training. This reduces the complex, non-convex optimization of a neural network to a simple linear system, equivalent to kernel regression with the limiting NTK.

However, a notable performance gap exists between real-world, finite-width networks and their infinite-width kernel regression counterparts \cite{arora2019exact}. This gap suggests that the finite-width effects, particularly the evolution of the NTK during training, play a crucial role in the learning process. The Neural Tangent Hierarchy (NTH), introduced by Huang \& Yau \cite{huang2019dynamics}, provides a systematic framework for studying these finite-width corrections.

The NTH describes the training dynamics as an infinite hierarchy of coupled ordinary differential equations (ODEs). The evolution of the network's output is governed by the NTK (denoted $K^{(2)}$), the evolution of $K^{(2)}$ is governed by a third-order kernel ($K^{(3)}$), and so on. This hierarchy provides a powerful tool to move beyond the static, infinite-width picture.

This report focuses on a detailed analysis of the first and most significant finite-width correction, which is captured by the $K^{(3)}$ kernel. Our objective is twofold:
\begin{enumerate}
    \item To provide a rigorous and explicit derivation of the formula for the $K^{(3)}$ kernel.
    \item To analyze the scaling behavior of $K^{(3)}$ with respect to the network's depth ($H$) and width ($m$).
\end{enumerate}
By understanding $K^{(3)}$, we can gain fundamental insights into how the geometry of the learning problem, as captured by the NTK, evolves during training in finite-width networks.

\section{The Neural Tangent Hierarchy}

We consider a fully-connected feedforward neural network of depth $H$ and uniform width $m$. The network function $f(x, \theta)$ is trained using gradient descent on a quadratic loss function. The training dynamics are described by the gradient flow:
\begin{equation}
\frac{d\theta_t}{dt} = -\nabla_\theta L(\theta_t) = -\frac{1}{n} \sum_{\beta=1}^n (f(x_\beta, \theta_t) - y_\beta) \nabla_\theta f(x_\beta, \theta_t)
\end{equation}

\subsection{The Neural Tangent Kernel ($K^{(2)}$)}
The evolution of the function output $f_\alpha(t) := f(x_\alpha, \theta_t)$ can be expressed using the chain rule:
\begin{equation}
\frac{d f_\alpha(t)}{dt} = \langle \nabla_\theta f_\alpha(t), \frac{d\theta_t}{dt} \rangle = -\frac{1}{n} \sum_{\beta=1}^n \langle \nabla_\theta f_\alpha(t), \nabla_\theta f_\beta(t) \rangle (f_\beta(t) - y_\beta)
\end{equation}
This motivates the definition of the Neural Tangent Kernel (NTK), $K^{(2)}$:
\begin{definition}[Neural Tangent Kernel]
The NTK at time $t$ is a matrix kernel defined for any pair of inputs $(x_\alpha, x_\beta)$ as the inner product of their output gradients with respect to the network parameters $\theta_t$:
\begin{equation}
K^{(2)}_t(x_\alpha, x_\beta) := \langle \nabla_\theta f_t(x_\alpha), \nabla_\theta f_t(x_\beta) \rangle
\end{equation}
\end{definition}
With this definition, the dynamics of the network output become:
\begin{equation}
\frac{d f_\alpha(t)}{dt} = -\frac{1}{n} \sum_{\beta=1}^n K^{(2)}_t(x_\alpha, x_\beta) (f_\beta(t) - y_\beta)
\end{equation}
In the infinite-width limit ($m \to \infty$), $K^{(2)}_t$ converges to a fixed kernel $K^{(2)}_\infty$ and does not change during training.

\subsubsection{Spectral Properties of the Infinite-Width NTK}
The spectrum of the infinite-width kernel $\Ktwo_\infty$ exhibits a distinct structure that is crucial for understanding training dynamics. It typically contains one large, isolated eigenvalue that remains relatively constant with network depth (Figure \ref{fig:largest_eigenvalue}), while the remaining eigenvalues form a "bulk". These bulk eigenvalues, including the smallest one $\lambda_{\min}$, scale linearly with depth $L$ and inversely with data size $N$, following a trend of $\Order(L/N)$. Our experiments confirm this, revealing a scaling of approximately $0.75 L/N$ (Figure \ref{fig:k_eigenvalue}).

Based on our experimental findings and theoretical considerations such as Weyl's inequality, which can be applied to the decomposition $K_{\text{emp}} \approx K_\infty + \frac{1}{M}\Theta^{(1)}$, we can conjecture a more precise lower bound for the smallest eigenvalue that accounts for the finite-width correction:
\begin{equation}
\lambda_{\min}(K_{\text{emp}}) \ge \frac{0.75L}{N} - C \frac{NL}{M}
\label{eq:lambda_min_conjecture}
\end{equation}
where $C$ is a constant. This conjectural scaling law highlights how the finite-width term can decrease the smallest eigenvalue, further amplifying the correction terms discussed in Section 2.2.1.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{largest_eigenvalue_vs_L_infinite.png}
    \caption{The largest eigenvalue of the infinite-width NTK remains approximately constant with depth $L$.}
    \label{fig:largest_eigenvalue}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{kth_eigenvalue_vs_L_infinite.png}
    \caption{The $k$-th smallest eigenvalues of the infinite-width NTK scale linearly with depth $L$.}
    \label{fig:k_eigenvalue}
\end{figure}

\subsection{The Hierarchy of Kernels}
At finite width, $K^{(2)}_t$ evolves. The Neural Tangent Hierarchy extends the dynamics by considering the time derivative of the kernels themselves. The evolution of $K^{(2)}_t$ is given by:
\begin{equation}
\frac{d K^{(2)}_t(x_\alpha, x_\beta)}{dt} = \langle \nabla_\theta K^{(2)}_t(x_\alpha, x_\beta), \frac{d\theta_t}{dt} \rangle = -\frac{1}{n} \sum_{\gamma=1}^n \langle \nabla_\theta K^{(2)}_t(x_\alpha, x_\beta), \nabla_\theta f_t(x_\gamma) \rangle (f_\gamma(t) - y_\gamma)
\end{equation}
This leads to the definition of the third-order kernel, $K^{(3)}$.

\begin{definition}[Third-Order Kernel]
The $K^{(3)}$ kernel is a third-order tensor defined as:
\begin{equation}
K^{(3)}_t(x_\alpha, x_\beta, x_\gamma) := \langle \nabla_\theta K^{(2)}_t(x_\alpha, x_\beta), \nabla_\theta f_t(x_\gamma) \rangle
\end{equation}
\end{definition}

This process can be continued indefinitely, defining a hierarchy of kernels where the dynamics of $K^{(r)}$ are governed by $K^{(r+1)}$:
\begin{equation}
\frac{dK_t^{(r)}(x_1, \dots, x_r)}{dt} = -\frac{1}{n} \sum_{\beta=1}^n K^{(r+1)}_t(x_1, \dots, x_r, x_\beta)(f_\beta(t)-y_\beta)
\end{equation}
This infinite set of coupled ODEs is the Neural Tangent Hierarchy. For wide networks, the higher-order kernels are suppressed by powers of the width $m$, allowing the hierarchy to be truncated for analysis. To understand the impact of these dynamics on the final learned function, our goal is to understand the full correction to the NTK.

\subsubsection{The Late-Time NTK Correction}
A key quantity to analyze is the total change in the kernel after training has converged, known as the late-time NTK correction, $\Theta^{(1)}_\infty$. This quantity, derived in \cite{large-width-feynman}, captures the leading-order change in the kernel and provides a direct link between the higher-order tensors ($O_3 \equiv K^{(3)}$, $O_4$, etc.) and the learned features.

\begin{theorem}[Late-Time NTK Correction \cite{large-width-feynman}]
The first-order correction to the NTK at late times is given by:
\begin{align}
\Theta^{(1)}_\infty(\vec{x}) = & -\sum_{i}\frac{1}{\lambda_{i}}(O_{3}(\vec{x};0)\cdot\hat{e}_{i})(\Delta f_{0}\cdot\hat{e}_{i}) \nonumber \\
& + \sum_{i,j}\frac{1}{\lambda_{i}(\lambda_{i}+\lambda_{j})}(\hat{e}_{i}^{T}O_{4}(\vec{x};0)\hat{e}_{j})(\Delta f_{0}\cdot \hat{e}_{i})(\Delta f_{0}\cdot \hat{e}_{j})
\label{eq:theta_correction}
\end{align}
where $\lambda_i$ and $\hat{e}_i$ are the eigenvalues and eigenvectors of the initial kernel $\Theta_0$.
\end{theorem}

This formula shows that to understand the full correction, we must first understand the structure and scaling of its components, starting with the $K^{(3)}$ (or $O_3$) kernel. The following sections are dedicated to its detailed analysis.

\section{Derivation of the $K^{(3)}$ Kernel Formula}

We now provide an explicit derivation for $K^{(3)}_t$. Let $\delta_\gamma(\cdot) := \langle \nabla_\theta (\cdot), \nabla_\theta f_t(x_\gamma) \rangle$ be the directional derivative operator along the gradient of the output $f_t(x_\gamma)$. Then $K^{(3)}_{\alpha\beta\gamma} = \delta_\gamma(K^{(2)}_{\alpha\beta})$.

The NTK for an $H$-layer MLP is given by:
\begin{equation}
K^{(2)}_t(x_\alpha, x_\beta) = \langle x^{(H)}_\alpha, x^{(H)}_\beta \rangle + \sum_{\ell=1}^{H} \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle
\end{equation}
where:
\begin{itemize}
    \item $x^{(p)}_\mu = \frac{1}{\sqrt{m}}\sigma(W^{(p)} x^{(p-1)}_\mu)$ are the forward-propagated activations.
    \item $G^{(\ell)}_\mu = \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) (W^{(\ell+1)})^T \cdots \frac{1}{\sqrt{m}}\sigma'_H(x_\mu) a_t$ are the backward-propagated gradient vectors.
\end{itemize}

Applying the product rule for the $\delta_\gamma$ operator, $K^{(3)}$ decomposes into terms where $\delta_\gamma$ acts on each component of $K^{(2)}$.
\begin{equation}
K^{(3)}_{\alpha\beta\gamma} = \underbrace{\left(\langle \delta_\gamma x^{(H)}_\alpha, x^{(H)}_\beta \rangle + \langle x^{(H)}_\alpha, \delta_\gamma x^{(H)}_\beta \rangle\right)}_{K^{(3, \text{out})}} + \sum_{\ell=1}^H \left( \underbrace{\delta_\gamma\langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle}_{K^{(3, G), \ell}} + \underbrace{\langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \delta_\gamma\langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle}_{K^{(3, x), \ell}} \right)
\end{equation}
The core of the derivation is to find expressions for $\delta_\gamma x^{(p)}_\mu$ and $\delta_\gamma G^{(\ell)}_\mu$.

\begin{proposition}[Recursive Derivatives]
The directional derivatives of the activations and backward vectors are given by the following recursions:
\begin{equation}
\delta_\gamma x^{(p)}_\mu = \frac{1}{\sqrt{m}} \sigma'_{p}(x_\mu) \left( W^{(p)} (\delta_\gamma x^{(p-1)}_\mu) + \langle x^{(p-1)}_\gamma, x^{(p-1)}_\mu \rangle G^{(p)}_\gamma \right)
\end{equation}
with base case $\delta_\gamma x^{(0)}_\mu = 0$.
\begin{align}
\delta_\gamma G^{(\ell)}_\mu = & \sum_{p=\ell}^{H} \left( \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) \cdots \delta_\gamma(\frac{1}{\sqrt{m}}(W^{(p+1)})^T) \cdots \sigma'_H(x_\mu) a_t \right) \nonumber \\
& + \sum_{p=\ell}^{H} \left( \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) \cdots \delta_\gamma(\sigma'_{p}(x_\mu)) \cdots \sigma'_H(x_\mu) a_t \right) \nonumber \\
& + \left( \frac{1}{\sqrt{m}}\sigma'_\ell(x_\mu) \cdots \sigma'_H(x_\mu) (\delta_\gamma a_t) \right)
\end{align}
\end{proposition}

\begin{remark}[Simplification for ReLU Networks]
For ReLU activation, $\sigma'(s) = \mathbb{I}(s>0)$ and $\sigma''(s) = \delta(s)$. Since $\delta_\gamma(\sigma'_{p}(x_\mu))$ involves $\sigma''_{p}$, this term vanishes almost everywhere. This significantly simplifies the expression for $\delta_\gamma G^{(\ell)}_\mu$ and, consequently, for $K^{(3)}$.
\end{remark}

By expanding these recursive formulas, we arrive at the complete, non-recursive expression for $K^{(3)}$.

\begin{theorem}[Fully Expanded Formula for $K^{(3)}$ with ReLU]
For a network with ReLU activation, the $K^{(3)}$ kernel is given by the sum of four main components:
\begin{align}
K^{(3)}_{\alpha\beta\gamma} = K^{(3,\text{out})}_{\alpha\beta\gamma} + K^{(3,G,W)}_{\alpha\beta\gamma} + K^{(3,G,a)}_{\alpha\beta\gamma} + K^{(3,x)}_{\alpha\beta\gamma}
\end{align}
where:
\begin{align}
% K^(3,out) term
K^{(3,\text{out})}_{\alpha\beta\gamma} = & \sum_{j=1}^{H} \Biggl( \left\langle \left( \prod_{k=j+1}^{H} \frac{\sigma'_{k}(x_\alpha) W^{(k)}}{\sqrt{m}} \right) \frac{\sigma'_{j}(x_\alpha)}{\sqrt{m}} \langle x^{(j-1)}_\gamma, x^{(j-1)}_\alpha \rangle G^{(j)}_\gamma, x^{(H)}_\beta \right\rangle \nonumber \\
& \quad + \left\langle x^{(H)}_\alpha, \left( \prod_{k=j+1}^{H} \frac{\sigma'_{k}(x_\beta) W^{(k)}}{\sqrt{m}} \right) \frac{\sigma'_{j}(x_\beta)}{\sqrt{m}} \langle x^{(j-1)}_\gamma, x^{(j-1)}_\beta \rangle G^{(j)}_\gamma \right\rangle \Biggr) \\
% K^(3,G,W) term
K^{(3,G,W)}_{\alpha\beta\gamma} = & \sum_{\ell=1}^{H} \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle \sum_{p=\ell}^{H-1} \Biggl( \left\langle \left( \prod_{k=\ell}^{p} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\alpha)}{\sqrt{m}} \right) \frac{G^{(p+1)}_\gamma \langle x^{(p)}_\gamma, x^{(p)}_\alpha \rangle}{m}, G^{(\ell)}_\beta \right\rangle \nonumber \\
& \qquad + \left\langle G^{(\ell)}_\alpha, \left( \prod_{k=\ell}^{p} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\beta)}{\sqrt{m}} \right) \frac{G^{(p+1)}_\gamma \langle x^{(p)}_\gamma, x^{(p)}_\beta \rangle}{m} \right\rangle \Biggr) \\
% K^(3,G,a) term
K^{(3,G,a)}_{\alpha\beta\gamma} = & \sum_{\ell=1}^{H} \langle x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle \Biggl( \left\langle \left( \prod_{k=\ell}^{H-1} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\alpha)}{\sqrt{m}} \right) \frac{x^{(H)}_\gamma}{\sqrt{m}}, G^{(\ell)}_\beta \right\rangle \nonumber \\
& \quad + \left\langle G^{(\ell)}_\alpha, \left( \prod_{k=\ell}^{H-1} \frac{(W^{(k+1)})^T \sigma'_{k+1}(x_\beta)}{\sqrt{m}} \right) \frac{x^{(H)}_\gamma}{\sqrt{m}} \right\rangle \Biggr) \\
% K^(3,x) term
K^{(3,x)}_{\alpha\beta\gamma} = & \sum_{\ell=1}^{H} \langle G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle \sum_{j=1}^{\ell-1} \Biggl( \left\langle \left( \prod_{k=j+1}^{\ell-1} \frac{\sigma'_{k}(x_\alpha) W^{(k)}}{\sqrt{m}} \right) \frac{\sigma'_{j}(x_\alpha)}{\sqrt{m}} \langle x^{(j-1)}_\gamma, x^{(j-1)}_\alpha \rangle G^{(j)}_\gamma, x^{(\ell-1)}_\beta \right\rangle \nonumber \\
& \quad + \left\langle x^{(\ell-1)}_\alpha, \left( \prod_{k=j+1}^{\ell-1} \frac{\sigma'_{k}(x_\beta) W^{(k)}}{\sqrt{m}} \right) \frac{\sigma'_{j}(x_\beta)}{\sqrt{m}} \langle x^{(j-1)}_\gamma, x^{(j-1)}_\beta \rangle G^{(j)}_\gamma \right\rangle \Biggr)
\end{align}
In this expression, the products $\prod$ represent ordered matrix products from right to left. An empty product is the identity matrix.
\end{theorem}


\newpage

\section{Deeper Analysis of Finite-Width Corrections}

While the previous section confirmed the scaling of the $K^{(3)}$ tensor, we now turn to a more direct analysis of the finite-width correction to the NTK itself. We study the quantity $M(K_{\text{emp}} - K_{\infty})$, where $K_{\text{emp}}$ is the empirically measured NTK for a network of width $M$, and $K_{\infty}$ is the theoretical infinite-width kernel. This quantity represents the leading order finite-width correction, scaled by the width, and is central to understanding feature learning.

\subsection{Empirical Scaling Laws}

To precisely quantify the dependence of the finite-width correction on the network parameters, we performed a multivariate linear regression in the logarithmic space of the parameters. We model the spectral radius of the NTK correction, $\|K_{\text{emp}} - K_{\infty}\|_\infty$, as a power law of the depth ($L$), input dimension ($D_{\text{in}}$), number of data points ($N$), and width ($M$).

\begin{remark}[Computational Cost and Reproducibility]
It is important to note that the empirical results presented in this section are the culmination of extensive numerical experiments. Obtaining these data points and ensuring their stability required significant computational resources, amounting to approximately 24 hours of dedicated computation time. The full codebase is organized to be fully reproducible, ensuring the transparency and verifiability of these findings.
\end{remark}

The regression analysis yields an extremely clear relationship with a goodness-of-fit score of $R^2 = 0.987$. The resulting scaling law is:
\begin{equation}
    \|K_{\text{emp}} - K_{\infty}\|_\infty \propto L^{1.099} \cdot D_{\text{in}}^{0.028} \cdot N^{0.902} \cdot M^{-1.013}
    \label{eq:scaling_law}
\end{equation}
This empirical model, derived from a comprehensive multivariate regression with an intercept of $-1.278$, provides a precise characterization of the correction's behavior. Extensive further experiments, analyzing the scaling with respect to each parameter individually, have confirmed these findings across a wide range of configurations. We consistently observe that the correction's magnitude grows approximately linearly with the network's depth ($L^{1.099}$) and the number of data points ($N^{0.902}$). The correction also reliably decays as the inverse of the width ($M^{-1.013}$), which is strongly consistent with the theoretical expectation of an $\mathcal{O}(1/M)$ scaling for finite-width effects. Most importantly, our experiments suggest that the dependence on the input dimension $D_{\text{in}}$ is minimal ($D_{\text{in}}^{0.028}$). This is a crucial property for our setup, as it suggests that the feature-learning capabilities captured by the NTK's evolution do not degrade in high-dimensional spaces, making this approach highly promising for applications such as PDE solving in high dimensions.

\subsection{Theoretical Interpretation}

These rich empirical findings can be understood through the lens of the formula for the late-time NTK correction (Equation \ref{eq:theta_correction}), which we introduced in Section 2. This formula provides a deep connection between the observed scaling laws and the underlying structure of the theory.

This illuminates two crucial mechanisms that explain our empirical findings. First, we consider the influence of the higher-order kernels, $O_3$ (which is equivalent to our $K^{(3)}$) and $O_4$. Although our analysis has shown that the norm of $K^{(3)}$ itself grows with depth, the robust linear growth of the total NTK correction with depth $L$ strongly suggests that the term involving the $O_4$ kernel is the primary driver of this scaling behavior.

Second, the formula reveals the critical role of the initial kernel's spectrum. As discussed in Section 2.1, the correction terms are inversely proportional to the eigenvalues $\lambda_i$ of the initial kernel $\Theta_0$. We established that the smallest eigenvalues scale as $\mathcal{O}(L/N)$. This inverse relationship provides a compelling theoretical explanation for the observed scaling of the correction: for deeper networks (larger $L$) or for larger datasets (larger $N$), the smallest eigenvalues become smaller. A smaller denominator in Equation \ref{eq:theta_correction} naturally amplifies the entire finite-width correction, explaining why it scales so clearly with both depth $L$ and data size $N$.

\section{Experimental Validation}

To validate the theoretical formula for the late-time NTK correction $\Theta^{(1)}_\infty$ (Equation \ref{eq:theta_correction}), we conduct extensive numerical experiments. Our goal is to analyze how the correction term $M(K_{\text{emp}} - K_{\infty})$ scales with network depth $L$, width $M$, input dimension $D_{\text{in}}$, and dataset size $N$, and compare these empirical results with the theoretical predictions.

\subsection{Experimental Setup}

The experiments leverage JAX and the Neural Tangents library for efficient computation of finite-width neural networks, complemented by our custom implementation of the infinite-width limit. This setup allows us to precisely measure the finite-width corrections and their scaling behavior.

\begin{itemize}
    \item \textbf{Network Architecture:} We employ a standard feedforward Multi-Layer Perceptron (MLP) with ReLU activation functions. The architecture is characterized by:
    \begin{itemize}
        \item Input dimension $D_{\text{in}}$
        \item Uniform hidden layer width $M$
        \item Scalar output for simplified analysis
        \item Standard normal weight initialization scaled by $1/\sqrt{M}$
    \end{itemize}

    \item \textbf{Data Generation:} We generate $N$ input samples from a standard normal distribution, with each sample normalized to unit norm to ensure stable signal propagation through the network.

    \item \textbf{Parameter Space:} To comprehensively validate the scaling law in Equation \ref{eq:scaling_law}, we systematically explore:
    \begin{itemize}
        \item Network depth $L \in [2, 1000]$ to probe the $L^{1.171}$ scaling
        \item Width $M \in [10, 5000]$ to verify the $M^{-1.009}$ decay
        \item Input dimension $D_{\text{in}} \in [20, 5000]$ to confirm the weak $D_{\text{in}}$ dependence
        \item Sample size $N \in [8, 256]$ to test the $N^{0.900}$ scaling
    \end{itemize}
\end{itemize}

\subsection{Methodology}

For each configuration in our parameter sweep:

\begin{enumerate}
    \item We compute the empirical NTK $K_{\text{emp}}$ for the finite-width network using Neural Tangents, capturing all finite-width effects
    \item We calculate the theoretical infinite-width kernel $K_{\infty}$ using our InfiniteWidth implementation
    \item We form the width-scaled correction term $M(K_{\text{emp}} - K_{\infty})$
    \item We analyze the spectral radius $\|M(K_{\text{emp}} - K_{\infty})\|_\infty$ to quantify the correction's magnitude
\end{enumerate}

To ensure statistical significance and account for initialization effects, we repeat each measurement 10 times with different random seeds. This robust methodology allows us to accurately measure the scaling exponents that appear in Equation \ref{eq:scaling_law}.

\subsection{Results and Discussion}

The primary output of the experiment is a plot of the spectral radius of the correction term versus various network parameters.
\newpage
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{spectralradius.png}
    \caption{Spectral radius of the correction term $M(K_{\text{emp}} - K_{\infty})$ versus network depth $L$, plotted on a log-log scale. Different curves correspond to different widths $M$. The observed trend confirms that the correction grows with depth and scales inversely with width.}
    \label{fig:correction_vs_depth}
\end{figure}
\newpage
The results demonstrate clear scaling relationships between the correction term and network parameters. Most notably, we observe that the correction's magnitude grows with depth $L$ and number of samples $N$, while showing minimal dependence on input dimension $D_{in}$. The $1/M$ scaling is particularly well-validated, confirming our theoretical expansion.

A more precise analysis of the minimum eigenvalue $\lambda_{\min}(K_{\text{emp}})$ reveals an important finite-width effect. Based on our experiments and Weyl's inequality, we conjecture the bound:

\[ \lambda_{\min}(K_{\text{emp}}) \ge \frac{0.75L}{N} - C \frac{NL}{M} \]

where $C$ is a constant. This shows how finite width further decreases the small eigenvalues through the negative correction term.

For a parameter budget $P = M^2L$, we first optimize with respect to $L$:
\[
M = \sqrt{\frac{P}{L}} \implies \lambda_{\min} \ge \frac{0.75L}{N} - C \frac{NL}{\sqrt{\frac{P}{L}}} 
\]

The optimal value of $L$ occurs when the derivative of $\lambda_{\min}$ with respect to $L$ is zero:
\[
\frac{\partial}{\partial L} \lambda_{\min} = \frac{0.75}{N} - \frac{3}{2} C \frac{N}{\sqrt{P}} L^{1/2} = 0 \implies L^* = \frac{0.25 P}{C^2 N^4}
\]

This gives the optimal width:
\[
M^* = \sqrt{\frac{P}{L^*}} = 2 C N^2
\]

These values maximize the lower bound on $\lambda_{\min}$ for a fixed parameter budget $P$ and dataset size $N$. A more rigorous approach requires careful analysis of the higher-order corrections $K^{(3)}$ and $K^{(4)}$, which we explore in detail in the following sections.


\newpage
\section{Scaling Analysis of $K^{(3)}$ with Network Depth and Width}



\subsection{Experimental Validation}

To validate the theoretical scaling law $K^{(3)} \sim \Order(H^2/\sqrt{m})$, we conduct a series of numerical experiments. The goal is to compute the $K^{(3)}$ tensor empirically for networks of varying depths and analyze how its magnitude scales.

\subsection{Experimental Setup}

The experiments are implemented using JAX for efficient numerical computation.

\begin{itemize}
    \item \textbf{Network Architecture:} We use a standard feedforward Multi-Layer Perceptron (MLP) with ReLU activation functions. The network has an input dimension $D_{in}$, a uniform width $m$ for all $H$ hidden layers, and a scalar output. Weights are initialized using a standard normal distribution, consistent with the scaling used in our theoretical analysis.

    \item \textbf{Data:} The input data consists of $N$ samples drawn from a $D_{in}$-dimensional standard normal distribution. Each sample is then normalized to have a unit norm ($\|x_\alpha\|_2 = 1$), a common practice to ensure stable signal propagation.

    \item \textbf{Parameters:} In a typical experiment run, we set the number of data points $N=8$, the input dimension $D_{in}=20$, and the network width $M=100$. We then vary the network depth $L$ (equivalent to $H$ in our theoretical notation) across a range of values (e.g., from 2 to 5) to observe the scaling behavior.
\end{itemize}

\subsection{Methodology}

The computation of the empirical $K^{(3)}$ tensor follows directly from the formula derived in Section 3. The process is encapsulated in the `Kernel3Empirical` class and executed by the main experiment script.

\begin{enumerate}
    \item \textbf{Initialization:} For a given depth $L$, the network weights are initialized randomly.
    \item \textbf{Forward Pass:} A full forward pass is performed for all $N$ data points. During this pass, all intermediate activations $\{x^{(p)}_\alpha\}_{p=0..L, \alpha=1..N}$ and the diagonal matrices of activation derivatives $\{\sigma'_p(x_\alpha)\}_{p=1..L, \alpha=1..N}$ are computed and stored.
    \item \textbf{Kernel Computation:} The `kernel3(alpha, beta, gamma)` method is called for all triplets of input indices $(\alpha, \beta, \gamma)$. This method implements the recursive formulas for $\delta_\gamma x^{(p)}$, $G^{(\ell)}$, and $\delta_\gamma G^{(\ell)}$, combining them to calculate the final value of $K^{(3)}_{\alpha\beta\gamma}$. The full $N \times N \times N$ tensor is constructed by exploiting its symmetries.
    \item \textbf{Metric Calculation:} To quantify the magnitude of the $K^{(3)}$ tensor, we compute its element-wise infinity norm: $\|K^{(3)}\|_\infty = \max_{\alpha,\beta,\gamma} |K^{(3)}_{\alpha\beta\gamma}|$. We also analyze the eigenvalues of the $N \times N$ matrices obtained by "slicing" the tensor along one of its axes, i.e., the matrices $(K^{(3)}_{::\gamma})_{\alpha\beta} = K^{(3)}_{\alpha\beta\gamma}$ for each $\gamma$.
\end{enumerate}


\subsection{Scaling of Core Components and Numerical Challenges}

The analysis relies on the scaling of the fundamental building blocks of the NTK formalism: the forward-propagated activations $x^{(p)}_\mu$ and the backward-propagated gradient vectors $G^{(\ell)}_\mu$.

With proper initialization designed to achieve a state of "dynamical isometry", the norms of these vectors are stable across layers. This means the signal neither vanishes nor explodes as it propagates through a deep network. We can therefore assume that $\|x^{(p)}_\mu\| \sim \Order(1)$ and $\|G^{(\ell)}_\mu\| \sim \Order(1)$ for all layers $p$ and $\ell$.

The stability of the backward vectors $G^{(\ell)}$ is particularly subtle. It relies on the theory of products of random matrices. A naive application of the sub-multiplicative property for matrix norms might suggest an exponential explosion or decay of the norm with depth. However, this is avoided because $G^{(\ell)}$ results from the product of a sequence of \textit{independent} random matrices. On average, for an infinitely wide network, the norm is preserved.

However, for any finite-width network, the norm of the product of random matrices is governed by Lyapunov exponents. While a state of dynamical isometry aims for the leading Lyapunov exponent to be zero, fluctuations in finite-size systems can lead to slight exponential trends with depth. Disentangling this potential exponential behavior from the polynomial scaling of the correction terms that we seek to measure is a significant challenge in numerical experiments.

With this context, we can analyze the scaling of the derivative terms required for $K^{(3)}$.
\begin{itemize}
    \item \textbf{Forward Derivative Term $\delta_\gamma x^{(p)}_\mu$}: Starting from the recursion in Proposition 3.2 and $\delta_\gamma x^{(0)}=0$, we can show that the norm grows linearly with the layer index $p$:
    \[
    \|\delta_\gamma x^{(p)}_\mu\| \sim \Order\left(\frac{p}{\sqrt{m}}\right)
    \]
    \item \textbf{Backward Derivative Term $\delta_\gamma G^{(\ell)}_\mu$}: This term involves a sum over subsequent layers from $\ell$ to $H$. Each term in the sum is of order $\Order(1/m)$, and there are $H-\ell$ such terms. This results in a norm that scales as:
    \[
    \|\delta_\gamma G^{(\ell)}_\mu\| \sim \Order\left(\frac{H-\ell}{m}\right)
    \]
\end{itemize}


\subsection{Term-by-Term Scaling Analysis}
We now use these component scalings to analyze the four main parts of $K^{(3)}_{\alpha\beta\gamma}$ from Theorem 3.4.

\begin{enumerate}
    \item \textbf{Output Term ($K^{(3,\text{out})}$):} This term is dominated by inner products like $\langle \delta_\gamma x^{(H)}_\alpha, x^{(H)}_\beta \rangle$. Using the scaling above, its magnitude is $\|\delta_\gamma x^{(H)}_\alpha\| \sim \Order(H/\sqrt{m})$.

    \item \textbf{Backward Term from Weights ($K^{(3,G,W)}$):} This involves a double sum over $\ell$ and $p$. Each term contains inner products of $\delta G$ vectors, like $\langle \delta_\gamma G^{(\ell)}_\alpha, G^{(\ell)}_\beta \rangle$, whose norm scales as $\Order((H-\ell)/m)$. The double summation over approximately $H^2/2$ terms leads to a total contribution of $\Order(H^2/m)$.

    \item \textbf{Backward Term from Output Layer ($K^{(3,G,a)}$):} This is a sum over $H$ terms, each involving $\delta G$. A representative term scales as $\Order(H/m)$, but a more careful analysis of the specific structure reveals a scaling of $\Order(1/\sqrt{m})$ per term, for a total of $\Order(H/\sqrt{m})$.

    \item \textbf{Forward Term from Activations ($K^{(3,x)}$):} This term is a sum over $\ell$ from 1 to $H$. Each term contains an inner product of $\delta x$ vectors, for instance $\langle \delta_\gamma x^{(\ell-1)}_\alpha, x^{(\ell-1)}_\beta \rangle$. The norm of this component scales as $\Order((\ell-1)/\sqrt{m})$. The sum over all layers is $\sum_{\ell=1}^{H} \Order((\ell-1)/\sqrt{m}) \sim \frac{1}{\sqrt{m}} \sum_{j=0}^{H-1} j \sim \Order(H^2/\sqrt{m})$.
\end{enumerate}

\subsection{Overall Scaling and Conclusion}

Combining the scaling of the four components, we have:
\[
K^{(3)}_{\alpha\beta\gamma} \sim \Order(H/\sqrt{m}) + \Order(H^2/m) + \Order(H/\sqrt{m}) + \Order(H^2/\sqrt{m})
\]
For typical architectures where $H \ll \sqrt{m}$, the dominant term is the one with the slowest decay in $m$ and fastest growth in $H$, which is the forward term $K^{(3,x)}$.

\begin{theorem}[Scaling of $K^{(3)}$]
The magnitude of the entries of the third-order kernel $K^{(3)}$ scales with network depth $H$ and width $m$ as:
\begin{equation}
K^{(3)}_{\alpha\beta\gamma} \sim \Order\left(\frac{H^2}{\sqrt{m}}\right)
\end{equation}
\end{theorem}

This result is a cornerstone of our analysis. It demonstrates that the influence of the first-order correction to the NTK dynamics does not diminish with depth; on the contrary, it grows quadratically. This implies that for deep networks, the evolution of the NTK is a significant phenomenon that cannot be ignored. The finite-width corrections become progressively more important as networks get deeper.



\subsection{Results and Discussion}

The primary output of the experiment is a plot of the infinity norm of $K^{(3)}$ as a function of the network depth $L$.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{k3analysis.png}
    \caption{Infinity norm of the empirical $K^{(3)}$ tensor versus network depth $L$, plotted on a log-log scale. The width is fixed at $M=100$. The observed trend confirms that the magnitude of the correction kernel grows with depth.}
    \label{fig:k3_norm_vs_l}
\end{figure}

The results, as shown in Figure \ref{fig:k3_norm_vs_l}, demonstrate a clear increase in the magnitude of $K^{(3)}$ as the depth $L$ increases. While a precise fit to a quadratic curve would require more extensive simulations over a wider range of depths, the observed trend is strongly consistent with our theoretical prediction of $K^{(3)} \sim \Order(L^2/\sqrt{m})$. The experiment successfully validates that finite-width effects, as captured by the first-order NTK correction, become significantly more pronounced in deeper networks. This provides empirical support for the idea that deep networks are less "lazy" and their internal representations evolve more substantially during training compared to shallow ones.


To understand the practical impact of the $K^{(3)}$ correction, we analyze how its magnitude scales with the network depth $H$ and width $m$. This analysis reveals how the importance of the NTK's evolution changes with the network architecture.





\section{Future Work}
This research opens up several promising avenues for future investigation. A primary goal is to further clarify the Neural Tangent Hierarchy dynamics by computing the scaling of its hierarchical coefficients, particularly $\lambda_H$, and empirically verifying its predicted linear scaling with depth. Building on this, since the $O_4$ kernel appears to be a critical component of the NTK correction, a detailed study of its properties, including its eigendecomposition, is necessary. Should a full implementation not be available in existing literature, developing one will be essential to fully analyze its structure and contribution to feature learning.

Beyond the specific kernels, the emergence of particular geometric patterns in the NTK spectrum, such as the observed "parabola shape," also warrants deeper investigation. Understanding the origin of these shapes could reveal new insights into the inductive biases of deep networks. To test the generality of our findings, the numerical framework should also be extended to data with inherent geometric structure, for instance, data defined on the torus or over $\mathbb{R}^d$. This would require leveraging techniques like Kronecker decomposition and spherical harmonics to analyze how the NTK evolution depends on the data manifold and input dimension.

Finally, to bridge the gap between theory and practice, this analysis must be expanded to include more complex and modern architectures. Investigating the NTK hierarchy in different settings, such as for narrow networks or for networks with skip connections like ResNets and varying initialization schemes, will be a crucial next step.

\section{Conclusion}

This report has detailed the Neural Tangent Hierarchy as a framework for analyzing the training dynamics of finite-width neural networks. We focused on the first-order correction kernel, $K^{(3)}$, which governs the time evolution of the standard NTK, $K^{(2)}$.

Our primary contributions are:
\begin{itemize}
    \item A complete, non-recursive formula for the $K^{(3)}$ kernel, derived from first principles.
    \item A scaling analysis demonstrating that the magnitude of $K^{(3)}$ grows quadratically with network depth ($H^2$) and is suppressed by the square root of the width ($\sqrt{m}$).
\end{itemize}

The scaling law $K^{(3)} \sim \Order(H^2/\sqrt{m})$ has profound implications. It shows that the "laziness" of training, characterized by a static NTK, is less applicable to deeper networks. The training dynamics of deep networks are inherently richer than those of their shallow counterparts, as the learning geometry itself evolves more significantly. This provides a theoretical underpinning for the empirical observation that deeper networks often outperform shallow ones, as they are better able to adapt their internal feature representations during training.

Future work should aim to connect this scaling behavior to concrete measures of generalization and performance, as well as to validate these theoretical predictions through targeted numerical experiments.

\newpage

\begin{thebibliography}{11}

    \bibitem{jacot2018neural}
    A. Jacot, F. Gabriel, and C. Hongler,
    \newblock ``Neural Tangent Kernel: Convergence and Generalization in Neural Networks,''
    \newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2018.
    
    \bibitem{arora2019exact}
    S. Arora, S. Du, W. Hu, Z. Li, and R. Wang,
    \newblock ``On Exact Computation with an Infinitely Wide Net,''
    \newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2019.
    
    \bibitem{huang2019dynamics}
    J. Huang and H.-T. Yau,
    \newblock ``Dynamics of Deep Neural Networks and Neural Tangent Hierarchy,''
    \newblock \emph{arXiv preprint arXiv:1909.08156}, 2019.
    \newblock E-mail: jiaoyang@ias.edu, htyau@math.harvard.edu.
    
    \bibitem{dyer2020asymptotics}
    E. Dyer and G. Gur-Ari,
    \newblock ``Asymptotics of Wide Networks from Feynman Diagrams,''
    \newblock \emph{arXiv preprint arXiv:2006.12345}, 2020.
    \newblock E-mail: edyer@google.com, guyga@google.com.
    
    \end{thebibliography}

\newpage
\appendix
\section*{Appendix A: Raw Scaling Data for N}

\begin{verbatim}
Slopes for N scaling:
Configuration | Slope | R^2 | Points
--------------------------------------------------
D_IN=20, L=2, M=10 | 0.852 | 0.993 | 13
D_IN=20, L=12, M=10 | 0.943 | 0.999 | 11
D_IN=50, L=4, M=10 | 0.898 | 0.997 | 10
D_IN=20, L=8, M=20 | 0.927 | 0.998 | 10
D_IN=20, L=2, M=30 | 0.798 | 0.995 | 9
D_IN=50, L=10, M=10 | 0.928 | 0.998 | 9
D_IN=100, L=8, M=10 | 0.897 | 0.999 | 8
D_IN=20, L=26, M=10 | 0.886 | 0.999 | 8
D_IN=50, L=18, M=10 | 0.901 | 0.999 | 8
D_IN=50, L=12, M=20 | 0.902 | 1.000 | 8
D_IN=50, L=6, M=20 | 0.882 | 0.999 | 8
D_IN=20, L=22, M=20 | 1.032 | 0.951 | 8
D_IN=50, L=4, M=30 | 0.857 | 0.999 | 7
D_IN=100, L=18, M=10 | 0.864 | 0.999 | 7
D_IN=200, L=10, M=10 | 0.889 | 1.000 | 7
D_IN=20, L=6, M=40 | 0.869 | 0.998 | 7
D_IN=20, L=18, M=30 | 0.853 | 0.976 | 7
D_IN=50, L=16, M=20 | 0.889 | 0.998 | 7
D_IN=50, L=20, M=20 | 0.889 | 0.999 | 7
D_IN=20, L=42, M=10 | 0.883 | 0.999 | 7
D_IN=100, L=12, M=20 | 0.906 | 1.000 | 7
D_IN=20, L=16, M=30 | 0.872 | 0.990 | 7
D_IN=200, L=4, M=10 | 0.869 | 0.999 | 7
D_IN=100, L=32, M=10 | 0.878 | 0.999 | 6
D_IN=20, L=46, M=10 | 0.871 | 0.999 | 6
D_IN=20, L=48, M=10 | 0.861 | 1.000 | 6
D_IN=20, L=38, M=20 | 0.912 | 0.963 | 6
D_IN=200, L=2, M=30 | 0.759 | 0.998 | 6
D_IN=20, L=2, M=60 | 0.777 | 0.996 | 6
D_IN=100, L=6, M=30 | 0.877 | 1.000 | 6
D_IN=1000, L=2, M=10 | 0.762 | 0.998 | 6
D_IN=500, L=8, M=10 | 0.881 | 0.999 | 6
D_IN=20, L=8, M=50 | 0.884 | 0.998 | 6
D_IN=200, L=14, M=10 | 0.875 | 0.999 | 6
D_IN=20, L=16, M=40 | 0.837 | 0.996 | 6
D_IN=100, L=20, M=20 | 0.882 | 0.999 | 6
D_IN=50, L=38, M=10 | 0.867 | 1.000 | 6
D_IN=50, L=20, M=30 | 0.891 | 1.000 | 6
D_IN=50, L=14, M=30 | 0.915 | 1.000 | 6
D_IN=50, L=30, M=20 | 0.875 | 0.999 | 6
D_IN=100, L=36, M=10 | 0.875 | 0.999 | 5
D_IN=500, L=2, M=30 | 0.744 | 0.997 | 5
D_IN=1000, L=12, M=10 | 0.876 | 1.000 | 5
D_IN=50, L=40, M=20 | 0.857 | 0.998 | 5
D_IN=100, L=2, M=50 | 0.747 | 0.997 | 5
D_IN=200, L=6, M=30 | 0.867 | 1.000 | 5
D_IN=20, L=12, M=60 | 0.921 | 0.994 | 5
D_IN=100, L=34, M=10 | 0.866 | 1.000 | 5
D_IN=100, L=40, M=10 | 0.863 | 0.999 | 5
D_IN=1000, L=10, M=10 | 0.873 | 0.999 | 5
D_IN=100, L=18, M=30 | 0.835 | 1.000 | 5
D_IN=100, L=14, M=30 | 0.874 | 0.999 | 5
D_IN=20, L=42, M=30 | 0.844 | 1.000 | 5
D_IN=200, L=32, M=10 | 0.866 | 0.999 | 5
D_IN=20, L=30, M=40 | 0.859 | 0.999 | 5
D_IN=20, L=38, M=30 | 0.836 | 0.991 | 5
D_IN=20, L=2, M=70 | 0.737 | 0.997 | 5
D_IN=100, L=10, M=40 | 0.875 | 1.000 | 5
D_IN=50, L=10, M=50 | 0.849 | 0.999 | 5
D_IN=100, L=32, M=20 | 0.835 | 0.999 | 5
D_IN=20, L=58, M=10 | 0.858 | 0.999 | 5
D_IN=50, L=22, M=40 | 0.855 | 0.992 | 5
D_IN=20, L=24, M=50 | 0.838 | 0.990 | 4
D_IN=500, L=24, M=10 | 0.871 | 0.998 | 4
D_IN=200, L=2, M=50 | 0.726 | 0.999 | 4
D_IN=100, L=8, M=50 | 0.850 | 0.999 | 4
D_IN=500, L=6, M=30 | 0.854 | 0.999 | 4
D_IN=50, L=48, M=20 | 0.841 | 1.000 | 4
D_IN=200, L=18, M=30 | 0.859 | 1.000 | 4
D_IN=500, L=14, M=20 | 0.869 | 1.000 | 4
D_IN=50, L=22, M=50 | 0.854 | 0.999 | 4
D_IN=20, L=10, M=70 | 0.907 | 0.998 | 4
D_IN=50, L=42, M=30 | 0.821 | 1.000 | 4
D_IN=50, L=70, M=10 | 0.842 | 0.999 | 4
D_IN=20, L=90, M=10 | 0.839 | 0.999 | 4
D_IN=2000, L=12, M=10 | 0.863 | 1.000 | 4
D_IN=50, L=52, M=20 | 0.863 | 0.999 | 4
D_IN=2000, L=4, M=10 | 0.832 | 0.999 | 4
D_IN=50, L=2, M=200 | 0.787 | 0.995 | 4
D_IN=100, L=50, M=10 | 0.847 | 0.999 | 4
D_IN=20, L=56, M=20 | 0.832 | 1.000 | 4
D_IN=100, L=22, M=40 | 0.851 | 0.991 | 4
D_IN=50, L=32, M=40 | 0.877 | 0.994 | 4
D_IN=2000, L=6, M=10 | 0.853 | 0.999 | 4
D_IN=100, L=14, M=40 | 0.856 | 0.999 | 4
D_IN=100, L=26, M=30 | 0.854 | 1.000 | 4
D_IN=500, L=16, M=20 | 0.865 | 1.000 | 4
D_IN=20, L=54, M=20 | 1.406 | 0.927 | 4
D_IN=500, L=26, M=10 | 0.857 | 1.000 | 4
D_IN=100, L=36, M=20 | 0.850 | 0.999 | 4
D_IN=100, L=40, M=20 | 0.855 | 0.999 | 4
D_IN=50, L=8, M=60 | 0.879 | 0.999 | 4
D_IN=1000, L=20, M=10 | 0.860 | 1.000 | 4
D_IN=20, L=44, M=30 | 0.834 | 0.996 | 4
D_IN=50, L=8, M=100 | 0.904 | 0.999 | 4
\end{verbatim}

\newpage
\section*{Appendix B: Raw Scaling Data for L}
\begin{verbatim}
Slopes for L scaling:
Configuration | Slope | R^2 | Points
--------------------------------------------------
N=8, D_IN=20, M=10 | 1.027 | 0.957 | 53
N=16, D_IN=20, M=10 | 1.026 | 0.991 | 41
N=8, D_IN=100, M=20 | 1.021 | 0.996 | 37
N=8, D_IN=50, M=30 | 1.035 | 0.995 | 37
N=8, D_IN=100, M=30 | 1.031 | 0.995 | 32
N=10, D_IN=50, M=30 | 1.036 | 0.992 | 32
N=25, D_IN=50, M=10 | 1.075 | 0.969 | 31
N=10, D_IN=100, M=30 | 1.047 | 0.993 | 27
N=10, D_IN=500, M=10 | 1.047 | 0.993 | 27
N=8, D_IN=200, M=30 | 1.061 | 0.986 | 27
N=40, D_IN=20, M=10 | 1.074 | 0.920 | 26
N=25, D_IN=100, M=10 | 1.071 | 0.989 | 26
N=10, D_IN=20, M=60 | 1.080 | 0.986 | 22
N=8, D_IN=1000, M=20 | 1.061 | 0.993 | 22
N=32, D_IN=100, M=10 | 1.100 | 0.988 | 21
N=16, D_IN=200, M=20 | 1.082 | 0.989 | 21
N=32, D_IN=20, M=30 | 1.164 | 0.970 | 21
N=10, D_IN=500, M=30 | 1.091 | 0.991 | 17
N=10, D_IN=2000, M=10 | 1.093 | 0.992 | 17
N=8, D_IN=20, M=80 | 1.099 | 0.992 | 17
N=32, D_IN=200, M=10 | 1.132 | 0.986 | 16
N=16, D_IN=50, M=50 | 1.123 | 0.989 | 16
N=16, D_IN=1000, M=10 | 1.113 | 0.989 | 16
N=16, D_IN=20, M=60 | 1.142 | 0.990 | 16
N=8, D_IN=500, M=50 | 1.129 | 0.992 | 12
N=10, D_IN=20, M=80 | 1.152 | 0.992 | 12
N=8, D_IN=200, M=60 | 1.128 | 0.993 | 12
N=8, D_IN=100, M=70 | 1.132 | 0.993 | 12
N=50, D_IN=100, M=10 | 1.205 | 0.983 | 11
N=25, D_IN=100, M=40 | 1.189 | 0.988 | 11
N=25, D_IN=20, M=60 | 1.178 | 0.985 | 11
N=16, D_IN=2000, M=10 | 1.172 | 0.990 | 11
N=25, D_IN=500, M=20 | 1.190 | 0.988 | 11
N=8, D_IN=50, M=200 | 1.158 | 0.993 | 9
N=8, D_IN=20, M=500 | 1.182 | 0.993 | 9
N=32, D_IN=20, M=200 | 1.248 | 0.983 | 9
N=16, D_IN=20, M=100 | 1.240 | 0.991 | 9
N=16, D_IN=20, M=200 | 1.207 | 0.991 | 9
N=8, D_IN=100, M=500 | 1.167 | 0.993 | 9
N=10, D_IN=5000, M=20 | 1.222 | 0.993 | 7
N=8, D_IN=1000, M=50 | 1.205 | 0.994 | 7
N=10, D_IN=2000, M=30 | 1.222 | 0.993 | 7
N=10, D_IN=500, M=50 | 1.224 | 0.993 | 7
N=16, D_IN=50, M=70 | 1.283 | 0.990 | 6
N=16, D_IN=500, M=40 | 1.288 | 0.992 | 6
N=40, D_IN=200, M=20 | 1.339 | 0.989 | 6
N=32, D_IN=50, M=50 | 1.318 | 0.987 | 6
N=16, D_IN=200, M=50 | 1.286 | 0.992 | 6
N=100, D_IN=20, M=10 | 1.407 | 0.991 | 6
N=32, D_IN=200, M=30 | 1.329 | 0.990 | 6
N=32, D_IN=100, M=40 | 1.326 | 0.989 | 6
N=64, D_IN=20, M=30 | 1.383 | 0.986 | 6
N=64, D_IN=50, M=100 | 1.475 | 0.992 | 4
N=128, D_IN=20, M=20 | 1.526 | 0.994 | 4
\end{verbatim}

\end{document}
