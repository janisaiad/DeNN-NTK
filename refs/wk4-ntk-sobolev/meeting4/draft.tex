\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumerate}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usetikzlibrary{math}
\usepackage[round]{natbib}

% Page setup
\pagestyle{plain}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}

% Mathematical commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\evmin}[1]{\lambda_{\min}\left(#1\right)}
\newcommand{\evmax}[1]{\lambda_{\max}\left(#1\right)}
\newcommand{\svmin}[1]{\sigma_{\min}\left(#1\right)}
\newcommand{\tr}{\text{tr}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\bigOmg}{\Omega}
\newcommand{\bigTheta}{\Theta}
\newcommand{\KNTK}{K_{\text{NTK}}}
\newcommand{\KLim}{K^{(L)}}
\newcommand{\KEmp}{\overline{K}^{(L)}}
\newcommand{\KCosine}{K_L}
\newcommand{\lambdaMin}{\lambda_{\min}}
\newcommand{\lambdaMax}{\lambda_{\max}}
\newcommand{\sigmaMin}{\sigma_{\min}}
\newcommand{\Order}{\mathcal{O}}
\newcommand{\TildeOrder}{\tilde{\mathcal{O}}}
\newcommand{\OmegaNotation}{\Omega}
\newcommand{\TildeOmega}{\tilde{\Omega}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\LaplaceBeltrami}{\Delta_{\mathbb{S}^{d_0-1}}}
\newcommand{\limiting}[1]{#1^{\infty}}
\newcommand{\Kinf}{K^{\infty}}

\title{Spectral Analysis of the Neural Tangent Kernel and its Modification by Sobolev-type Training}

\author{Synthesis and Analysis}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document re-examines the eigenvalue scaling of the Neural Tangent Kernel (NTK) and its modification through Sobolev-type training, drawing heavily on the work of Yu, Yang, and Townsend (ICLR 2023, arXiv:2205.14300v2). We focus on their analysis of frequency bias for non-uniform data and how training with a Sobolev norm modifies the learning dynamics of different frequencies. The aim is to calculate the final spectral exponent of the modified NTK and to justify the assumption of common eigenfunctions for the involved operators.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

The Neural Tangent Kernel (NTK) has become a central tool for the theoretical analysis of overparameterized neural networks (NNs) \citep{jacot2018neural}. A key phenomenon observed during the training of such networks is "frequency bias": gradient-based algorithms tend to first minimize the error in low frequencies before addressing high-frequency residuals \citep{rahaman2019spectral, basri2020frequency}. Yu, Yang, and Townsend (YYT23, \citep{yu2022tuning}) extended the rigorous analysis of this bias to the case of non-uniform training data, using the NTK and data-dependent quadrature rules.

YYT23 also propose replacing the usual $L^2$ loss function with a carefully chosen Sobolev $H^s$ norm to amplify, dampen, counterbalance, or reverse this intrinsic frequency bias. This document aims to synthesize this approach, focusing on:
\begin{enumerate}
    \item Adopting the notation and framework of YYT23 to describe the NTK and its spectral bias.
    \item Calculating the exponent that characterizes the decay (or growth) of the eigenvalues of the effective NTK after applying Sobolev training, particularly for data on the sphere $\mathbb{S}^{d-1}$.
    \item Theoretically justifying the sharing of eigenfunctions (spherical harmonics) by the NTK operator and the Sobolev preconditioning operator, under isotropy assumptions.
\end{enumerate}

\section{The NTK and its Spectral Bias according to YYT23}

Let a neural network be $\mathcal{N}(\mathbf{x}; \mathbf{W})$ where $\mathbf{x} \in \mathbb{R}^d$ and $\mathbf{W}$ represents the parameters. YYT23 consider training data $(\mathbf{x}_i, y_i)_{i=1}^n$ where $y_i = g(\mathbf{x}_i)$ for an underlying function $g$, and the $\mathbf{x}_i$ are normalized on the unit sphere $\mathbb{S}^{d-1}$.

\subsection{The Limiting Neural Tangent Kernel from Terjék \& Gonzáles-Sánchez (MLPs at EOC)}
Terjék and Gonzáles-Sánchez (TGS25, \citep{terjek2025ntk}) study the NTK for Multilayer Perceptrons (MLPs) with $(a,b)$-ReLU activations, $\phi(s) = as + b|s|$, initialized at the Edge of Chaos (EOC). The EOC initialization implies setting the initial weight variance $\sigma^2 = (a^2+b^2)^{-1}$. A key parameter characterizing the activation is $\Delta_\phi = \frac{b^2}{a^2+b^2}$.

They define a cosine map $\varrho: [-1,1] \to [-1,1]$ (Prop. 3.4, TGS25):
\begin{equation}
    \varrho(\rho) = \rho + \Delta_\phi \frac{2}{\pi}\left( \sqrt{1-\rho^2} - \rho \arccos(\rho) \right)
\end{equation}
\begin{equation}
    \varrho'(\rho) = 1 - \Delta_\phi \frac{2}{\pi}\arccos(\rho)
\end{equation}
Let $\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2) = \frac{\langle \mathbf{x}_1, \mathbf{x}_2 \rangle}{\left\lVert\mathbf{x}_1\right\rVert \left\lVert\mathbf{x}_2\right\rVert}$ be the initial cosine similarity. The limiting NTK $K^{\infty}$ at EOC is given by (Prop. 3.5, TGS25):
\begin{equation}
    K^{\infty}(\mathbf{x}_1, \mathbf{x}_2) = \left\lVert\mathbf{x}_1\right\rVert \left\lVert\mathbf{x}_2\right\rVert \left( \sum_{k=1}^l \varrho^{\circ (k-1)}\left( \rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2) \right) \prod_{k'=k}^{l-1} \varrho'\left( \varrho^{\circ (k'-1)}\left( \rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2) \right) \right) \right) \mathbf{I}_{m_l}.
    \label{eq:NTK_TGS25}
\end{equation}
where $l$ is the depth and $m_l$ is the output dimension. Their spectral analysis (Thm. 3.10, TGS25) reveals that for a dataset of size $n$:
\begin{itemize}
    \item The eigenvalues of the $nm_l \times nm_l$ NTK matrix group together. There are $m_l$ large eigenvalues scaling roughly as $\overline{\tau}^2 (l/4 + \xi)$ and $(n-1)m_l$ smaller ones scaling as $\underline{\tau}^2 (3l/(4n) - \xi/n)$, where $\xi$ depends on depth and initial data separation, and $\tau$ relates to input norms.
    \item The condition number $\kappa(K^{\infty})$ converges to $1 + n/3$ as depth $l$ increases.
    \item $\Delta_\phi$ (e.g., $\Delta_\phi=1$ for absolute value is better than $\Delta_\phi=1/2$ for ReLU) affects the convergence rate and conditioning for a given depth.
\end{itemize}
This framework provides a detailed view of NTK spectrum scaling with depth but does not directly use a harmonic basis for its primary spectral decay analysis.

\subsection{The Limiting Neural Tangent Kernel $K^{\infty}$ in YYT23}
In the infinite-width limit, the NTK $K(\mathbf{x}, \mathbf{x}'; \mathbf{W})$ converges to a deterministic kernel $K^{\infty}(\mathbf{x}, \mathbf{x}')$. For 2-layer ReLU networks with zero biases (simplified YYT23 setting for main analysis), and weights $\mathbf{w}_r$ i.i.d. Gaussian $\mathcal{N}(\mathbf{0}, \kappa^2 \mathbf{I})$, $K^{\infty}$ is given by (Eq. 2.4, YYT23, adapted):
\begin{equation}
    K^{\infty}(\mathbf{x}, \mathbf{y}) = \frac{(\langle \mathbf{x}, \mathbf{y} \rangle + 1) (\pi - \arccos(\langle \mathbf{x}, \mathbf{y} \rangle))}{4\pi}
    \label{eq:Kinf_YYT23}
\end{equation}
Note that YYT23 use $\mathbf{H}^\infty_{ij} = K^{\infty}(\mathbf{x}_i, \mathbf{x}_j)$ for the discrete matrix.

\subsection{Integral Operator and Intrinsic Spectral Bias}
The evolution of the residual $z(\mathbf{x}) = g(\mathbf{x}) - \mathcal{N}(\mathbf{x})$ during gradient flow training can be described by the integral operator (YYT23, after Eq. 1.1, and with $d\mathbf{x}'$ for their Eq. 1.2 framework):
\begin{equation}
    (\mathcal{L} z)(\mathbf{x}) = \int_{\mathbb{S}^{d-1}} K^{\infty}(\mathbf{x}, \mathbf{x}') z(\mathbf{x}') d\mathbf{x}'
\end{equation}
When the domain is $\mathbb{S}^{d-1}$ and $K^{\infty}(\mathbf{x}, \mathbf{x}')$ is rotationally invariant (i.e., depends only on $\langle \mathbf{x}, \mathbf{x}' \rangle$), the eigenfunctions of $\mathcal{L}$ are the spherical harmonics $Y_{\ell,p}(\mathbf{x})$. The corresponding eigenvalues, denoted $\mu_\ell$ by YYT23 (Eq. 2.5), depend only on the degree $\ell$.

A central result, recalled by YYT23 (end of Section 2, citing \cite{basri2020frequency, bietti2019inductive}), is that for large values of $\ell$:
\begin{equation}
    \mu_\ell = \mathcal{O}(\ell^{-d})
    \label{eq:mu_ell_decay}
\end{equation}
This rapid decay of eigenvalues $\mu_\ell$ with frequency $\ell$ is the \textbf{intrinsic spectral bias} of the NTK: the operator $\mathcal{L}$ strongly attenuates high-frequency components of the residual function, leading to slower learning of these components.

\subsection{Case of Non-Uniform Data (YYT23)}
Most real-world datasets are not uniformly distributed. YYT23 address this problem by considering a continuous loss function with respect to the Lebesgue measure $d\mathbf{x}$ on $\mathbb{S}^{d-1}$ (their Eq. 1.2):
\begin{equation}
    \widetilde{\Phi}(\mathbf{W}) = \frac{1}{2}   \int_{\mathbb{S}^{d-1}}  | g(\mathbf{x}) - \mathcal{N}(\mathbf{x};\mathbf{W})|^2 d\mathbf{x}
\end{equation}
This is then discretized using a quadrature rule with weights $c_i > 0$ at data points $\mathbf{x}_i$:
\begin{equation}
    \widetilde{\Phi}(\mathbf{W}) \approx \frac{1}{2} \sum_{i=1}^n  c_i |y_i - \mathcal{N}(\mathbf{x}_i;\mathbf{W})|^2 = \frac{1}{2} (\mathbf{y} - \mathbf{u})^T \mathbf{D_c} (\mathbf{y} - \mathbf{u})
    \label{eq:loss_quadrature}
\end{equation}
Where $\mathbf{u}_i = \mathcal{N}(\mathbf{x}_i)$, $\mathbf{y}_i = g(\mathbf{x}_i)$, and $\mathbf{D_c} = \text{diag}(c_1, \dots, c_n)$.
Theorem 4.1 of YYT23 (\textit{A frequency-based formula for the training error}) shows that, even with this discretization and non-uniform data, the learning dynamics of the frequency component $g_\ell$ of $g$ are still dictated by the $\mu_\ell$ of the continuous kernel $K^{\infty}$ (up to quadrature errors):
\begin{equation}
    \left\lVert\mathbf{y} - \mathbf{u}(k) \right\rVert_{\mathbf{D_c}} \approx \sqrt{\sum_{\ell=0}^L (1-2\eta\mu_\ell)^{2k} \left\lVert g_\ell\right\rVert^2_{L^2(\mathbb{S}^{d-1})}}
\end{equation}
Thus, the frequency bias $\mu_\ell \sim \ell^{-d}$ persists for non-uniform data when the loss is defined with respect to the underlying Lebesgue measure.

\section{Sobolev-type Training according to YYT23}

To actively control the frequency bias, YYT23 propose using a loss function based on an $H^s$ Sobolev norm.

\subsection{Sobolev Norm and Loss Function}
The $H^s(\mathbb{S}^{d-1})$ Sobolev norm is defined (YYT23, Section 5) by:
\begin{equation}
    \left\lVert f\right\rVert_{H^s(\mathbb{S}^{d-1})}^2 = \sum_{\ell=0}^\infty \sum_{p=1}^{N(d,\ell)} (1+\ell)^{2s}|\hat{f}_{\ell,p}|^2
\end{equation}
where $\hat{f}_{\ell,p}$ are the spherical harmonic coefficients of $f$. The discretized Sobolev loss function is (Eq. 5.3, YYT23):
\begin{equation}
    \Phi_s(\mathbf{W}) = \frac{1}{2} (\mathbf{y} - \mathbf{u})^T \mathbf{P}_s (\mathbf{y} - \mathbf{u})
    \label{eq:sobolev_loss_YYT23}
\end{equation}
where $\mathbf{P}_s = \sum_{\ell=0}^{\ell_{max}} \sum_{p=1}^{N(d,\ell)} (1+\ell)^{2s} \mathbf{a}_{\ell,p} \mathbf{a}_{\ell,p}^T$, with $(\mathbf{a}_{\ell,p})_i = c_i Y_{\ell,p}(\mathbf{x}_i)$. Essentially, $\mathbf{P}_s$ applies a frequency re-weighting.

\subsection{Training Dynamics with Sobolev Loss}
Theorem 5.1 of YYT23 describes the error dynamics for this loss. For the $k$-th gradient descent iteration and neglecting quadrature and NTK approximation errors:
\begin{equation}
    \mathbf{y} - \mathbf{u}(k) \approx \sum_{\ell=0}^L (1-2\eta\mu_\ell(1+\ell)^{2s})^k \mathbf{y}^\ell
\end{equation}
where $\mathbf{y}^\ell$ is the vector of $g_\ell(\mathbf{x}_i)$.
This means the effective convergence rate for the frequency component $\ell$ is now $e^{-2\eta \mu_\ell (1+\ell)^{2s} \cdot \text{time}}$.

\subsection{Calculation of the Final Spectral Exponent}
The continuous NTK operator $\mathcal{L}$ (with respect to $d\mathbf{x}$) has eigenvalues $\lambda_\ell(\mathcal{L}) = \mu_\ell \sim \ell^{-d}$.
The effect of the Sobolev loss is to introduce a frequency preconditioner $P_s$ whose action on the $\ell$-th harmonic component is a multiplication by $(1+\ell)^{2s}$. So, $\lambda_\ell(P_s) \sim \ell^{2s}$ for large $\ell$.

The "effective modified kernel" $K_S$ can be seen as having eigenvalues:
\begin{equation}
    \lambda_\ell(K_S) = \lambda_\ell(\mathcal{L}) \cdot \lambda_\ell(P_s) \sim \ell^{-d} \cdot \ell^{2s} = \ell^{2s-d}
    \label{eq:final_exponent}
\end{equation}
The final exponent of spectral decay (or growth) is therefore $\mathbf{2s-d}$.
\begin{itemize}
    \item If $s = d/2$, then $2s-d = 0$. The eigenvalues $\lambda_\ell(K_S)$ become approximately constant for large $\ell$. The frequency bias is compensated.
    \item If $s > d/2$, then $2s-d > 0$. The eigenvalues $\lambda_\ell(K_S)$ increase with $\ell$. High frequencies are amplified (frequency bias reversed).
    \item If $s < d/2$, then $2s-d < 0$. The eigenvalues $\lambda_\ell(K_S)$ still decay, but slower than $\ell^{-d}$ if $s>0$.
\end{itemize}
This parameter $s$ thus allows to \textit{tune} the frequency bias, as indicated by the title of YYT23.

\section{Common Eigenfunctions for Operators $K$ and $P_s$}

The preceding analysis relies on the NTK operator $\mathcal{L}$ and the frequency preconditioning operator $P_s$ (implicit in the Sobolev norm) sharing the same eigenfunctions, namely the spherical harmonics $Y_{\ell,p}(\mathbf{x})$, when the domain is $\mathbb{S}^{d-1}$.

\begin{proposition}[Common eigenfunctions on the sphere]
Let $\mathcal{L}$ be an integral operator on $L^2(\mathbb{S}^{d-1})$ with a kernel $k(\mathbf{x}, \mathbf{y}')$ that is rotationally invariant (i.e., $k(\mathbf{x}, \mathbf{y}') = \tilde{k}(\langle \mathbf{x}, \mathbf{y}' \rangle)$). Let $P_s$ be an operator defined by its action on the spherical harmonic coefficients of a function $f = \sum \hat{f}_{\ell,p} Y_{\ell,p}$ as $P_s f = \sum (1+\ell)^{s_0} \hat{f}_{\ell,p} Y_{\ell,p}$ (where $(1+\ell)^{s_0}$ represents a polynomial factor in $\ell$, e.g., $(1+\ell)^{2s}$ for the $H^s$ norm). Then, $\mathcal{L}$ and $P_s$ share the spherical harmonics $Y_{\ell,p}$ as eigenfunctions.
\end{proposition}
\begin{proof}
1. \textbf{NTK Operator $\mathcal{L}$}: If the kernel $K^{\infty}(\mathbf{x}, \mathbf{y}')$ is rotationally invariant (as is the case for the standard NTK on the sphere, e.g., Eq. \eqref{eq:Kinf_YYT23}), then the operator $\mathcal{L}$ commutes with all rotations of the sphere. The Laplace-Beltrami operator $\Delta_{\mathbb{S}^{d_0-1}}$ on $\mathbb{S}^{d-1}$ is the generator of rotations (more precisely, its enveloping algebra). The eigenspaces of $\Delta_{\mathbb{S}^{d_0-1}}$ are precisely the spaces $\mathcal{H}^d_\ell = \text{span}\{Y_{\ell,p}\}_{p=1}^{N(d,\ell)}$ for each degree $\ell$, with eigenvalues $- \ell(\ell+d-2)$.
    By Schur's lemma (or by direct arguments about commuting operators), any operator that commutes with all rotations (and thus with $\Delta_{\mathbb{S}^{d_0-1}}$) must be a multiple of the identity on each irreducible eigenspace $\mathcal{H}^d_\ell$. Thus, $\mathcal{L} Y_{\ell,p} = \mu_\ell Y_{\ell,p}$ for a constant $\mu_\ell$ that only depends on $\ell$. The $Y_{\ell,p}$ are therefore the eigenfunctions of $\mathcal{L}$.

2. \textbf{Sobolev Operator $P_s$}: The operator $P_s$ is defined by its action in the spherical harmonic basis. For example, if $P_s = (\mathbf{I} - c \Delta_{\mathbb{S}^{d_0-1}})^{s'}$ (for $s_0 = 2s's_{laplace}$), or more directly as in the definition of the $H^s$ norm where $P_s$ acts as a multiplication of coefficients $\hat{f}_{\ell,p}$ by $(1+\ell)^{s_0/2}$ (to get $(1+\ell)^{s_0}$ in the square of the norm). In any case, its action on $Y_{\ell,p}$ is $P_s Y_{\ell,p} = \lambda_\ell(P_s) Y_{\ell,p}$, where $\lambda_\ell(P_s)$ is proportional to a power of $\ell$ (e.g., $(1+\ell)^{s_0/2}$). Thus, the $Y_{\ell,p}$ are also the eigenfunctions of $P_s$.

Since $\mathcal{L}$ and $P_s$ share the same eigenfunctions $Y_{\ell,p}$, the composite operator (or the product of eigenvalues in the learning dynamics) is simple to analyze:
\begin{equation*}
    (\mathcal{L} P_s) Y_{\ell,p} = \mathcal{L} (\lambda_\ell(P_s) Y_{\ell,p}) = \lambda_\ell(P_s) (\mathcal{L} Y_{\ell,p}) = \lambda_\ell(P_s) \mu_\ell Y_{\ell,p}
\end{equation*}
The eigenvalues of the effective operator are thus the product of the individual eigenvalues.
\end{proof}

\section{Conclusion and Discussion}
The approach of Yu, Yang, and Townsend (YYT23) provides a rigorous framework for understanding and manipulating the frequency bias of neural networks, even with non-uniform data. By relating back to a continuous analysis via quadrature rules, they show that the intrinsic spectral bias of the NTK (decay of $\mu_\ell \sim \ell^{-d}$) persists.

The introduction of a loss function based on the $H^s$ Sobolev norm allows for the modification of this bias. The effective preconditioning operator $P_s$ introduces a frequency re-weighting $\sim \ell^{2s}$. Consequently, the effective spectrum of the modified NTK behaves as $\ell^{2s-d}$. This offers direct control over how the network learns different frequencies:
\begin{itemize}
    \item $s = d/2$: Balanced learning of frequencies (flat spectrum for large $\ell$).
    \item $s > d/2$: Accelerated learning of high frequencies (reversed bias).
    \item $s < d/2$: Maintenance of a bias towards low frequencies, potentially attenuated if $s>0$.
\end{itemize}
The justification that NTK and Sobolev operators (under rotational invariance on the sphere) share spherical harmonics as eigenfunctions makes this spectral modification analysis direct and powerful. The work of YYT23 not only shows that frequency bias is a fundamental phenomenon but also that it can be actively "tuned" by the judicious choice of the loss function, opening perspectives for improving the convergence and robustness of neural networks.

\section*{References}

\begin{thebibliography}{99} % Manual bibliography, natbib warnings may occur if keys don't match cites

\bibitem{jacot2018neural}
Jacot, A., Gabriel, F., Hongler, C. (2018).
Neural tangent kernel: Convergence and generalization in neural networks.
\textit{Advances in neural information processing systems, 31}.

\bibitem{rahaman2019spectral}
Rahaman, N. S., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F., ... & Bengio, Y. (2019).
On the spectral bias of neural networks.
\textit{International Conference on Machine Learning (pp. 5301-5310). PMLR}.

\bibitem{basri2020frequency}
Basri, R., Jacot, A., Lelarge, M., Netrapalli, P., & Bach, F. (2020).
Frequency bias of neural networks for input of varying dimension.
\textit{International Conference on Machine Learning (pp. 670-680). PMLR}.

\bibitem{yu2022tuning}
Yu, A., Yang, Y., Townsend, A. (2023).
Tuning Frequency Bias in Neural Network Training with Nonuniform Data.
\textit{International Conference on Learning Representations (ICLR 2023). arXiv:2205.14300v2}.

\bibitem{bietti2019inductive}
Bietti, A., Mairal, J. (2019).
On the inductive bias of neural tangent kernels.
\textit{arXiv preprint arXiv:1905.12173}.

% References from previous draft
\bibitem{terjek2025ntk}
Terjék, D., Gonzáles-Sánchez, D. (2025). MLPs at the EOC: Spectrum of the NTK. \textit{arXiv:2501.13225v1} (as provided in user's files).

\bibitem{nguyen2021tight}
Nguyen, Q., Mondelli, M., Montúfar, G. (2021). Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks. \textit{ICML 2021 (arXiv:2006.06333)}.

\bibitem{druker2023power}
Druker, R., Falik, M., Razin, N., Shamir, O., Wagner, T. (2023). On the Power of Preconditioning in Kernel Ridge Regression: A Theoretical and Empirical Study of Sobolev Training for Neural Tangent Kernels. \textit{arXiv:2305.19380}.

\bibitem{bach2017breaking}
Bach, F. (2017). Breaking the Curse of Dimensionality with Convex Neural Networks. \textit{Journal of Machine Learning Research, 18}(19), 1-53.

\end{thebibliography}

\bibliographystyle{plainnat} % Basic style compatible with natbib and manual bibliography

\end{document}
