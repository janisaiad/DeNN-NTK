\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen-zhu2019}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97, pp.\  242--252, 2019.

\bibitem[Amari et~al.(2020)Amari, Ba, Grosse, Li, Nitanda, Suzuki, Wu, and Xu]{amari2020does}
Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny Wu, and Ji~Xu.
\newblock When does preconditioning help or hurt generalization?
\newblock \emph{arXiv preprint arXiv:2006.10732}, 2020.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  322--332. PMLR, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{arXiv preprint arXiv:1904.11955}, 2019{\natexlab{b}}.

\bibitem[Bach(2017)]{bach2017breaking}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0 (1):\penalty0 629--681, 2017.

\bibitem[Barzilai \& Shamir(2023)Barzilai and Shamir]{barzilai2023generalization}
Daniel Barzilai and Ohad Shamir.
\newblock Generalization in kernel regression under realistic assumptions.
\newblock \emph{arXiv preprint arXiv:2312.15995}, 2023.

\bibitem[Barzilai et~al.(2022)Barzilai, Geifman, Galun, and Basri]{barzilai2022kernel}
Daniel Barzilai, Amnon Geifman, Meirav Galun, and Ronen Basri.
\newblock A kernel perspective of skip connections in convolutional networks.
\newblock \emph{arXiv preprint arXiv:2211.14810}, 2022.

\bibitem[Basri et~al.(2019)Basri, Jacobs, Kasten, and Kritchman]{basri2019convergence}
Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman.
\newblock The convergence rate of neural networks for learned functions of different frequencies.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  4763--4772, 2019.

\bibitem[Basri et~al.(2020{\natexlab{a}})Basri, Galun, Geifman, Jacobs, Kasten, and Kritchman]{basri2020Nonuniform}
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{International Conference on Machine Learning}, 2020{\natexlab{a}}.

\bibitem[Basri et~al.(2020{\natexlab{b}})Basri, Galun, Geifman, Jacobs, Kasten, and Kritchman]{basri2020frequency}
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{International Conference on Machine Learning}, pp.\  685--694. PMLR, 2020{\natexlab{b}}.

\bibitem[Belfer et~al.(2021)Belfer, Geifman, Galun, and Basri]{belfer2021spectral}
Yuval Belfer, Amnon Geifman, Meirav Galun, and Ronen Basri.
\newblock Spectral analysis of the neural tangent kernel for deep residual networks.
\newblock \emph{arXiv preprint arXiv:2104.03093}, 2021.

\bibitem[Bietti \& Bach(2020)Bietti and Bach]{bietti2020deep}
Alberto Bietti and Francis Bach.
\newblock Deep equals shallow for relu networks in kernel regimes.
\newblock \emph{arXiv preprint arXiv:2009.14397}, 2020.

\bibitem[Bowman \& Montufar(2022)Bowman and Montufar]{bowman2022spectral}
Benjamin Bowman and Guido Montufar.
\newblock Spectral bias outside the training set for deep networks in the kernel regime.
\newblock \emph{arXiv preprint arXiv:2206.02927}, 2022.

\bibitem[Braun(2005)]{braun2005spectral}
Mikio~Ludwig Braun.
\newblock \emph{Spectral properties of the kernel matrix and their relation to kernel methods in machine learning}.
\newblock PhD thesis, Universit{\"a}ts-und Landesbibliothek Bonn, 2005.

\bibitem[Cagnetta et~al.(2022)Cagnetta, Favero, and Wyart]{cagnetta2022wide}
Francesco Cagnetta, Alessandro Favero, and Matthieu Wyart.
\newblock How wide convolutional neural networks learn hierarchical tasks.
\newblock \emph{arXiv preprint arXiv:2208.01003}, 2022.

\bibitem[Cai et~al.(2019)Cai, Gao, Hou, Chen, Wang, He, Zhang, and Wang]{cai2019gram}
Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di~He, Zhihua Zhang, and Liwei Wang.
\newblock Gram-gauss-newton method: Learning overparameterized neural networks for regression problems.
\newblock \emph{arXiv preprint arXiv:1905.11675}, 2019.

\bibitem[Cao et~al.(2019)Cao, Fang, Wu, Zhou, and Gu]{cao2019}
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu.
\newblock Towards understanding the spectral bias of deep learning.
\newblock \emph{arXiv preprint arXiv:2009.01198}, 2019.

\bibitem[Carlson et~al.(2015)Carlson, Collins, Hsieh, Carin, and Cevher]{carlson2015preconditioned}
David~E Carlson, Edo Collins, Ya-Ping Hsieh, Lawrence Carin, and Volkan Cevher.
\newblock Preconditioned spectral descent for deep learning.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Cho \& Saul(2009)Cho and Saul]{cho2009kernel}
Youngmin Cho and Lawrence Saul.
\newblock Kernel methods for deep learning.
\newblock In Y.~Bengio, D.~Schuurmans, J.~Lafferty, C.~Williams, and A.~Culotta (eds.), \emph{Advances in Neural Information Processing Systems}, volume~22. Curran Associates, Inc., 2009.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\  2253--2261, 2016.

\bibitem[Fridovich-Keil et~al.(2021)Fridovich-Keil, Gontijo-Lopes, and Roelofs]{fridovich2021spectral}
Sara Fridovich-Keil, Raphael Gontijo-Lopes, and Rebecca Roelofs.
\newblock Spectral bias in practice: The role of function frequency in generalization.
\newblock \emph{arXiv preprint arXiv:2110.02424}, 2021.

\bibitem[Geifman et~al.(2022)Geifman, Galun, Jacobs, and Basri]{geifman2022spectral}
Amnon Geifman, Meirav Galun, David Jacobs, and Ronen Basri.
\newblock On the spectral bias of convolutional neural tangent and gaussian process kernels.
\newblock \emph{arXiv preprint arXiv:2203.09255}, 2022.

\bibitem[Huang et~al.(2020)Huang, Wang, Tao, and Zhao]{huang2020deep}
Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao.
\newblock Why do deep residual networks generalize better than deep feedforward networks?---a neural tangent kernel perspective.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 2698--2709, 2020.

\bibitem[Ionescu et~al.(2017)Ionescu, Popa, and Sminchisescu]{ionescu2017large}
Catalin Ionescu, Alin Popa, and Cristian Sminchisescu.
\newblock Large-scale data-dependent kernel approximation.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  19--27. PMLR, 2017.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Kennedy et~al.(2013)Kennedy, Sadeghi, Khalid, and McEwen]{kennedy2013classification}
Rodney~A Kennedy, Parastoo Sadeghi, Zubair Khalid, and Jason~D McEwen.
\newblock Classification and construction of closed-form kernels for signal representation on the 2-sphere.
\newblock In \emph{Wavelets and sparsity XV}, volume 8858, pp.\  169--183. SPIE, 2013.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein, and Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under gradient descent.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Lee et~al.(2020{\natexlab{a}})Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and Sohl-Dickstein]{lee2020finite}
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15156--15172, 2020{\natexlab{a}}.

\bibitem[Lee et~al.(2020{\natexlab{b}})Lee, Shen, Song, Wang, et~al.]{lee2020generalized}
Jason~D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, et~al.
\newblock Generalized leverage score sampling for neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 10775--10787, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2019)Li, Wang, Yu, Du, Hu, Salakhutdinov, and Arora]{li2019enhanced}
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon~S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev Arora.
\newblock Enhanced convolutional neural tangent kernels.
\newblock \emph{arXiv preprint arXiv:1911.00809}, 2019.

\bibitem[Ma \& Belkin(2017)Ma and Belkin]{ma2017diving}
Siyuan Ma and Mikhail Belkin.
\newblock Diving into the shallows: a computational perspective on large-scale shallow learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{ma2018power}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: Understanding the effectiveness of sgd in modern over-parametrized learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3325--3334. PMLR, 2018.

\bibitem[Mallinar et~al.(2022)Mallinar, Simon, Abedsoltan, Pandit, Belkin, and Nakkiran]{mallinar2022benign}
Neil Mallinar, James~B Simon, Amirhesam Abedsoltan, Parthe Pandit, Mikhail Belkin, and Preetum Nakkiran.
\newblock Benign, tempered, or catastrophic: A taxonomy of overfitting.
\newblock \emph{arXiv preprint arXiv:2207.06569}, 2022.

\bibitem[Misiakiewicz \& Mei(2021)Misiakiewicz and Mei]{misiakiewicz2021learning}
Theodor Misiakiewicz and Song Mei.
\newblock Learning with convolution and pooling operations in kernel methods.
\newblock \emph{arXiv preprint arXiv:2111.08308}, 2021.

\bibitem[Mohamadi et~al.(2023)Mohamadi, Bae, and Sutherland]{mohamadi2023fast}
Mohamad~Amin Mohamadi, Wonho Bae, and Danica~J Sutherland.
\newblock A fast, well-founded approximation to the empirical neural tangent kernel.
\newblock In \emph{International Conference on Machine Learning}, pp.\  25061--25081. PMLR, 2023.

\bibitem[Montanari \& Zhong(2022)Montanari and Zhong]{montanari2022interpolation}
Andrea Montanari and Yiqiao Zhong.
\newblock The interpolation phase transition in neural networks: Memorization and generalization under lazy training.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (5):\penalty0 2816--2847, 2022.

\bibitem[Murray et~al.(2022)Murray, Jin, Bowman, and Montufar]{murray2022characterizing}
Michael Murray, Hui Jin, Benjamin Bowman, and Guido Montufar.
\newblock Characterizing the spectrum of the ntk via a power series expansion.
\newblock \emph{arXiv preprint arXiv:2211.07844}, 2022.

\bibitem[Neal(2012)]{neal2012bayesian}
Radford~M Neal.
\newblock \emph{Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Nguyen et~al.(2021)Nguyen, Mondelli, and Montufar]{nguyen2021tight}
Quynh Nguyen, Marco Mondelli, and Guido~F Montufar.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  8119--8129. PMLR, 2021.

\bibitem[Nocedal \& Wright(1999)Nocedal and Wright]{nocedal1999numerical}
Jorge Nocedal and Stephen~J Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer, 1999.

\bibitem[Novak et~al.(2022)Novak, Sohl-Dickstein, and Schoenholz]{novak2022fast}
Roman Novak, Jascha Sohl-Dickstein, and Samuel~S Schoenholz.
\newblock Fast finite width neural tangent kernel.
\newblock In \emph{International Conference on Machine Learning}, pp.\  17018--17044. PMLR, 2022.

\bibitem[Oymak \& Soltanolkotabi(2020)Oymak and Soltanolkotabi]{oymak2020toward}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 1\penalty0 (1):\penalty0 84--105, 2020.

\bibitem[Rosasco et~al.(2010)Rosasco, Belkin, and De~Vito]{rosasco2010learning}
Lorenzo Rosasco, Mikhail Belkin, and Ernesto De~Vito.
\newblock On learning with integral operators.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (2), 2010.

\bibitem[Saitoh \& Sawano(2016)Saitoh and Sawano]{saitoh2016theory}
Saburou Saitoh and Yoshihiro Sawano.
\newblock \emph{Theory of reproducing kernels and applications}.
\newblock Springer, 2016.

\bibitem[Sch{\"o}lkopf et~al.(2001)Sch{\"o}lkopf, Herbrich, and Smola]{scholkopf2001generalized}
Bernhard Sch{\"o}lkopf, Ralf Herbrich, and Alex~J Smola.
\newblock A generalized representer theorem.
\newblock In \emph{International conference on computational learning theory}, pp.\  416--426. Springer, 2001.

\bibitem[Simon(2022)]{simon2022kernel}
James~B Simon.
\newblock On kernel regression with data-dependent kernels.
\newblock \emph{arXiv preprint arXiv:2209.01691}, 2022.

\bibitem[Sindhwani et~al.(2005)Sindhwani, Niyogi, and Belkin]{sindhwani2005beyond}
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.
\newblock Beyond the point cloud: from transductive to semi-supervised learning.
\newblock In \emph{Proceedings of the 22nd international conference on Machine learning}, pp.\  824--831, 2005.

\bibitem[Song \& Yang(2019)Song and Yang]{song2019quadratic}
Zhao Song and Xin Yang.
\newblock Quadratic suffices for over-parametrization via matrix chernoff bound.
\newblock \emph{arXiv preprint arXiv:1906.03593}, 2019.

\bibitem[Steinwart \& Scovel(2012)Steinwart and Scovel]{steinwart2012mercer}
Ingo Steinwart and Clint Scovel.
\newblock Mercer’s theorem on general domains: On the interaction between measures, kernels, and rkhss.
\newblock \emph{Constructive Approximation}, 35:\penalty0 363--417, 2012.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil, Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng.
\newblock Fourier features let networks learn high frequency functions in low dimensional domains.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 7537--7547, 2020.

\bibitem[Tirer et~al.(2022)Tirer, Bruna, and Giryes]{tirer2022kernel}
Tom Tirer, Joan Bruna, and Raja Giryes.
\newblock Kernel-based smoothness analysis of residual networks.
\newblock In \emph{Mathematical and Scientific Machine Learning}, pp.\  921--954. PMLR, 2022.

\bibitem[Tsigler \& Bartlett(2023)Tsigler and Bartlett]{tsigler2023benign}
Alexander Tsigler and Peter~L Bartlett.
\newblock Benign overfitting in ridge regression.
\newblock \emph{J. Mach. Learn. Res.}, 24:\penalty0 123--1, 2023.

\bibitem[Wang et~al.(2022)Wang, Yu, and Perdikaris]{wang2022and}
Sifan Wang, Xinling Yu, and Paris Perdikaris.
\newblock When and why pinns fail to train: A neural tangent kernel perspective.
\newblock \emph{Journal of Computational Physics}, 449:\penalty0 110768, 2022.

\bibitem[Wang \& Zhu(2021)Wang and Zhu]{wang2021deformed}
Zhichao Wang and Yizhe Zhu.
\newblock Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks.
\newblock \emph{arXiv preprint arXiv:2109.09304}, 2021.

\bibitem[Williams(1997)]{williams1997computing}
Christopher~KI Williams.
\newblock Computing with infinite networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\  295--301, 1997.

\bibitem[Xiao(2022)]{xiao2022eigenspace}
Lechao Xiao.
\newblock Eigenspace restructuring: a principle of space and frequency in neural networks.
\newblock In \emph{Conference on Learning Theory}, pp.\  4888--4944. PMLR, 2022.

\bibitem[Xu et~al.(2019)Xu, Zhang, Luo, Xiao, and Ma]{xu2019frequency}
Zhi-Qin~John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma.
\newblock Frequency principle: Fourier analysis sheds light on deep neural networks.
\newblock \emph{arXiv preprint arXiv:1901.06523}, 2019.

\bibitem[Xu et~al.(2022)Xu, Zhang, and Luo]{xu2022overview}
Zhi-Qin~John Xu, Yaoyu Zhang, and Tao Luo.
\newblock Overview frequency principle/spectral bias in deep learning.
\newblock \emph{arXiv preprint arXiv:2201.07395}, 2022.

\bibitem[Yang(2019)]{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Yang(2020)]{yang2020tensor}
Greg Yang.
\newblock Tensor programs ii: Neural tangent kernel for any architecture.
\newblock \emph{arXiv preprint arXiv:2006.14548}, 2020.

\bibitem[Yang \& Salman(2019)Yang and Salman]{yang2019fine}
Greg Yang and Hadi Salman.
\newblock A fine-grained spectral perspective on neural networks.
\newblock \emph{arXiv preprint arXiv:1907.10599}, 2019.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[You et~al.(2019)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song, Demmel, Keutzer, and Hsieh]{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76 minutes.
\newblock \emph{arXiv preprint arXiv:1904.00962}, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Martens, and Grosse]{zhang2019fast}
Guodong Zhang, James Martens, and Roger~B Grosse.
\newblock Fast convergence of natural gradient descent for over-parameterized neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\end{thebibliography}
