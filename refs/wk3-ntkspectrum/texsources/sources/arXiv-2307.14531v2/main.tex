
\documentclass[10pt]{article} % For LaTeX2e
\usepackage[accepted]{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[normalem]{ulem}
\usepackage{booktabs}

\usepackage{algorithm}
\usepackage{algcompatible}
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{hyperref}
\usepackage{url}
% \usepackage[symbol]{footmisc}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newenvironment{sproof}{%
  \renewcommand{\proofname}{Proof Sketch}\proof}{\endproof}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
% vectors
\newcommand{\vv}{\mathbf{v}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\calx}{\mathcal{X}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\res}{\mathbf{r}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\alphabold}{\mathbf{a}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\kk}{\mathbf{k}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\hh}{\mathcal{H}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\g}{\mathbf{g}}
%\newcommand{\kr}{\mathrlap{^-}\boldsymbol{k}}
\newcommand{\kr}{\boldsymbol{k}}
\DeclareMathOperator*{\argmin}{arg\,min}


% remarks
\newcommand{\rb}[1]{\textcolor{magenta}{[Ronen: #1]}}
\newcommand{\ag}[1]{\textcolor{red}{[Amnon: #1]}}
\newcommand{\db}[1]{\textcolor{orange}{[Daniel: #1]}}
\newcommand{\mg}[1]{\textcolor{cyan}{[Meirav: #1]}}
\newcommand{\gencom}[1]{\textcolor{blue}{[#1]}}
%%\renewcommand{\gencom}[1]{#1}
% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\title{Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernelâ€™s Spectrum}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Amnon Geifmann\footnote[1]{Equal Contribution} \email amnon.geifmann@weizmann.ac.il \\
      \addr Weizmann Institute of Science
      \AND
      \name Daniel Barzilai\footnote[1]{} \email daniel.barzilai@weizmann.ac.il \\
      \addr Weizmann Institute of Science
      \AND
      \name Ronen Basri \email ronen.basri@weizmann.ac.il \\
      \addr Weizmann Institute of Science \\ 
      Meta AI
      \AND
      \name Meirav Galun \email meirav.galun@weizmann.ac.il \\
      \addr Weizmann Institute of Science 
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{02}  % Insert correct month for camera-ready version
\def\year{2024} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=aD0ExytnEK}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]{Equal Contribution}\renewcommand{\thefootnote}{\arabic{footnote}}\addtocounter{footnote}{-1}

\begin{abstract}
Wide neural networks are {\em biased} towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this {\em bias} according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement. 
\end{abstract}

\section{Introduction}

Recent years have seen remarkable advances in understanding the inductive bias of neural networks. Deep neural networks are biased toward learning certain types of functions. On the one hand, this bias may have a positive effect by inducing an implicit form of regularization. But on the other hand, it also implies that networks may perform poorly when the target function is not well aligned with this bias. 

A series of works showed that when the width of a neural network tends to infinity (and with a certain initialization and small learning rate), training a network with GD converges to a kernel regression solution with a kernel called the Neural Tangent Kernel (NTK) \citep{jacot2018neural, lee2019wide, allen-zhu2019, chizat2019lazy}. Subsequent work showed that the inductive bias of wide neural networks can be characterized by the spectral decomposition of NTK \citep{arora2019fine, basri2019convergence, yang2019fine}. Following this characterization, we will use the term \emph{spectral bias of neural networks} to refer to the inductive bias induced by their corresponding NTK spectrum. Specifically, it has been observed both theoretically and empirically that for a wide neural network, learning an eigen-direction of the NTK with GD requires a number of iterations that is inversely proportional to the corresponding eigenvalue \citep{bowman2022spectral, fridovich2021spectral, xu2022overview}. Thus, if this spectral bias can be modified, it could lead to accelerated network training of certain target functions. Typically, the eigenvalue of NTK decays at least at a polynomial rate, implying that many eigen-directions cannot be learned in polynomial time with gradient descent \citep{ma2017diving}. As such, modifying the spectral bias of a neural network is necessary to enable a feasible learning time, allowing learning target functions that are not well aligned with the top eigen-directions of NTK. This prompts the following motivating question:
\begin{center}
\emph{Is it possible to manipulate the spectral bias of neural networks?}
\end{center}

The spectrum of a kernel also determines the prediction function when using kernel ridge regression. Therefore, we introduce a family of kernel manipulations that enables generating a wide range of new kernels with nearly arbitrary spectra. These manipulations do not require explicitly working in feature space and are, therefore, computationally efficient.
We leverage this technique to design a kernel-based preconditioner that significantly modifies the linear training dynamics of wide neural networks with GD. This preconditioner modifies the spectral bias of the network so that the convergence speed is no longer tied to the NTK but rather to a kernel of our choice. This yields a polynomial or even exponential speedup (depending on the decay rate of the spectrum of the NTK) in the convergence time of GD. We subsequently show that the proposed accelerated convergence does not alter the network's prediction at convergence on test points.


In sum, our main contributions are: 
\begin{enumerate}
    \item We introduce Modified Spectrum Kernels (MSK's), which enable approximating the kernel matrix for many kernels for which a closed form is unknown (Section \ref{sec:spec_ker}). 
    \item We introduce preconditioned gradient descent and prove its convergence for wide neural networks (Section \ref{sec:PGD_conv}).
    \item We prove that our method is consistent, meaning that the preconditioning does not change the final network prediction on test points (Section \ref{sec:conv_analysis}).
    \item We provide an algorithm that enables a wide range of spectral bias manipulations, yielding a significant convergence speedup for training wide neural networks. We finally demonstrate this acceleration on synthetic data (Section \ref{sec:alg}).
\end{enumerate}

\section{Preliminaries and Background}\label{sec:prelim}
We consider positive definite kernels $\kr: \calx \times \calx \rightarrow \Real$ defined over a compact metric space $\calx$ endowed with a finite Borel measure $\mu$.
Given such $\kr$, its corresponding (linear) integral operator $L_{\kr}:L^2_\mu (\calx)\to L^2_\mu (\calx)$ is defined as
\begin{align*}
    L_{\kr}(f)(\x)=\int_{\z \in \calx} \kr(\x,\z) f(\z) d\mu(\z).
\end{align*}
This is a compact, positive self-adjoint operator that therefore has an eigen-decomposition of the form
\begin{align*}
    L_{\kr}(f)=\sum_{i\geq 0}\lambda_i\langle f,\Phi_i\rangle_\mu \Phi_i,
\end{align*}
where the inner product $\langle , \rangle_\mu $ is with respect to $L^2_\mu(\mathcal{X})$, and $\lambda_i,\Phi_i$ are the eigenvalues and eigenfunctions of the integral operator satisfying
\begin{align*}
    L_{\kr}(\Phi_i)=\lambda_i \Phi_i.
\end{align*}
According to Mercer's Theorem, $\kr$ can be written using the eigen-decomposition of the integral operator as
\begin{align}  \label{eq:mercer}
    \kr(\x,\z)=\sum_{i\in I} \lambda_i \Phi_i(\x) \Phi_i(\z), ~~~ \x,\z \in {\calx}.
\end{align}

Furthermore, each such kernel is associated with a unique Reproducing Kernel Hilbert Space (RKHS) $\hh_{\kr}\subseteq L^2_\mu (\calx)$ (which we denote by $\hh$ when the kernel is clear by context), consisting of functions of the form $f(\x)=\sum_{i \in I} \alpha_i \Phi_i(\x)$ whose RKHS norm is finite, i.e., $\|f\|_\hh:=\sum_{i \in I} \frac{\alpha_i^2}{\lambda_i}<\infty$. The latter condition restricts the set of functions in an RKHS, allowing only functions that are sufficiently smooth in accordance to the asymptotic decay of $\lambda_k$. 
For any positive definite kernel there exists a feature map $\phi:\mathcal{X}\to\hh$ s.t $\kr(\x,\z)=\langle \phi(\x),\phi(\z) \rangle_\hh$. As such, we may occasionally call the RKHS $\hh$ the \emph{feature space} of $\kr$.

Given training data $X=\{\x_1,...,\x_n\}$, $\x_i \in \calx$, corresponding labels  $\{y_i\}_{i=1}^n$, $y_i \in \Real$, and a regularization parameter $\gamma > 0$, the problem of \emph{Kernel Ridge Regression (KRR)} is formulated as
\begin{equation}  \label{eq:regression}
    \min_{f \in \hh}\sum_{i=1}^n(f(\x_i)-y_i)^2+ \gamma\|f\|_\hh.
\end{equation}
The solution satisfies $f(\x)=\kk_\x^T (K+\gamma I)^{-1}\y$, where the entries of $\kk_\x \in \Real^n$ are $\frac{1}{n}\kr(\x,\x_i)$, $K$ is the $n \times n$ kernel matrix with $K_{ij}=\frac{1}{n}\kr(\x_i,\x_j)$, and $\y=(y_1,...,y_n)^T$. 
% The spectral properties of a kernel uniquely define its RKHS. Namely, given a Mercer kernel $\kr(\x,\z)=\sum_{i\geq 0} \lambda_i\phi_i(\x)\phi_i(\z)$ the eigensystem $\{\lambda_i,\phi_i\}$ of the integral operator, uniquely determine the RKHS norm. This means that by manipulating the eigenvalues of the kernel, we can create a new family of kernels with , but with arbitrary eigenvalues. Of course, this is not a trivial task since we only have access to the values \rb{eigenvalues? kernel matrix?} of the kernel.


% Let $\kr(\x,\z)=\sum_{i\geq 0} \lambda_i\phi_i(\x)\phi_i(\z)$ and $ \kr_g(\x,\z)=\sum_{i\geq 0}  g(\lambda_i)\phi_i(\x)\phi_i(\z)$ be two Mercer kernels. Where $g:\Real\rightarrow [0,\infty)$ such that $\sum_ig(\lambda_i)^2<\infty $. The form of $g$ is general enough so that $\kr_g$ can be any kernel that shares the same non-zero eigenfunctions  with $\kr$. Let $K$ and $K_g$ respectively be their normalized $n\times n$ kernel matrices. Below describe a simple algorithm that enables to compute $K_g$ without even knowing the closed form of the kernel. Let $\x_1,...,\x_n\in \calx$ we do the following steps:
% \begin{enumerate}
%     \item Construct an $n\times n$ kernel matrix $K$ with $K_{ij}=\kr(\x_i,\x_j)$.
%     \item Compute the eigenvectors and eigenvalues $\{\lambda_i,\vv_i\}_{i=1}^{n}$ of $K$.
%     \item Construct a matrix $S=VDV^T$, where the $i$'th column of $V$ is $\vv_i$, and $D$ is a diagonal matrix with $D_{ii}=g(\lambda_i)/\lambda_i$ if $\lambda_i>0$ and otherwise $D_{ii}=0$.
%     \item Define $\tilde K_g:=SK$ and $\tilde \kr_g(\x_i,\x_j):=(\tilde K_g)_{ij}$.
% \end{enumerate}

% Note that $K$ and $\tilde K_g$ share the same eigenvectors, and the corresponding eigenvalues of $K$ are transformed by $g(\cdot)$.

% We next show that $\tilde K_g $ is close to $K_g$.


\section{Modified Spectrum Kernels} \label{sec:spec_ker}
Our aim in this section is to describe and analyze the construction of novel kernels for which no closed form is known. The novel kernels are constructed by directly manipulating the kernel matrix of existing kernels, which are easy to compute. The theory in this section refers to arbitrary kernels, and the connection to NTK and wide neural networks is deferred to Sec. \ref{sec:pgd_intro}.

\begin{definition} {\bf Modified Spectrum Kernel (MSK).} \label{Def:MSK}
Let $\kr(\x,\z):=\sum_{k=1}^\infty \lambda_k\Phi_k(\x)\Phi_k(\z)$ be a Mercer kernel with eigenvalues $\lambda_i$ and eigenfunction $\Phi_i(\cdot)$ and $g:\Real\to\Real$ a function which is non-negative and $L$-Lipschitz. The Modified Spectrum Kernel (w.r.t.\ $\kr$) is defined as $\kr_g(\x,\z):=\sum_{k=1}^\infty g(\lambda_k)\Phi_k(\x)\Phi_k(\z)$. 
\end{definition}

The MSK has the same eigenfunctions as the source kernel $\kr$, while its eigenvalues are modified by $g$. Clearly, constructing a closed-form solution to the modified kernel is rarely possible. However, it is possible to approximate the modified kernel efficiently under certain conditions, given the values of the original kernel $\kr$, as proved in Theorem \ref{thm:norm_conv}.

%We next show that under certain conditions, it is possible to approximate the kernel matrix of the MSK, given the values of the original kernel $\kr$.

\begin{theorem}\label{thm:norm_conv}
Let $g, \kr, \kr_g$ be as in Def. \eqref{Def:MSK} and assume that $\forall \x\in \mathcal X,\abs{\Phi_i(\x)}\leq M$. Let $K,K_g$ be the corresponding kernel matrices on i.i.d samples $\x_1,..,\x_n\in \mathcal X$. Define the kernel matrix $\tilde K_g=V  D V^T$ where $V=(\vv_1,..,\vv_n)$ with $\vv_i$ the i'th eigenvector of $K$ and $ D$ is a diagonal matrix with $ D_{ii}=g(\hat \lambda_i)$ where $\hat \lambda_i$ is the i'th eigenvalue of $K$. Then, for $n\rightarrow \infty $
\begin{align*}
\norm{\tilde K_g-K_g}_F\overset{a.s.}{\rightarrow} 0,
\end{align*}
where a.s.\ stands for almost surely.
\end{theorem}

We next provide a proof sketch. The full proof is given in Appendix \ref{appendix:engineering}. For an eigenfunction $\Phi$ of $\kr(\x,\z)$, we let $\Phi(X):=(\Phi(\x_1),\ldots,\Phi(\x_n))^T\in\Real^n$ be the evaluation of the eigenfunctions on the data. By the definition of the kernels it can be shown that $K=\sum_{k=1}^\infty\lambda_k\Phi_k(X)\Phi_k(X)^T$,  and similarly, $K_g=\sum_{k=1}^\infty g(\lambda_k)\Phi_k(X)\Phi_k(X)^T$. Since $\tilde K_g$ is composed of the eigenvectors of $K$ with the eigenvalues $g(\lambda_k)$, we would like to show that the eigenvalues of $K_g$ are close to $g(\lambda_k)$ and that the eigenvectors of $K$ are close to those of $K_g$. It is already known that the eigenvalues of a kernel matrix converge to those of its integral operator (with the suitable normalization) \citep{rosasco2010learning}, and as such, we know that the eigenvalues of $\tilde K_g$ should be close to those of $K_g$. The challenge is that the eigenvectors $\vv_k$ do not have to be close to $\Phi_k(X)$, the evaluations of the eigenfunctions on the data (for example when there is an eigenvalue of multiplicity greater than 1). This means that the eigenvectors of $\tilde K_g$ can be very different from those of $K_g$. We work around this by showing that for large $n$, the eigenvectors of $K$ corresponding to an eigenvalue $\lambda_k$ are related to an eigenfunction $\Phi_i$ as
\begin{align*}
\sum_{\vv:K\vv=\lambda_k \vv}(\vv^T\Phi_i(X))^2\underset{n\to\infty}{\longrightarrow} 
\begin{cases} 1~ \text{ if } \Phi_i \text{ is an eigenfunction of } \lambda_k\\
0 ~ \text{ else}
\end{cases}.
\end{align*}
This allows us to show that the norm between the eigenspaces of $\tilde K_g$ and $K_g$ tends to 0. \\

\noindent\textbf{Kernel Construction with MSKs}.
Generating new kernels from existing ones is a long-standing problem in kernel theory \citep{saitoh2016theory}. The classical kernel theory uses arithmetic operations such as addition and multiplication of kernels to generate new kernels with unique RKHSs. Recent papers provided tools to building data dependent kernels based on training points \citep{simon2022kernel,sindhwani2005beyond,ionescu2017large} . Nevertheless, there are still many kernels for which a closed form is unknown. 

MSKs allow extending existing RKHS theory by computing new kernels with predetermined Mercer decomposition even when a  closed form is unknown and, specifically, solve KRR with these new kernels. Suppose we would like to solve the problem of KRR as defined in \eqref{eq:regression}. Let $\kr(\x,\z)=\sum_{k=1}^\infty \lambda_k\Phi_k(\x)\Phi_k(\z)$ and $\kr_g(\x,\z)=\sum_{k=1}^\infty g(\lambda_k)\Phi_k(\x)\Phi_k(\z)$ be two Mercer kernels where $\kr(\x,\z)$ has a known closed form, whereas $\kr_g(\x,\z)$ does not. Assuming we obtain i.i.d.\ samples of training points $\x_1,..,\x_n$ and a test point $\x$, we can build $\tilde K_g$ (as in Theorem \ref{thm:norm_conv}) using the $n+1$ points $\x_1,..,\x_n,\x$. Then, by a continuity argument, Theorem  \ref{thm:norm_conv} guarantees that the predictor $f^*(\x)=[\tilde K_g]_{n+1,1:n}([\tilde K_g]_{1:n,1:n}+\gamma I)^{-1}\y$ converges to the KRR prediction with the kernel $\kr_g$, where $[~\cdot~]_{:,:}$ corresponds to the sub-matrix induced by the specified indices.



\section{Provable NTK Based Preconditioning for Neural Networks}\label{sec:pgd_intro}

In this section, we develop and analyze a preconditioning scheme for accelerating the convergence rate of gradient descent in the training phase of neural networks while minimizing the Mean Squares Error (MSE) loss. The acceleration is achieved by manipulating the spectrum of the NTK, overcoming the spectral bias of neural networks. We begin by explaining how the convergence rate of neural networks is related to the spectrum of the NTK. Then, we introduce a preconditioning scheme for wide neural networks and prove that it attains a global optimum. We further prove that in the infinite width limit and when training the network to completion, preconditioned and standard gradient descent converge to the same global minimizer.


We consider a fully connected neural network parameterized as follows:
\begin{align*}
    \g^{(0)}(\x) & = \x  \\
    \f^{(l)}(\x) & = W^{(l)}\g^{(l-1)} (\x) + \mathbf{b}^{(l)}  \in \Real^{d_l}, ~~~~~l=1,\ldots L \\
    \g^{(l)}(\x) & = 
    \rho\left(\f^{(l)}(\x)\right)\in \Real^{d_l}, ~~~~~l=1,\ldots L \\
    f(\x,\w) & = f^{(L+1)}(\x) = W^{(L+1)} \cdot \g^{(L)}(\x) + b^{(L+1)}.
\end{align*}
where $\w\in \Real^p$ is the set of all the network parameters. We select a simple architecture and note that our results can easily be extended to many other architectures. We denote by $m$ the width of the network and assume that $d_1=d_2=..=d_L=m$. The activation function is denoted by $\rho$ and the following quantities  $|\rho(0)|$, $\norm{\rho'}_\infty$, and $\sup_{x\neq x'}|\rho'(x)-\rho'(x')|/|x-x'|$ should be finite. The initialization follows the standard NTK parametrization \citep{lee2020finite} (see more details in Appendix \ref{appendix:convergence}).

We denote the vector of labels for all the data points  $(y_1, \ldots,y_n)$  by $\y\in \Real^n$ and the vector of network predictions $f(\x_i,\w)$  by $f(X,\w) \in \Real^n$.  The residual at time $t$ is $\res_t:=f(X,\w_t)-\y$. Letting $\calL$ be the squared error loss, $\calL(\w_{t})=\frac{1}{2}\norm{\res_t}^2$, a gradient descent iteration for optimizing $\w \in \Real^p$ is given by
\begin{align}\label{eq:GD_NN}
    \w_{t+1} - \w_{t} = - \eta \nabla_\w \calL(\w_{t}) = -\eta \nabla_\w f(X,\w_t)^T\res_t,
\end{align}
where $\eta$ is the learning rate, and  $\nabla_\w f(X,\w_t)\in \Real^{n\times p}$ is the Jacobian of the network. 

% In the standard machine learning settings, we look for a prediction function $f(\x,\w)$ that minimizes the empirical loss $L(\w)$, where the minimization is done over the parameters $\w\in \Real^p$. Typically, when $f$ is complicated (e.g a deep neural network), an iterative method such as gradient descent (GD) is applied. The gradient descent update is given by 
% \begin{align*}
%     \w_{t+1}=\w_{t}-\eta \nabla_\w L(\w_{t}),
% \end{align*}
% where $\w_t$ denote the parameters at time $t$, and $\eta $ is the learning rate. In particular, when $L$ is the MSE loss, we can denote the concatenation of the labels $y_i$ by $\y\in \Real^n$ and the concatenation of the predictions on the training set $f(\x_i,\w)$ by $f(X,\w)$. We also denote the residual at time $t$ by $\res_t:=f(X,\w_t)-\y$, so the loss becomes $\frac{1}{2}L(\w_{t})=\norm{\res_t}^2$ and the GD update becomes
% \begin{align*}
%     \w_{t+1} = \w_{t}-\eta \nabla_\w f(X,\w_t)^T\res_t,
% \end{align*}
% where $\nabla_\w f(X,\w_t)\in \Real^{n\times p}$ is the Jacobian of the network.

The empirical NTK matrix at time $t$, $K_t\in \Real^{n\times n}$, and the NTK matrix, $K\in \Real^{n\times n}$, are defined as 
\begin{align}\label{def:ntk}
    K_t =& \frac{1}{m}\nabla_\w f(X,\w_t)\nabla_\w f(X,\w_t)^T \\
    K =& \text{lim}_{m\rightarrow \infty}K_{0}.
\end{align}

We assume that $\lambda_{\min}(K) > 0$. A simple case where this condition is satisfied is whenever $\calx = \mathbb{S}^{d-1}$ and $\rho$ grows non-polynomially (e.g ReLU) \cite{jacot2018neural}, and also various other settings as given by \cite{oymak2020toward, wang2021deformed, nguyen2021tight, montanari2022interpolation, barzilai2023generalization}.

\cite{arora2019fine} showed that for a wide neural network with ReLU activation, small initialization, and a small learning rate, it holds that for every $\epsilon>0$, the residual at time $t$ evolves with the following linear dynamics 
\begin{align}\label{eq:linear_dynamics}
    %L(w_{t})
    \norm{\res_t}
    =& \norm{(I-\eta K)^t \y} \pm \epsilon
    = \sqrt{\sum_{i=1}^{n}(1-\eta \lambda_i)^{2t}(\vv_i^T\y)^2}\pm \epsilon,
\end{align}
where $\{\lambda_i\}_{i=1}^n$ and $\{\vv_i\}_{i=1}^n$ respectively are the eigenvalues and eigenvectors of the NTK matrix $K$. 

Eq. \eqref{eq:linear_dynamics} reveals the relation between the inductive bias of neural networks and the spectrum of the NTK matrix~\citep{basri2020frequency}. Specifically, to learn an eigen-direction $\vv_i$ of a target function within accuracy $\delta>0$, it is required that $(1-\eta \lambda_i)^t < \delta + \epsilon$. When the learning rate is sufficiently small to imply convergence, $0< \eta < \frac{2}{\lambda_1}$, the number of iterations needed is
\begin{align*}
    t>-\log(\delta+\epsilon)/\eta \lambda_i = O\left(\frac{\lambda_1}{\lambda_i}\right).
\end{align*} 
The eigenvalues and eigenvectors of $K$ depend on the data distribution and are not yet fully characterized for arbitrary distributions (e.g., \cite{basri2020Nonuniform}). In the case of a fully connected network with ReLU activation and with data points distributed uniformly on the sphere $\mathbb{S}^{d-1}$, 
the eigenvectors are discretizations of the spherical harmonics and the eigenvalues asymptotically decay as $\lambda_k\approx k^{-d}$, where $k$ is the frequency \citep{basri2020frequency,bietti2020deep}. In this scenario, learning a high-frequency eigenvector with gradient descent is computationally prohibitive, even for a low-dimension sphere. With other activation functions, the asymptotic decay of the eigenvalues might be even exponential \citep{murray2022characterizing}, yielding an infeasible computational learning cost for learning high frequency target functions. 

\subsection{Preconditioned Gradient Descent} \label{sec:PGD_conv}
To accelerate the convergence of standard gradient descent \eqref{eq:GD_NN}, we propose a \textit{preconditioned gradient descent} (PGD). The update rule of our PGD is given by
\begin{align} \label{eq:PGD}
    \w_{t+1} = \w_{t}-\eta \nabla_\w f(X,\w_t)^TS\res_t,
\end{align}
where $S\in \Real^{n\times n}$ is a preconditioning matrix that satisfies $S\succ 0$. 

Standard preconditioning techniques multiply the network's gradient from the left by a $p\times p$ matrix (where $p$ is the number of parameters in the network, usually huge). In contrast, our preconditioner multiplies the network's gradient from the right by an $n\times n$ matrix, reducing the cost per iteration from $p^2$ to $n^2$. This is significant since in the over-parameterized case $n \ll p$.


We next derive the linear dynamics of the form of \eqref{eq:linear_dynamics} for PGD. 
One of the key properties of PGD, is that carefully choosing $S$ allows modifying the dynamics of gradient descent almost arbitrarily.


\begin{theorem}\label{Thm:precond_dynamic}
Suppose assumptions \ref{assumption:1}-\ref{assumption:4} are satisfied. Let $\eta_0, \delta_0, \epsilon > 0$,  $S \in \Real^{n \times n}$ such that $S \succ 0$ and $\eta_0 < \frac{2}{\lambda_{min}(KS)+\lambda_{max}(KS)}$. Then, there
exists $N \in  \mathbb{N}$ such that for every $m \geq N$, the following holds with probability at
least $1-\delta_0$ over random initialization when applying preconditioned GD with learning rate $\eta=\eta_0/m$ 
\begin{align*}
    \res_t=(I-\eta_0 K S)^t \y \pm \xi(t), 
\end{align*}
where $\norm{\xi}_2\leq \epsilon$. 
\end{theorem}

Recall that we assume $K \succ 0$ and $S \succ 0$, so while $KS$ is not necessarily positive definite or symmetric, it has positive real eigenvalues. As such, the term $\frac{2}{\lambda_{min}(KS)+\lambda_{max}(KS)}$ is positive and defines the maximal feasible learning rate.
The formal proof of Theorem \ref{Thm:precond_dynamic} is given in Appendix \ref{appendix:convergence}, and here we give some key points of the proof.
The proof relies on Lemma \ref{lemma:conv_proof}, which shows that PGD finds a global minimum in which the weights are close to their initial values. In particular, for any iteration $t$, $K_tS\approx K_0S\approx KS$. Based on the results of Lemma \ref{lemma:conv_proof}, Theorem \ref{Thm:precond_dynamic} carefully bounds the distance $\xi(t)$ between $\res_t$ and the linear dynamics $(I-\eta_0 K S)^t\y$. 

\begin{lemma}\label{lemma:conv_proof} 
Suppose assumptions \ref{assumption:1}-\ref{assumption:4} are satisfied. For $\delta_0 > 0$, $\frac{2}{\lambda_{min}(KS)+\lambda_{max}(KS)}>\eta_0 $ and $S$ such that $S\succ 0$, there
exist $C > 0$, $N \in  \mathbb{N}$ and $\kappa > 1$, such that for every $m \geq N$, the following holds with probability at
least $1-\delta_0$ over random initialization. When applying  preconditioned gradient descent as in \eqref{eq:PGD} with learning rate $\eta=\eta_0/m$
\begin{enumerate}
    \item $\norm{\res_t}_2\leq \left(1-\frac{\eta \lambda_{min}}{3}\right)^tC$
    \item $\sum_{j=1}^t\norm{\w_j-\w_{j-1}}_2\leq \frac{3\kappa C}{\lambda_{min}}m^{-1/2}$
    \item $\sup_t \norm{(K_0-K_t)S}_F\leq \frac{6\kappa^3C}{\lambda_{min}}m^{-1/2}$,
\end{enumerate}
where $\lambda_{min}$ is the minimal eigenvalue of $KS$. 
\end{lemma}
The full proof of Lemma \ref{lemma:conv_proof} is given in Appendix \ref{appendix:convergence}.

 Theorem \ref{Thm:precond_dynamic} implies that the dynamics of preconditioned gradient descent are characterized by $KS$ instead of $K$. In particular, if $KS$ is symmetric, PGD follows the dynamics 
\begin{align}\label{eq:linear_dynamics_PGD}
    %L(w_{t})
    \norm{\res_t}
    =\norm{(I-\eta KS)^t \y} \pm \epsilon
    =\sqrt{\sum_{i=1}^{n}(1-\eta \hat \lambda_i)^{2t}(\hat \vv_i^T\y)^2}\pm \epsilon,
\end{align}
where now, in contrast to \eqref{eq:linear_dynamics}, $\hat \lambda_i,\hat \vv_i$ are the eigenvalues and eigenvectors of $KS$. The role of the preconditioner $S$ is to reduce the condition number of $K$, i.e., improving the worst-case convergence rate. Moreover, it can be chosen to accelerate the convergence rate in directions corresponding to any eigenvector of the NTK matrix. 

The results of \citet{zhang2019fast} and \citet{cai2019gram} can be viewed as a special case of \eqref{eq:linear_dynamics_PGD}. Specifically, these works characterize the convergence of an approximate second-order method (Gauss-Newton) when applied to a wide, two-layer network. In this case, the update rule is of the form 
\begin{align*}
        \w_{t+1} - \w_{t} = - \eta F(\w_t)^{-1}\nabla_\w \calL(\w_{t}),  
\end{align*}
with $F$ being the Fisher information matrix associated with the network's predictive distribution. Their derivation leads to similar convergence results as shown in \eqref{eq:linear_dynamics_PGD} but is limited to the case of $S=K_t^{-1}$. For sufficiently wide networks, $K_t$ is very close to $K$ (in spectral norm). Thus, taking $S=K^{-1}$ suffices for achieving the same convergence rates as if $S=K_t^{-1}$. Furthermore, our method requires computing the preconditioning matrix only once and is therefore more efficient. In fact, Figure \ref{fig:numeric} demonstrates that taking $S=K^{-1}$ leads to an even faster convergence.

We show in Sec. \ref{sec:alg} how $S$ can be chosen to reduce by a polynomial (or even exponential) factor the number of iterations required to converge in the directions corresponding to small eigenvalues of $K$. This is highly beneficial when the projections of the target function on the eigenvectors of $K$, $(\hat \vv_i^T\y)$ are relatively large in directions corresponding to small eigenvalues. For example, for image reconstruction or rendering, this equates to learning more efficiently the fine details,  which correspond to high frequencies in the image \citep{tancik2020fourier}. Another example arises in Physics Informed Neural Networks (PINN), which were shown to suffer significantly from the slow convergence rate in the directions corresponding to small eigenvalues \citep{wang2022and}.


% Another view of our preconditioner is that the trajectory of a precondition gradient descent is close to the trajectory of
% \begin{align}\label{eq:prec_feat_space}
%     \min_w \norm{S^{1/2}(\y-\Phi(X)^T\w)},
% \end{align} \db{This needs to be proven} where $\Phi(X)$ are the features of $X$ so that $K=\Phi(X)^T\Phi(X)$. We next show that  \eqref{eq:prec_feat_space} does not change the solution of the standard linear regression in the feature space.
% \begin{lemma}
% The minimizer of kernel ridge regression with the preconditioned loss:
% \[
% f^*(X):=\argmin_{f\in \mathcal{H}_K} \frac{1}{2}\norm{S^{1/2}(f(X,\w)-\y)}_2^2 + \frac{1}{2}\lambda \norm{f}_2^2
% \]
% satisfies:
% \[
% f^*(\x)=K(\x,X)(K+\lambda S^{-1})^{-1}\y.
% \]
% \end{lemma}
% This serves as a generalization of the standard kernel ridge regression solution, which is given by taking $S=I$. Therefore using the preconditioned loss implies changing the regularization. When taking $\lambda\to 0$, the solution becomes equivalent to that of the standard kernel ridge regression. In the infinite width limit of neural networks, training the network to completion is equivalent to kernel ridge regression with the NTK as $\lambda\to 0$. Therefore, if training the network to completion, using the preconditioned loss does not change the solution. 

\subsection{Convergence Analysis} \label{sec:conv_analysis}
In this section, we characterize the resulting prediction function of the network on unseen test points and show that PGD generates a consistent prediction function in the following sense. At convergence, the prediction function of a network trained with PGD is close to the prediction function of a network trained with standard GD. 

We start by defining a preconditioned loss, $\calL_S(f,\w)$, which modifies the standard MSE loss
\begin{equation}\label{eq:modified_loss}
\mathcal{L}_S(f, \w)=\frac{1}{2} \norm{S^{1/2}(f(X,\w)-\y)}^2_2,
\end{equation}
where $S \succ 0$ is the preconditioning matrix. An iteration of standard GD w.r.t $\calL_S(f,\w)$ yields 
\begin{equation}
    \w_{t+1}- \w_{t} = -\eta\nabla_{\w} \mathcal{L}_S(f, \w_t) = -\eta \nabla_{\w} f(X,\w_t)^T S \res_t,
\end{equation}
which is equivalent to a PGD iteration \eqref{eq:PGD} with the standard MSE loss. This has two implications. First, it implies that PGD can easily be implemented by simply modifying the loss function. Second, it enables us to analyze the prediction function generated by PGD.

Specifically, given an initialization $\w_0\in\Real^p$, let $\phi(\x):=\nabla f(\x,\w_0)$ and $h(\x,\w'):=\langle \w'
, \phi(\x)\rangle$. For simplicity, we denote $h(X,\w'
):=(h(\x_1,\w'
),\ldots,h(\x_n,\w'
))^T\in\Real^n$.

The minimizer of the kernel ridge regression objective  with respect to the preconditioned loss is defined as 
\begin{align} \label{eq:w_pgd}
\w^{*}_{\gamma}:=\argmin_{\w'\in\Real^p} \frac{1}{2}\norm{S^{1/2}(h(X,\w'
)-\y)}_2^2 + \frac{1}{2}\gamma \norm{\w'
}_2^2,
\end{align}
and the minimizer of a kernel ridge regression with respect to the standard MSE loss is defined as 
\begin{align}\label{eq:w_gd}
\w^{**}_{\gamma}:=\argmin_{\w'\in\Real^p} \frac{1}{2}\norm{(h(X,\w'
)-\y)}_2^2 + \frac{1}{2}\gamma \norm{\w'
}_2^2.
\end{align}
We denote by $\w^*$ and $\w^{**}$ the limits of $\w^{*}_{\gamma}$ and $\w^{**}_{\gamma}$, respectively, when $\gamma\to 0$. In Lemma \ref{lem:conv_analysis} we show that the two minimizers are equal, which means that the preconditioned loss does not change the prediction function in kernel regression.

For a given learning rate $\eta$ and loss $\mathcal{L}_S$, $h$ can be optimized through gradient descent with respect to the loss $\mathcal{L}_S$ via the iterations
\begin{align}\label{eq:pgd_h}
    \w'_0=0 ~~~~;~~~~ \w'_{t+1} =\w'_{t}-\eta\nabla\mathcal{L}_S(h,\w'_t).
\end{align}
We show in Lemma \ref{lem:conv_analysis} that this iterative procedure yields a prediction function that remains close to the neural network prediction throughout PGD.


\begin{lemma} \label{lem:conv_analysis}
    \begin{enumerate}
        \item Let $\w_0\in\Real^p$ and $\phi(\x):=\nabla f(\x,\w_0)$, it holds that $\w^*=\w^{**}$. 
        \item  Let $\delta_0 > 0$, $\epsilon>0$, a test point $\x\in\Real^d$ and $T>0$ number of iteration. Under the conditions of Theorem \ref{Thm:precond_dynamic}, $\exists N\in \mathbb{N}$ s.t $\forall m>N$, it holds with probability at least $1-\delta_0$ that
        \[
        \abs{h(\x,\w'_T)- f(\x,\w_T)} \leq \epsilon,
        \]
        where $\w'$ is as in \eqref{eq:pgd_h} and $f$ is optimized with PGD.
    \end{enumerate}
\end{lemma}
The full proof of Lemma \ref{lem:conv_analysis} can be found in Appendix \ref{appendix:conv_analysis}.
Point (1) is proved by deriving $\w^*_\gamma$ in closed form, i.e.,  $\w^*_\gamma = \sum_{i=1}^{n}\alpha^*_i\phi(\x_i)$, with
\[
\alpha^* = (K_0+\gamma S^{-1})^{-1}\y.
\]
This serves as a generalization of the standard kernel ridge regression solution, which is obtained by substituting $S=I$. Therefore, using the preconditioned loss yields a corresponding change of the regularization. When taking $\gamma\to 0$, the solution becomes equivalent to that of the standard kernel ridge regression problem.

Point (2) states that in the limit of infinite width, the value of $f(\cdot,\w_T)$ at a test point is very close to $h(\cdot,\w'_T)$. We note that as $\mathcal{L}_{S}$ is convex, $h(\x,\w'_T)$  approaches $h(\x,\w^*)$ for sufficiently large $T$. 

By combining these two points we obtain that, with sufficiently many iterations and large width, $f$ well approximates the solution of the standard kernel ridge regression problem, and thus of the standard non-preconditioned gradient descent on the network itself \citep{lee2019wide}.



% \begin{lemma}
% For any kernel $\kr$ and $S\succ 0$, the minimizer of kernel ridge regression with the preconditioned loss, defined as:
% \[
% f^{*}:=\argmin_{f\in \hh_{\kr}} \frac{1}{2}\norm{S^{1/2}(f(X)-\y)}_2^2 + \frac{1}{2}\gamma \norm{f}_2^2
% \]
% satisfies:
% \[
% f^*(\tilde\x)=\kr_{\tilde\x}(K+\gamma S^{-1})^{-1}\y,
% \]
% where $K$ is the $n\times n$ matrix whose entries are $[\kr(X,X)]_{ij}=\frac{1}{n}\kr(\x_i,\x_j)$ and $[\kr_{\tilde\x}]_{1,i}=\frac{1}{n}\kr(\tilde\x,\x_i)$.
% \end{lemma}
% This serves as a generalization of the standard kernel ridge regression solution, which is obtained by substituing $S=I$. Therefore, using the preconditioned loss yields a corresponding change of the regularization. When taking $\gamma\to 0$, the solution becomes equivalent to that of the standard kernel ridge regression problem. In the infinite width limit of neural networks, training the network to completion \rb{convergence?} is equivalent to kernel ridge regression with the NTK as $\gamma\to 0$. Therefore, for sufficiently large widths and if the network is trained to completion, using the preconditioned loss does not change the solution. 

\subsection{An Algorithm for Modifying the Spectral Bias} \label{sec:alg}
In light of Theorem \ref{Thm:precond_dynamic}, $S$ can be chosen to mitigate any negative effect arising from NTK. Specifically, we can modify the spectral bias of neural networks in a way that enables them to efficiently learn the eigen-directions of the NTK that correspond to small eigenvalues. To this end, we use a MSK based precondition, as outlined in the PGD algorithm \ref{alg:algorithm}.

First, given the NTK matrix $K$ of size $n \times n$, we construct a pre-conditioner from $K$ by applying a spectral decomposition, obtaining the top $k+1$ eigenvalues and eigenvectors. We denote the top $k+1$ eigenvalues by $\lambda_1 \geq \ldots \geq \lambda_k \geq \lambda_{k+1} > 0$, and their corresponding eigenvectors by $\vv_1,\ldots,\vv_k,\vv_{k+1}$. The proposed pre-conditioner is of the form 
\begin{equation}\label{eq:preconditioner}
    S = I - \sum_{i=1}^k \left(1-\frac{g(\lambda_{i})}{\lambda_i}\right) \vv_i \vv_i^T.
\end{equation}
It can be readily observed that $S$ and $K$ share the same eigenvectors, and as long as $g(\lambda_i)>0$ then $S\succ 0$ and its eigenvalues given in descending order are: 
\begin{align*}
    \left[1, \ldots, 1,\frac{g(\lambda_{k})}{\lambda_k},\ldots,\frac{g(\lambda_{1})}{\lambda_1} \right].
\end{align*}
Since $S$ and $K$ share the same eigenvectors, it holds that the eigenvalues of $KS$ are 
\begin{align*}[g(\lambda_{1}),\ldots,g(\lambda_{k}),\lambda_{k+1},\lambda_{k+2},..,\lambda_{n}].
\end{align*}
The connection between the proposed preconditioner  and Theorem \ref{thm:norm_conv} can now be clearly seen, as the product $KS$ yields a MSK matrix, and therefore approximates a kernel that shares the same eigenfunctions of  $\kr$ and its eigenvalues are modified by $g$ (where for $\lambda \geq \lambda_{k+1}$ we let $g(\lambda)=\lambda$). Together with Theorem \ref{Thm:precond_dynamic}, the dynamics of the neural network are controlled by this modified kernel, and as such we obtain a way to alter the spectral bias of neural networks. From Eq. \eqref{eq:linear_dynamics_PGD}, we obtain the following corollary, stating that the top $k+1$ eigenvectors $\vv_i$ of $K$ can be learnt in $O(1)$ time.

\begin{algorithm}[tb]
\caption{Preconditioned Gradient descent}\label{alg:algorithm}
\begin{algorithmic}
\REQUIRE $X,\y,f(\x,\w),K,\epsilon,g(\cdot),w_0$
\STATE Decompose $K\gets V^TDV$
\STATE Define $S \gets I - \sum_{i=1}^k \left(1-\frac{g(\lambda_{i})}{\lambda_i}\right) \vv_i \vv_i^T$
\STATE $t\gets 0$
\WHILE{$\norm{\res_t}>\epsilon$}
\STATE $\w_{t+1}\gets \w_{t}-\eta \nabla_\w f(X,\w_t)^TS\res_t$
\STATE $t \gets t+1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{corollary}
    Let $g(\lambda_i)=\lambda_{k+1}$, $1 \le i \le k$, and $S$ be given by \eqref{eq:preconditioner} (making the top $k+1$ eigenvalues of $KS$ equal to each other). Under the conditions of Theorem \ref{Thm:precond_dynamic}, by picking the learning rate $\eta=\frac{2}{\lambda_{k+1}+\lambda_{n}}$, $\exists N\in \mathbb{N}$ s.t $\forall m>N$, it holds with probability at least $1-\delta_0$ that for every $i\leq k+1$, $\vv_i$ can be learnt (up to $\epsilon$ error) in $O(1)$ time (number of iterations).
\end{corollary} 

This starkly contrasts the typical spectral bias of neural networks, under which for large $i$, learning $\vv_i$ may be infeasible in polynomial time. For example, when the activation function $\rho$ is tanh, \citet{murray2022characterizing} showed that $\lambda_k=O\left(k^{-d+1/2}e^{-\sqrt{k}}\right)$. Applying \eqref{eq:linear_dynamics}, vanilla GD requires $O\left(k^{d+1/2}e^{\sqrt{k}}\right)$ iterations to converge, exponentially slower in $k$ compared to PGD.

Another natural choice for $g$, is given some $\tilde{\gamma}>0$, to let $g(\lambda) := \lambda + \tilde{\gamma}$ and $k=n$, implying that $KS=K+\tilde{\gamma}I$. Thus, we obtain a direct correspondence between PGD with such a choice of $g$, and \emph{regularized} kernel regression. Such a choice of $g$ may help prevent overfitting, even without early stopping. This is demonstrated in Figure \ref{fig:g_comparison}. 

The NTK matrix may be ill-conditioned with the smallest eigenvalues very close to 0. This issue is magnified by the fact that some approximation may be needed when computing the NTK for arbitrary architectures \cite{novak2022fast, mohamadi2023fast}. However, a small $k$ can help avoid numerical instabilities in the preconditioner by only modifying sufficiently large eigenvalues. Furthermore, since only the top $k$ eigenvalues and eigenvectors need to be calculated, choosing a small $k$ allows for a more efficient calculation of $S$. By contrast, choosing $k=n$ implies that $S=K^{-1}$. We leave the choice of $k$, the number of modified eigenvalues, to the practitioners, but in general, one should think of $k$ as providing a trade-off between the worst-case rate of convergence, and computational stability and efficiency. 

Although our method involves computing the preconditioner, it still results in more efficient optimization in the over-parameterized regime. To see this, we note that for $L>2$, if the width $m$ of every layer is at least $\sim n^q$ for some $q>0$, then the number of parameters of a fully-connected network is $\Omega(n^{2q})$. This implies that the worst-case number of operations for GD to converge is $\omega\left(\frac{n^{2q}}{\lambda_{min}(K)}\right)$ where ${\lambda_{min}(K)}$ is the minimal eigenvalue of $K$. For a fully connected ReLU network, upper bounds on $\lambda_{\min}(K)$ decay polynomially in $n$ \citep{barzilai2023generalization}, implying that the complexity is $\omega(n^{2q+1})$. Under the most general assumptions, \citet{song2019quadratic} achieved $q=4$. In this case, we have a clear computational advantage since inverting $K$ or calculating its eigenvalues costs $O(n^3)$. Nevertheless, under stricter assumptions, smaller widths than $q=4$ may suffice, but our method is still more efficient even for linear width ($q=1$).
% Moreover, even for $q=1$ inverting $K$ or calculating its eigenvalues ($O(n^3)$ operations) does not reside in any computational overhead} 
% To see this, previous work showed that the networks' width in the NTK regime should generally exceed $m\sim n^4$ \citep{song2019quadratic}. Therefore, in this regime inverting $K$ or calculating its eigenvalues ($O(n^3)$ operations) is cheaper than computing the gradient ($O(n^4)$ operations). 
Our method is also significantly more efficient than a standard preconditioner matrix whose size is quadratic in the number of parameters.
% $\sim L^2m^2\sim L^2n^8$.
Furthermore, our preconditioner needs to be computed only once, so its computational complexity is unrelated to the number of iterations needed to converge, which as already discussed, can be exponential. Lastly, in practice, one may choose $k$, the number of eigenvalues to modify, to be small, leading to further speedups.

% In practice, it is not always possible to use wide enough networks for the linear dynamics to occur. Therefore, in our method, we allow to use $K_0=\langle\nabla_w f(x_i,w_0),\nabla_w f(x_j,w_0)\rangle $ and  $K_t=\langle\nabla_w f(x_i,w_t),\nabla_w f(x_j,w_t)\rangle $  as an input to our preconditioning scheme. This might increase the cost per iteration but, on the other hand, better account to the network dynamics during training. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.50\textwidth]{figures/bsize256.png}\\
    \includegraphics[width=0.23\textwidth]{figures/k5bsize256.png}
    \includegraphics[width=0.23\textwidth]{figures/k7bsize256.png}
    \includegraphics[width=0.23\textwidth]{figures/k10bsize256.png}
    \includegraphics[width=0.23\textwidth]{figures/k12bsize256.png}
    \caption{\small Numerical validation. Top: The number of iterations required to learn different Fourier components as a function of frequency. Standard SGD is shown in blue, and (stochastic) PGD with a preconditioner derived from the NTK matrix $K$ and the empirical NTK $K_t$ respectively are shown in green and orange. Bottom: Training curves with four different frequencies ($k=5,7,10,12$). The graphs show the MSE loss as a function of iteration number with stochastic GD and PGD.}
    \label{fig:numeric}
\end{figure}


\begin{figure}[tb]
    \centering
    \includegraphics[width=0.50\textwidth]{figures/benign_overfitting_new.png}
    \caption{The effect of various MSKs on overfitting to noise. Choosing an MSK with a slower eigenvalue decay helps prevent overfitting. Starting from a Laplace kernel, for each choice of $g$, we perform unregularized kernel regression (with $\gamma=0$) using an MSK as defined in \ref{Def:MSK}. The target function is identically $0$, with i.i.d Gaussian noise added to the training set ($y_i\sim \mathcal{N}(0,1)$). The $x$ axis denotes the number of samples, and $y$ axis the MSE on the test set (for which the target is $0$). }
    \label{fig:g_comparison}
\end{figure}



\textbf{Experiments}. 
We validated our results numerically by tracking the speed of convergence for a neural network both with and without a preconditioner. We generated inputs from a uniform distribution on the circle, $\x_i\sim U(\mathbb{S}^1)$. With each input we associated a target value $y_i$ by taking a Fourier component of frequency $k$ (we repeated this experiment with different values of $k$), i.e., if $\x_i=(\cos(\theta),\sin(\theta))^T$ then $y_i=\sin( k \theta)$. Note that under the uniform distribution on the circle, the Fourier components are the eigenfunctions of the integral operator of 
NTK for fully connected networks \citep{basri2019convergence}. We then trained a fully connected network to regress the target function. For efficiency, we trained the network with stochastic gradient descent (SGD). The preconditioner $S$ is chosen to be either $I_n$ (no preconditioning), $K^{-1}$ (inverse NTK matrix), or $K_t^{-1}$ (inverse empirical NTK matrix). Figure \ref{fig:numeric} shows that without preconditioning, the number of standard GD iterations is roughly quadratic in the frequency of the target function and quickly reaches the iteration limit. In contrast, using preconditioners based on either $K$ or $K_t$ boosts the convergence, yielding a near-constant number of iterations, irrespective of the frequency of the target function. 
It can be seen that without preconditioning, the number of standard GD iterations is roughly quadratic in the frequency of the target function and quickly reaches the iteration limit. In contrast, using preconditioners based on either $K$ or $K_t$ boosts the convergence, yielding a near constant number of iterations, irrespective of the frequency of the target function. The lower panel in Figure \ref{fig:numeric} also shows a convergence plot for a few different functions.

Furthermore, we analyze the \emph{variance} of kernel regression under different choices of MSKs in Figure \ref{fig:g_comparison}. The error of kernel regression is often decomposed into a bias and variance term, where the variance is the error when training on labels given by a constant $0$ target function with noise added to the training set \citep{tsigler2023benign}. In our experiment, starting from a Laplace kernel, for each choice of $g$, we perform unregularized kernel regression (with $\gamma=0$) using an MSK as defined in \ref{Def:MSK}. We take inputs drawn uniformly from $\mathbb{S}^2$ and  $y_i\sim \mathcal{N}(0,1)$. The $y$ axis depicts the MSE over a test set of $1000$ samples (where the test set target labels are $0$), averaged over $25$ trials. Various works suggest that a slower eigenvalue decay helps induce an implicit regularization in the kernel, helping it to avoid overfitting \citep{mallinar2022benign, tsigler2023benign}. We can observe this phenomenon in Figure \ref{fig:g_comparison}, where the MSKs with a slower eigenvalue decay achieve a smaller variance.

% \begin{figure}[tb]
%     \centering
%     \includegraphics[width=0.50\textwidth]{figures/bsize256.png}
%     \caption{\small figure}
%     \label{fig:numeric}
% \end{figure}



\section{Related Works}
A long string of works established the connection between kernel regression and neural networks. Early works considered a Bayesian inference setting for neural networks, showing that randomly initialized networks are equivalent to Gaussian processes \citep{williams1997computing,neal2012bayesian}. In this direction, \citet{cho2009kernel} introduced the Arc-cosine kernel, while \citet{daniely2016toward} studied general compositional kernels and its relation to neural networks. Recently, a family of kernels called the Neural Tangent Kernels (NTK) was introduced \citep{jacot2018neural,allen-zhu2019,arora2019exact}. These papers proved that training an over-paramterized neural network is equivalent to using gradient decent to solve kernel regression with NTK. Follow-up works defined analogous kernels for residual networks \citep{huang2020deep}, convolutional networks \citep{arora2019exact,li2019enhanced}, and other standard architectures \citep{yang2020tensor}.

Subsequent work used the NTK theory to characterize the spectral bias of neural networks. \citet{bach2017breaking, basri2019convergence, cao2019, bietti2020deep, xu2019frequency} have studied the eigenfunctions and eigenvalues of NTK under the uniform distribution and further showed that fully connected neural networks learn low-frequency functions faster than higher-frequency ones. \citet{basri2020Nonuniform} further derived the eigenfunctions of NTK with non-uniform data distribution. \citet{yang2019fine, misiakiewicz2021learning} analyzed the eigenvalues of NTK over the Boolean cube. More recent studies investigated this bias with other network architectures, including fully-connected ResNets \citep{belfer2021spectral, tirer2022kernel}, convolutional networks \citep{geifman2022spectral,cagnetta2022wide, xiao2022eigenspace}, and convolutional ResNets \citep{barzilai2022kernel}. \citep{murray2022characterizing} characterized the spectrum of NTK using its power series expansion.


% Several previous works have analyze the connection between neural networks under the "lazy regime" and kernel regressors \cite{allen-zhu2019,arora2019exact,du2019gradient,jacot2018neural,lee2019wide} showing that for highly over-parametrized Neural Netowks the training dynamics of SGD is close to training dynamic of kernel gradient decent. Followup works have shown that this connection imply a spectral bias in over-parametrized Neural Networks \cite{bach2017breaking,basri2020Nonuniform,basri2019convergence}, meaning that low frequency will be learnt before higher ones. A few recent works have formalized this spectral bias for Convolutional architectures \cite{geifman2022spectral,barzilai2022kernel}.

Preconditioning is a widely used approach to accelerating convex optimization problems. A preconditioner is typically constructed using the inverse of the Hessian or its approximation \citep{nocedal1999numerical} to improve the condition number of the problem. Recent works tried to improve the condition number of kernel ridge regression. \citet{ma2017diving,ma2018power} suggested using a left preconditioner in conjunction with Richardson iterations. Another line of work analyzed the speed of convergence of an approximate second order method for two-layer neural networks  \citep{zhang2019fast,cai2019gram}. They showed that natural gradient descent improves convergence in a factor proportional to the condition number of the NTK Gram matrix.

Several studies aim to construct preconditioners for neural networks. \citet{carlson2015preconditioned} built a diagonal preconditioner based on Adagrad and RMSProp updates. Other heuristic precondtioiners use layer-wise adaptive learning rates \citep{you2017large,you2019large}. In this line of work, it is worth mentioning \citep{lee2020generalized}, who used the convergence results of \cite{arora2019exact} to incorporate leverage score sampling into training neural networks. \citet{amari2020does} studied the generalization of overparameterized ridgeless regression under a general class of preconditioners via the bias-variance trade-off.

Past studies explored the concept of data-dependent kernels. \citet{simon2022kernel} suggested using the posterior of the target function to produce a new type of data-dependent kernel. \citet{sindhwani2005beyond} studied a wide class of kernel modifications conditioned on the training data and explored their RKHS. \citet{ionescu2017large} extended this method to build a new class of kernels defined by multiplying the random feature approximation with a weighted covariance matrix. \citet{kennedy2013classification} constructed a new family of kernels for a variety of data distributions using the Spherical Harmonic functions on the 2-Sphere.

\section{Conclusion}
% Our paper has considered the problem of manipulating the spectral bias of neural networks. We formulated a unique preconditioning scheme that enables to manipulate the speed of convergence in each eigen-direction of the NTK. We showed that for large large number of iterations, this scheme doesn't change the final network's prediction. We further explored the limiting kernel generated by using a specific family of preconditioners we called the MSK. We showed that in the limit of large data-set, this limit is consistent. Lastly we derive a piratical algorithm to perform the preconditioning and studied its results on synthetic data. On future work we plan to explore other spectral manipulation and explore the applicability of our method to real world scenarios. 

In this work we addressed the problem of manipulating the spectral bias of neural networks. We formulated a unique preconditioning scheme which enables manipulating the speed of convergence of gradient descent in each eigen-direction of the NTK. This preconditioning scheme can be efficiently implemented by a slight modification to the loss function.  Furthermore, for sufficient training time and width, we showed that the predictor obtained by standard GD is approximately the same as that of PGD. We also showed how to construct novel kernels with nearly arbitrary eigenvalues through the use of kernel spectrum manipulations. Our theory is supported by experiments on synthetic data. In future work we plan to explore other forms of spectral manipulations and apply our method in real-world scenarios.


% \subsubsection*{Author Contributions}
% If you'd like to, you may include a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors. Only add
% this information once your submission is accepted and deanonymized. 

\subsubsection*{Acknowledgments}
Research at the Weizmann Institute was partially supported by the Israel Science Foundation, grant No. 1639/19, by the Israeli Council for Higher Education (CHE) via the Weizmann Data Science Research Center, by the MBZUAI-WIS Joint Program for Artificial Intelligence Research and by research grants from the Estates of Tully and Michele Plesser and the Anita James Rosen and Harry Schutzman Foundations.

\bibliography{main}
\bibliographystyle{tmlr}

\newpage
\appendix
\section{Convergence of PGD}\label{appendix:convergence}
In this section we prove Theorem \ref{Thm:precond_dynamic}. We begin with assumptions and notations for the proof. The assumptions follows those of \citet{lee2019wide}:
\begin{enumerate}
    \item \label{assumption:1} The width of the hidden layers are identical and equal to $m$
    \item The analytic NTK $K$ is full rank with $\lambda_{min}(K)>0$. %We set $\eta_{critical}=\frac{2}{\lambda_{min}(K)+\lambda_{max}(K)}$
    \item \label{assumption:3} The training set $\{\x_i,y_i\}_{i=1}^n$ is contained in some compact set.
    \item \label{assumption:4} The activation function $\rho$ satisfies
    \begin{align*}
        |\rho(0)|,~\norm{\rho'}_\infty, ~sup_{x\neq x'}|\rho'(x)-\rho'(x')|/|x-x'|<\infty  
    \end{align*}
\end{enumerate}
The network parametrization is as follows
\begin{align*}
    \g^{(0)}(\x) & = \x  \\
    \f^{(l)}(\x) & = W^{(l)}\g^{(l-1)} (\x) + \beta \mathbf{b}^{(l)}  \in \Real^{d_l}, ~~~~~l=1,\ldots L \\
    \g^{(l)}(\x) & = 
    \rho\left(\f^{(l)}(\x)\right)\in \Real^{d_l}, ~~~~~l=1,\ldots L \\
    f(\x,\w) & = f^{(L+1)}(\x) = W^{(L+1)} \cdot \g^{(L)}(\x) + b^{(L+1)}
\end{align*}
The network parameters $\w$ include $W^{(L+1)},W^{(L)},...,W^{(1)}$, where $W^{(l)} \in \Real^{d_l \times d_{l-1}}$, $\mathbf{b}^{(l)}\in \Real^{d_l\times 1}$, $W^{(L+1)} \in \Real^{1 \times d_L}$, $b^{(L+1)}\in \Real$, $\rho$ is the activation function. The  network parameters are initialized with ${\cal N}(0,\frac{c_\rho}{d_l})$ for $c_{\rho} = 1/\left( \mathbb{E}_{z \sim \mathcal{N}(0,1)} [\rho(z)^2] \right) $, except for the biases $\{\mathbf{b}^{(1)}, \ldots,\mathbf{b}^{(L)},b^{(L+1)}\}$, which are initialized with ${\cal N}(0,c_\rho)$. We further set at the last layer $c_\rho=\nu$ for a small constant $\nu$.

For the ease of notation we use the following short-hands:
\begin{equation}\label{eq:app_notationsF}
    f(\w_t) = f(X,\w_t)\in \Real ^n
\end{equation}
\begin{equation}\label{eq:app_notationsG}
   \res_t = f(X,\w_t) - Y \in \Real ^n
\end{equation}
\begin{equation}\label{eq:app_notationsJ}
    J(\w_t) = \nabla f(X,\w_t)\in \Real ^{n\times p}
\end{equation}
The empirical and analytical NTK is defined as 
\begin{align*}
    K_t &:= \frac{1}{m}J(\w_t)J(\w_t)^T\\
    K &:= lim_{m\rightarrow \infty } K_0
\end{align*}
Since $ f(X,\w_0)$ converge in distribution to gaussian process with zero mean, we denote by $R_0$ the constant such that with probability at least $(1-\delta_0)$ over the random initialization
\begin{align}\label{eq:g_init}
    \norm{\res_t}\leq R_0
\end{align}
Likewise we have that with the same probability $\norm{f(X,\w_0)}\leq \nu C_0 $ for a constant $C_0$. 

Finally, our PGD updates are
\begin{equation}\label{eq:pgd_updates}
    \w_{t+1}=\w_t-\eta J(\w_t)^TS\res_t
\end{equation}
We let $\lambda_{\min}:=\lambda_{\min}(KS)$. Our proofs are carried out for a fixed preconditioner $S$ which does not depend on $t$. However, the same proofs would hold for $S_t$ which changes with $t$, if one makes the additional assumption that $\norm{S_t}_F$ is uniformly bounded for all $t$. Of course, if $S$ is fixed, then $\norm{S}_F$ is finite and thus trivially bounded. This is used in Lemma $\ref{lemma:precond_jac}$ and we will make its use clear.

The bounded dataset of Assumption \ref{assumption:3} is used for simplicity in two places and can in both cases, be weakened. The assumption is first used to ensure that the kernel has a Mercer decomposition, as in Eq. \eqref{eq:mercer}. Weaker assumptions (which are slightly more complicated) can be used in this case \citep{steinwart2012mercer}. For example, one has a Mercer decomposition if $X$ is a Hausdorff space (any metric space suffices), the kernel function is continuous, $\mu$ is a probability measure, and the function $h(x):=k(x,x)$ has finite L2 norm. The boundedness assumption is also used to bound $f$ at initialization. However, even for many unbounded distributions, $f$ can be bounded with high probability. As such, both requirements are satisfied by many unbounded distributions, such as Gaussians, and our statements can thus be naturally extended to many such unbounded distributions.

\begin{lemma}\label{lemma:precond_jac} There is a $\kappa > 0$ such that for every $C > 0$,
with high probability over random initialization the following holds $\forall \w,\tilde \w \in B(\w_0,C m^{-1/2})$:
\begin{enumerate}
    \item $\frac{1}{\sqrt{m}}\norm{(J(\w)-J(\tilde\w))S}_F\leq \kappa\norm{\w-\tilde \w}$
    \item $\frac{1}{\sqrt{m}}\norm{J(\w)S}_F\leq \kappa$
\end{enumerate}
\end{lemma}
\begin{proof}
Using sub-multiplicativity of the Frobenius norm,
\begin{align*}
    &\frac{1}{\sqrt{m}}\norm{(J(\w)-J(\tilde\w))S}_F \\
    \leq & \frac{1}{\sqrt{m}}\norm{(J(\w)-J(\tilde\w))}_F\norm{S}_F\\
    \leq & ^{(1)} \norm{S}_F \tilde\kappa\norm{\w-\tilde \w}=: \kappa \norm{\w-\tilde \w}
\end{align*}
where $^{(1)}$ is given by Lemma (1) of \citet{lee2019wide} and $\kappa:=\tilde \kappa \norm{S}_F$ which is constant since $\norm{S}_F$ is bounded. The second part of the theorem can be shown using the same two arguments.  
\end{proof}
% \ag{Note that $\eta_0$ needs to be depend now on the eigenvalues of $K S$ and not only of $K $} \mg{Note that although $K \succ 0$ and $S \succ 0$ it does not follow directly that $K S \succ 0$. In particular, $K S$ is not necessarily symmetric, and therefore it is not clear why its eigenvalues are real. We need to clarify that $eigs(K S) = eigs(S^{1/2} K S^{1/2})$ and this is due to the spectral decomposition $S^{1/2} K S^{1/2}) = U\Lambda U^T$. Now, we multiply from the right by $S^{1/2}$ and from the left by $S^{-1/2}$ and this yields $K S = S^{-1/2}U\Lambda U^TS^{1/2}$. Indeed, $S^{-1/2}U U^TS^{1/2}=I$}

% \begin{lemma} \label{thm:conver_prec}
% For $\delta_0 > 0$ and $\eta_0 < \eta_{critical}$ and $S$ such that $S\succ 0$, there
% exist $R_0 > 0$, $N \in  \mathbb{N}$ and $\kappa > 1$, such that for every $m \geq N$, the following holds with probability at
% least $1-\delta_0$ over random initialization that when applying  preconditioned gradient descent as in \eqref{eq:pgd_updates} with learning rate $\eta=\eta_0/m$
% \begin{enumerate}
%     \item $\norm{\res_t}_2\leq \left(1-\frac{\eta_0\bar \lambda_{min}}{3}\right)^tR_0$
%     \item $\sum_{j=1}^t\norm{\w_j-\w_{j-1}}_2\leq \frac{3\kappa R_0}{\bar \lambda_{min}}m^{-1/2}$
%     \item $\sup_t \norm{(K_0-K_t)S}_F\leq \frac{6\kappa^3R_0}{\bar \lambda_{min}}m^{-1/2}$
% \end{enumerate}
% where $\bar \lambda_{min}$ is the minimal eigenvalue of $K\cdot S $
% \end{lemma}
\begin{lemma}(Lemma \ref{lemma:conv_proof} from the paper) %\label{lemma:conv_proof} 
For $\delta_0 > 0$, $\frac{2}{\lambda_{min}(KS)+\lambda_{max}(KS)}>\eta_0 $  and $S$ such that $S\succ 0$, there
exist $C > 0$, $N \in  \mathbb{N}$ and $\kappa > 1$, such that for every $m \geq N$, the following holds with probability at
least $1-\delta_0$ over random initialization. When applying  preconditioned gradient descent as in \eqref{eq:PGD} with learning rate $\eta=\eta_0/m$
\begin{enumerate}
    \item $\norm{\res_t}_2\leq \left(1-\frac{\eta \lambda_{min}}{3}\right)^tC$
    \item $\sum_{j=1}^t\norm{\w_j-\w_{j-1}}_2\leq \frac{3\kappa C}{\lambda_{min}}m^{-1/2}$
    \item $\sup_t \norm{(K_0-K_t)S}_F\leq \frac{6\kappa^3C}{\lambda_{min}}m^{-1/2}$,
\end{enumerate}
where $\lambda_{min}$ is the minimal eigenvalue of $KS$.
\end{lemma}
\begin{proof}
%$C=3\kappa R_0/\bar \lambda_{min}$ \db{What is $C$? It isn't used in the proof} where
We begin by proving parts (1) and (2) in the theorem with  $\kappa$ defined by Lemma \ref{lemma:precond_jac}. We do that by induction where the base case for $t=0$ is clear. For $ t>0$ we have that
\begin{align*}
    \norm{\w_{t+1}-\w_t}\leq \eta \norm{J(\w_t)S}_{op}\norm{\res_t}_2\leq \left(1-\frac{\eta_0 \lambda_{min}}{3}\right)^tR_0 \kappa \eta_0 m^{-1/2}
\end{align*}
Implying that 
\begin{align*}
    \sum_{j=0}^{t-1}\norm{\w_{j+1}-\w_j}\leq  R_0 \kappa \eta_0 m^{-1/2} \sum_{j=0}^{t-1}\left(1-\frac{\eta_0\lambda_{min}}{3}\right)^{j}\leq \frac{3\kappa R_0}{\lambda_{min}}m^{-1/2}
\end{align*}
Which concludes part (2) of the theorem. 

For part (1) we use the mean value theorem to get that 
\begin{align*}
    \norm{\res_{t+1}}&=\norm{\res_{t+1}-\res_t+\res_t}\\
    =&\norm{J(\tilde \w_t)(\w_{t+1}-\w_t)+\res_t}=\norm{\eta J(\tilde \w_t)J(  \w_t)^T S \res_t+\res_t }\\
    \leq & \norm{I-\eta J(\tilde \w_t)J(  \w_t)^T S}_{2}\norm{\res_{t}}\\
    \leq& \norm{I-\eta J(\tilde \w_t)J(  \w_t)^T S}_{2} \left(1-\frac{\eta_0 \lambda_{min}}{3}\right)^tR_0
\end{align*}
where $\tilde \w_{t}$ is some point on the line between $\w_{t+1}$ and $\w_{t}$. It remains to show that with probability at least $(1-\delta_0/2)$ it holds that 
\begin{align*}
    \norm{I-\eta J(\tilde \w_t)J(  \w_t)^T S}_{2}\leq \left(1-\frac{\eta_0 \lambda_{min}}{3}\right)
\end{align*}
We show that by observing that $S$ doesn't change the convergence in  probability so $K_0 S \rightarrow K S $ as in \citet{yang2019scaling}, therefore we can choose large enough $m$ such that
\begin{align*}
    \norm{K S-K_0 S}_F\leq \frac{\eta_0  \lambda_{min}}{3}
\end{align*}
Therefore we get
\begin{align*}
    &\norm{I-\eta J(\tilde \w_t)J(  \w_t)^T S}_{2}\\
    &\leq \norm{I-\eta_0K S}_{2} + \eta_0 \norm{(K-K_0)S}_{2}
    + \eta \norm{(J(\w_0)J(\w_0)^T-J(\tilde \w_t)J(\w_t))S}_{2}\\
    &\leq (1-\eta_0 \lambda_{min}+\frac{\eta_0 \lambda_{min}}{3} +2\eta_0\kappa^2\frac{3\kappa R_0}{ \lambda_{min}}m^{-1/2}\leq 1-\frac{\eta_0 \lambda_{min}}{3}
\end{align*}
which concludes part (1).

For part (3) we verify that using Lemma \ref{lemma:precond_jac}
\begin{align}\label{eq:prec_proof_transition}
    &\norm{(K_0-K_t)S}_F=\frac{1}{m}\norm{(J(\w_0)J(\w_0)^T-J(\w_t)J(\w_t)^T)S}_F\\
    &\leq \frac{1}{m}\left(\norm{J(\w_0)S}_{2}\norm{J(\w_0)^T-J(\w_t)^T}_F
    +\norm{J(\w_t)-J(\w_0)}_{2}\norm{J(\w_t)^TS}_F\right)\\
    &\leq 2\kappa^2\norm{\w_0-\w_t}_2\leq \frac{6\kappa^3R_0}{ \lambda_{min}}{m^{-1/2}}
\end{align}
\end{proof}
Next we prove the preconditioned dynamics given in \ref{Thm:precond_dynamic}
\begin{theorem}(Theorem \ref{Thm:precond_dynamic} from the paper) %\label{Thm:precond_dynamic}
Let $\delta_0 > 0$, $\frac{2}{\lambda_{min}(KS)+\lambda_{max}(KS)}>\eta_0 $ , $\epsilon>0$ and $S$ such that $S\succ 0$. Then, there
exists $N \in  \mathbb{N}$ such that for every $m \geq N$, the following holds with probability at
least $1-\delta_0$ over random initialization when applying preconditioned gradient descent with learning rate $\eta=\eta_0/m$ 
\begin{align*}
    \res_t=(I-\eta_0 K S)^t \y \pm \xi(t), 
\end{align*}
where $\norm{\xi}_2\leq \epsilon$. 
\end{theorem}
\begin{proof}
By using the mean value theorem we have that
\begin{align*}
    \res_{t+1}&=\res_{t+1}+\res_{t}-\res_{t} =J(\tilde \w )(\w_{t+1}-\w_t)+\res_{t}\\
    =^{(1)}&-\eta J(\tilde \w ) J(\w_t)^T S \res_t+\res_{t}
    =(I-\eta J(\w_t)J(\w_t)^T S)\res_t-\eta(J(\tilde \w )-J(\w_t))J(\w_t)^T S \res_t\\
    =^{(2)}&(I-\eta_0 K S)\res_t
    -\underbrace{\eta_0((K_t-K)S)\res_t-\eta(J(\tilde \w )-J(\w_t))J(\w_t)^T S \res_t}_{\epsilon(t)}
\end{align*}
where $(1)$ is by the gradient descent definition and $(2)$ is by $K_t$ definition.

Now applying the theorem inductively we get 
\begin{align*}
    \res_{t+1}=(I-\eta_0 K S)^t \res_0 + \underbrace{\sum_{i=1}^t (I-\eta_0 K S)^i\epsilon(t-i)}_{\xi(t)}
\end{align*}
Next we bound the norm of $\epsilon(t)=\underbrace{\eta_0(K_t-K)S\res_t}_A-\underbrace{\eta(J(\tilde \w )-J(\w_t))J(\w_t)^TS \res_t}_B$. 
For term $A$ we have
\begin{align*}
    \norm{\eta_0(K_t-K)S\res_t}_2 &\leq \eta_0( \norm{(K_t-K_0)S}_{op}+\norm{(K_0-K)S}_{op})\norm{\res_t}_2\\
    \leq^{(1)} &\eta_0(\frac{6\kappa^3R_0}{ \lambda_{min}}m^{-1/2}+\frac{ \lambda_{min}\epsilon}{2R_0})R_0=\frac{6\kappa^3R_0}{ \lambda_{min}}m^{-1/2}+\frac{\epsilon \lambda_{min}\eta_0}{2}
\end{align*}
where $(1)$ follows from Lemma \ref{lemma:conv_proof} and the fact that $K_0 S \rightarrow K  S$ in probability \citet{yang2019scaling}. So $(1)$ holds when $m$ is sufficiently large. 

For term $B$ we have that 
\begin{align}\label{eq:jac_conv}
    \norm{\eta(J(\tilde \w )-J(\w_t))J(\w_t)^TS\res_t}& \leq \frac{\eta_0}{m}\norm{J(\tilde \w )-J(\w_t)}_{op}\norm{J(\w_t)S\res_t}_2 \nonumber\\
    &\leq\frac{\eta_0}{m}\norm{J(\tilde \w )-J(\w_t)}_{op}\norm{J(\w_t)S}_{op}\norm{\res_t}_2 \leq^{(1)} \eta_0 \kappa^2\norm{\tilde \w - \w_t}R_0 \nonumber\\
    &\leq^{(2)} \eta_0 \kappa\norm{ \w_{t+1} - \w_t} R_0\leq \frac{\eta_0^2 \kappa^3 R_0^3}{\sqrt{m}}
\end{align}
where $(1)$ is by Lemma \ref{lemma:precond_jac} and $(2)$ it by Lemma \ref{lemma:conv_proof}. 
Therefore we have
\begin{align*}
    \res_t=(I-\eta K S)^t \y + \xi(t)
\end{align*}
where 
\begin{align*}
    \norm{\xi(t)}&=\norm{\sum_{i=1}^t (I-\eta_0 K S)^i\epsilon(t-i)+(I-\eta K S)^tf(X,\w_0)}\\
    \leq& \norm{\sum_{i=1}^t (I-\eta_0 K S)^i}_{op}(\frac{\eta_0^2 \kappa^3 R_0^3}{\sqrt{m}}+\frac{6\kappa^3R_0}{ \lambda_{min}\sqrt{m}} +\frac{\epsilon  \lambda_{min}\eta_0}{2})
    +\norm{(I-\eta K S)^tf(X,\w_0)}\\
    \leq& (\frac{\eta_0^2 \kappa^3 R_0^3}{\sqrt{m}}+\frac{6\kappa^3R_0}{ \lambda_{min}\sqrt{m}} +\frac{\epsilon \lambda_{min}\eta_0}{2}) \sum_{i=1}^t(1-\eta_0 \lambda_{min})^i+(1-\eta_0 \lambda_{min})^t\nu C_0\\
    &\leq (\frac{\eta_0^2 \kappa^3 R_0^3}{\sqrt{m}}+\frac{6\kappa^3R_0}{ \lambda_{min}\sqrt{m}} +\frac{\epsilon \lambda_{min}\eta_0}{2})\frac{1}{\eta_0 \lambda_{min}}+(1-\eta_0 \lambda_{min})^t\nu C_0\\
    =&\epsilon/2 +\frac{\eta_0^2 \kappa^3 R_0^3}{\sqrt{m}}+\frac{6\kappa^3R_0}{ \lambda_{min}\sqrt{m}}+(1-\eta_0 \lambda_{min})^t\nu C_0
\end{align*}
So by choosing $m$ to be large enough and $\nu <\frac{\epsilon}{3C_0}$ we conclude the theorem.
\end{proof}

% \section{Extension to Other Architectures}\label{appendix:architectures}
% Here we detail how Theorem \ref{Thm:precond_dynamic} may be extended to other architectures. 

% The first consideration for a new architecture is whether or not it converges to its NTK. As mentioned in the related works, this has already been done for many architectures. \ag{cite also here}

% Now assuming that this is indeed the case, the only dependence on the architecture is in Lemma \ref{lemma:precond_jac}, where we used Lemma (1) of \citet{lee2019wide} whose analysis was done for fully connected networks. This lemma proved a version of Lipschitzness for the Jacobian of the network in a small ball around the initialization. Therefore, in order to extend our results to other architectures which converge to their NTK, one needs to only to prove this lemma for that architecture. 

% \section{Multi-output regression}
% We would like to extend the success of the pre-conditioner to multi-output pre-conditioner. In particular, it can be useful in case of multi-class classification. Let assume that we have $C$ classes and the task is multi-class classification, using $l_2$ loss. For each data element $\{x_i\}_{i=1}^n$ its associated class in encoded by a one-hot vector, $y(x_i) \in \Real^C$. The corresponding loss can be written as 
% \begin{equation}
% L(w) = \frac{1}{2} \sum_{m=1}^{C} \sum_{i=1}^n \left(f_m(x_i,w)-y_m(x_i) \right)^2. \end{equation}
% We denote by $f(w) \in \Real^{n \cdot C}$ the vector composed of all the labels arranged  as follows 
% \begin{equation}
%     f(w) = [f_1(x_1,w),\ldots,f_1(x_n,w),\ldots,f_C(x_1,w),\ldots,f_C(x_n,w)]^T,
% \end{equation}
% and similarly
% \begin{equation}
%     y = [y_1(x_1),\ldots,y_1(x_n),\ldots,y_C(x_1),\ldots,y_C(x_n)]^T.
% \end{equation}
% The gradient matrix $\nabla_w f(w) \in \Real^{P \times n \cdot C}$ is defined as 
% \begin{equation}
%   \nabla_w f(w) = [\nabla_w f_1(x_1,w),\ldots,\nabla_w f_1(x_n,w),\ldots,\nabla_w f_C(x_1,w),\ldots,\nabla_w f_C(x_n,w)]^T.   
% \end{equation}
% Therefore, the gradient descent update rule is
% \begin{equation}
%     w^{k+1} = w^k - \eta \nabla_w f(w^k) (f(w^k) - y).
% \end{equation}

\section{Convergence Analysis}\label{appendix:conv_analysis}
Under the settings of Sec. \ref{sec:conv_analysis}, we here prove the following lemma.
\begin{lemma} (Lemma \ref{lem:conv_analysis} from the paper)
    \begin{enumerate}
        \item Let $\w_0\in\Real^p$ and $\phi(\x):=\nabla f(\x,\w_0)$, it holds that $\w^*=\w^{**}$.
        \item Let $\delta_0 > 0$, $\eta_0 < \frac{2}{\lambda_{min}(KS)+\lambda_{max}(KS)}$ , $\epsilon>0$, $x\in\Real^d$ be a test point, $T>0$ number of iterations and $S$ such that $S\succ0$. Then there
        exists $N \in  \mathbb{N}$ such that for every $m \geq N$, the following holds with probability at least $1-\delta_0$ over random initialization of $\w_0$ when applying preconditioned gradient descent to both $f$ and $h$ with learning rate $\eta=\eta_0/m$ 
        \[
        \abs{h(\x,\w'_T)- f(\x,\w_T)} \leq \epsilon,
        \]
        where $\w'$ is as in \eqref{eq:pgd_h}.
    \end{enumerate}
\end{lemma}
\begin{proof}
For point (1), applying lemma \ref{lem:krr_s} with the kernel $\kr(\x,\z)=\langle \phi(\x), \phi(\x) \rangle$ (and kernel matrix $K_0$) yields $h(\cdot, w^*)=\sum_{i=1}^n\alpha^*_i\langle \phi(\x), \phi(\x_i))$ with     
\[
    \alpha^*=\left(K_0 + \gamma S^{-1}\right)^{-1}\y,
\]
which uniquely determines that $w^*=\sum_{i=1}^n\alpha^*_i\phi(\x_i)$.

When $\gamma\to 0$ this becomes the same minimizer as of the standard kernel ridge regression problem with $L(\w)=\frac{1}{2}\norm{f(X, \w)-\y}_2^2$ (or equivalently, substituting $S=I$).

Point (2) is proved in lemma \ref{lem:conv_analysis_test}.
\end{proof}

\begin{lemma}\label{lem:krr_s}
    Let $\kr(\x,\z) = \langle \phi(\x), \phi(\z) \rangle$ be any kernel and $[K]_{ij}=\frac{1}{n}\kr(\x_i,\x_j)$ be its kernel matrix. Given $S\succ 0$, let  $L_S(f)=\frac{1}{2}\norm{S^\frac{1}{2}(f(X)-\y)}_2^2 + \frac{\gamma}{2}\norm{f}_\mathcal{H}^2$ (where $\gamma > 0$ is some regularization term).
    Then the unique minimizer $f^* = \arg\min_\w L(\w)$ takes the from $f^*(\x)=\sum_{i=1}^n\alpha^*_i\langle \phi(\x), \phi(\x_i))$ with
    \[
    \alpha^*=\left(K + \gamma S^{-1}\right)^{-1}\y
    \]
\end{lemma}
\begin{proof}
The Representer Theorem \citep{scholkopf2001generalized} states that any minimizer of the kernel ridge regression problem $f^* = \arg\min_\w L_S(f)$ is of the form $f^*(\x)=\sum_{i=1}^n\alpha^*_i\langle \phi(\x), \phi(\x_i))$.

Then $f^*(X)$ can be written as $K\alpha^*$ and $\norm{f^*}_\mathcal{H}^2={\alpha^*}^TK\alpha^*$. So we can equivalently solve for:
\[
\alpha^* = \arg\min_{\alpha\in\mathbb{R}^n} \frac{1}{2}\norm{S^\frac{1}{2}(K\alpha-\y)}_2^2+ \frac{\gamma}{2}\alpha^TK\alpha,
\]
where $K_{ij} = \langle(\phi(\x_i),\phi(\x_j)\rangle$. Now opening up the norm we get:
\[
\alpha^* = \arg\min_\alpha \frac{1}{2}\left(\alpha^TK^TSK\alpha -\alpha^TK^TS\y - \y^TSK\alpha + \y^TS\y\right) + \frac{\gamma}{2}\alpha^TK\alpha.
\]
Then as $K$, $S$ are symmetric, and $\y^TS\y$ doesn't depend on $\alpha$ the above simplifies to:
\[
\alpha^* = \arg\min_\alpha \frac{1}{2}\alpha^TK^TSK\alpha - \alpha^TK^TS\y + \frac{\gamma}{2}\alpha^TK\alpha
\]
\[
= \arg\min_\alpha \frac{1}{2}\alpha^T \left(KSK + \gamma K \right)\alpha - \alpha^TKS\y.
\]
Now deriving with respect to $\alpha$ and setting to $0$ we obtain:
\[
0=\left(KSK + \gamma K \right)\alpha - KS\y = KS\left(\left(K + \gamma S^{-1}\right)\alpha - \y\right).
\]
As $K\succ 0$ and $S\succ 0$ it must hold that:
\[
0=\left(K + \gamma S^{-1}\right)\alpha - \y.
\]
Since the loss is convex, $\alpha^*=\left(K + \gamma S^{-1}\right)^{-1}\y$ is the unique minimizer. 
\end{proof}

\begin{lemma}\label{lem:conv_analysis_test}
For $\delta_0 > 0$, $ \frac{2}{\lambda_{min}(KS)+\lambda_{max}(KS)}>\eta_0$ , $\epsilon>0$, $x\in\Real^d$ be a test point, $T>0$ number of iterations and $S$ such that $S\succ0$, there
exists $N \in  \mathbb{N}$ such that for every $m \geq N$, the following holds with probability at least $1-\delta_0$ over random initialization of $\w_0$ when applying preconditioned gradient descent to both $f$ and $h$ with learning rate $\eta=\eta_0/m$
\[
\abs{\langle \w'_T, \nabla f(\x,\w_0) \rangle - f(\x,\w_T)} < \epsilon
\]
where $\w'$ is as in \eqref{eq:pgd_h}.
\end{lemma}
\begin{proof}
Let $\x, y$ be some data point and its label, which are not necessarily in the train set. Denote by $\tilde \res_{t}=f(\x,\w_t)-y$, and $\tilde J(\w)=\nabla f(\x,\w)$.
We define a linearized model $f^{lin}(\x,\w'_{t})=f(\x,\w_0) + \tilde J(\w_0)\w'_{t}$ where $\w'_t$ is as in \eqref{eq:pgd_h}. In particular, note that $f^{lin}(\x,\w'_0)=f(\x,\w_0)$.

Let $h(\x,\w) = \langle \w, \nabla f(\x,\w_0) \rangle$. By the triangle inequality, 
\[
\abs{h(\x,\w'_T) - f(\x,\w_T)} \leq  \abs{f^{lin}(\x,\w'_{T}) - f(\x,\w_T)} + \abs{f(\x,\w_0)}
\]
The weights of the last layer are initialized $\mathcal{N}(0,\nu)$ with some constant $\nu$ which we may choose. Thus, by taking small enough $\nu$, $\abs{f(\x,\w_0)}<\frac{\epsilon}{3}$ with the desired probability. Notice that $\tilde \res^{lin}_{t}-\tilde\res_{t} =f^{lin}(\x,\w'_{t}) - f(\x,\w_t)$ and as such, it remains to show that $\abs{\tilde \res^{lin}_{T}-\tilde\res_{T}}<\frac{2\epsilon}{3}$.

By the mean value theorem, there exists some $\tilde \w$ s.t
\begin{align*}
    \tilde \res_{t+1}-\tilde \res_{t}
    &=\tilde J( \tilde\w)(\w_{t+1}-\w_t)\\
    =^{(1)}&-\eta \tilde J( \tilde\w) J(\w_t)^T S \res_t
    =-\eta \tilde J( \w_t)J(\w_t)^T S\res_t-\eta(\tilde J( \tilde\w )-\tilde J(\w_t))J(\w_t)^T S \res_t\\
    =^{(2)}&-\eta_0 K_0(\x,X) S\res_t-\underbrace{\eta_0((K_t(\x,X)-K_0(\x,X))S)\res_t-\eta(\tilde J( \tilde\w)-\tilde J( \w_t))J(\w_t)^T S \res_t}_{\epsilon(\x, t)}
\end{align*}
where $(1)$ is by the gradient descent definition and $(2)$ is by $K_t$ definition.

Thus, taking a telescopic series we get that for every $t$,
\begin{align*}
\tilde\res_{T} 
&= \tilde\res_{0} + \sum_{i=0}^{T-1}\tilde\res_{i+1}-\tilde\res_{i}
= \tilde\res_{0} -\eta_0\sum_{i=0}^{T-1}\left( K_0(\x,X)S\res_i + \epsilon(\x,i)\right)
\end{align*}

Now for the linear model, by definition of our PGD updates we have
\[
\w'_{t+1}-\w'_t=-\eta J(\w'_t)^TS\res^{lin}_t=-\eta J(\w_0)^TS\res^{lin}_t
\]
 and by definition of the linearized model,
\begin{align*}
f^{lin}(\x,\w'_{t+1})-f^{lin}(\x,\w'_{t})
=&f(\x,\w_0)+\tilde J(\w_0)(\w'_{t+1})-\left(f(\x,\w_{0})+\tilde J(\w_0)\w'_{t}\right)\\
=& \tilde J(\w_0)(\w'_{t+1}-\w'_{t}) = -\eta \tilde J(\w_0)J(\w_0)^TS\res^{lin}_t \\
=& -\eta_0 K_0(\x,X)S\res^{lin}_t
\end{align*}

And so 
\[
\tilde\res^{lin}_{t+1}-\tilde\res^{lin}_{t}=-\eta_0 K_0(\x,X)S\res^{lin}_t
\]
Thus, taking a telescopic series we get that for every $t$,
\begin{align*}
\tilde\res^{lin}_{T} 
&= \tilde\res^{lin}_{0} + \sum_{i=0}^{T-1}\tilde\res^{lin}_{i+1}-\tilde\res^{lin}_{i}
= \tilde\res^{lin}_{0} -\eta_0\sum_{i=0}^{T-1} K_0(\x,X)S\res^{lin}_i
\end{align*}

Now we can compare between the linearized and non linearized versions. Since $\tilde\res^{lin}_{0} = \tilde\res_{0}$ we have
\begin{align*}
\tilde\res^{lin}_{T} - \tilde\res_{t} 
= -\eta_0K_0(\x,X)S\left(\sum_{i=0}^{T-1}\res^{lin}_i -\res_i\right) + \left(\sum_{i=0}^{T-1}\epsilon(\x,i)\right)
\end{align*}

Notice that $\res^{lin}_i=(I-\eta_0 K_0S)^i\res_0$. Theorem \ref{Thm:precond_dynamic} combined with choosing $\nu$ sufficiently small, states that $\res_i=(I-\eta_0 KS)^i\res_0 \pm \xi(i)$ with $\norm{\xi_i}\underset{m\to\infty}{\longrightarrow}0$. Analogously, one could also show $\res_i=(I-\eta_0 K_0S)^i\res_0 \pm \xi(i)$ by replacing $K$ in the proof with $K_0$. As such, for every $i<T$, $\norm{\res^{lin}_i -\res_i}\leq\norm{\xi(i)}$ with $\norm{\xi_i}\underset{m\to\infty}{\longrightarrow}0$. Now by Cauchey Shwartz and the triangle inequality we get:
\begin{align*}
\abs{\tilde\res^{lin}_{T} - \tilde\res_{T}}
\leq& \norm{\eta_0K_0(\x,X)S}\left(\sum_{i=0}^{T-1}\norm{\res^{lin}_i -\res_i}\right) + \abs{\sum_{i=0}^{T-1}\epsilon(\x,i)}\\
=& \norm{\eta_0K_0(\x,X)S}\left(\sum_{i=0}^{T-1}\norm{\xi(i)}\right) + \abs{\sum_{i=0}^{T-1}\epsilon(\x,i)}\\
\leq& \norm{\eta_0K_0(\x,X)S}T\max_{0\leq i\leq T-1}\norm{\xi(i)} + T\abs{\max_{0\leq i\leq T-1}\epsilon(\x,i)}.
\end{align*}

Now each $\xi(i)$ tends to 0 as $m$ tends to infinity. So there exists some $N_0\in \mathbb{N}$ s.t for all $m>N_0$, $\max_{0\leq i\leq T-1}\norm{\xi(i)}<\frac{\epsilon}{3T\norm{\eta_0K_0(\x,X)S}}$ so that the entire left hand side is at most $\frac{\epsilon}{3}$.

It remains to bound $\abs{\max_{0\leq i\leq T-1}\epsilon(\x,i)}$. Fix some $t$ to be the argmax. Recalling the definition of $\epsilon(\x,t)$ and using the fact that $\res_t$ is bounded by Lemma \ref{lemma:conv_proof}, it suffices to bound $(K_0(\x,X)-K_t(\x,X)S$ and $(\tilde J(\tilde\w)-\tilde J( \w_t))J(\w_t)^T) S$.

\begin{align*}\label{eq:prec_proof_transition}
    &\norm{(K_0(\x,X)-K_t(\x,X)S}_F=\frac{1}{m}\norm{(\tilde J( \w_0)J(\w_0)^T-J(x, \w_t)J(\w_t)^T)S}_F\\
    &= \frac{1}{m}\norm{(\tilde J( \w_0)J(\w_0)^T - \tilde J( \w_0)J(\w_t)^T + (\tilde J( \w_0)J(\w_t)^T -\tilde J( \w_t)J(\w_t)^T)S}_F\\
    &\leq \frac{1}{m}\left(\norm{\tilde J( \w_0)}_{2}\norm{(J(\w_0)^T-J(\w_t)^T)S}_F +\norm{\tilde J( \w_0)-\tilde J(\w_t)}_{2}\norm{J(\w_t)^TS}_F\right)\\
    &\leq 2\kappa^2\norm{\w_0-\w_t}_2\leq \frac{6\kappa^3R_0}{ \lambda_{min}}{{m^{-1/2}}},
\end{align*}
where the last inequality holds since Lemma \ref{lemma:precond_jac} holds analogously for $\tilde J$ and using Lemma \ref{lemma:conv_proof} for $\norm{\w_0-\w_t}_2$. 

$(\tilde J(\tilde\w)-\tilde J( \w_t))J(\w_t)^T )S$ can be similarly be bound by applying by  Lemma \ref{lemma:precond_jac} analogously to \eqref{eq:jac_conv}. As such there is some $N_1$ s.t the entire right hand side is at most $\frac{\epsilon}{3}$. 

Taking $N=\max\{N_0,N_1\}$ completes the proof.
\end{proof}

\section{Consistency of Spectral Engineering}\label{appendix:engineering}
\begin{theorem}(Theorem \ref{thm:norm_conv} from the paper)
Let $\kr(\x,\z)=\sum_{k=1}^\infty \lambda_k\Phi_k(\x)\Phi_k(\z)$ and $\kr_g(\x,\z)=\sum_{k=1}^\infty g(\lambda_k)\Phi_k(\x)\Phi_k(\z)$ be two Mercer kernels with non-zero eigenvalues $\{\lambda_i\},\{g(\lambda_i)\}$ and eigenfunctions $\{\Phi_i\}$ such that $\forall \x\in \mathcal X,\abs{\Phi_i(\x)}\leq M$. Assuming $g$ is $L$ Lipchitz. Let $K,K_g$ be the corresponding kernel matrices on i.i.d samples $\x_1,..,\x_n\in \mathcal X$. Define the kernel matrix $\tilde K_g=V  D V^T$ where $V=(\vv_1,..,\vv_n)$ with $\vv_i$ the i'th eigenvector of $K$ and $ D$ is a diagonal matrix with $ D_{ii}=g(\hat \lambda_i)$ where $\hat \lambda_i$ is the i'th eigenvalue of $K$. Then, for $n\rightarrow \infty $
\begin{align*}
\norm{\tilde K_g-K_g}_F\overset{a.s}{\rightarrow} 0,
\end{align*}
where a.s.\ stands for almost surely.
\end{theorem}
% \begin{theorem}
% Let $\kr(x,z)=\sum_{k=1}^\infty \lambda_k\phi_k(x)\phi_k(z)$ and $\kr_g(x,z)=\sum_{k=1}^\infty g(\lambda_k)\phi_k(x)\phi_k(z)$ be two Mercer kernels with non-zero eigenvalues $\{\lambda_i\},\{g(\lambda_i)\}$ and eigenfunctions $\{\phi_i\}$ such that $\forall x\in \mathcal X,\abs{\phi_i(x)}\leq M$. Assuming $g$ is $L$ Lipchitz. Let $K,K_g$ be the corresponding kernel matrices on i.i.d samples $x_1,..,x_n\in \mathcal X$. Define the matrix $\tilde K_g=V\tilde D V^T$ where $V=(\vv_1,..,\vv_n)$ with $\vv_i$ the i'th eigenvector of $K$ and $\tilde D$ is a diagonal matrix with $\tilde D_{ii}=g(\hat \lambda_i)$ where $\hat \lambda_i$ is the i'th eigenvalue of $K$. Then, for $n\rightarrow \infty $
% \begin{align*}
% \norm{\tilde K_g-K_g}_2\overset{a.s}{\rightarrow} 0
% \end{align*}
% \end{theorem}
\begin{proof}
From the Mercer decomposition of $\kr_g$ we have that
\begin{align*}
     K_g &= \sum_{k=1}^\infty  g(\lambda_k)\Phi_k(X)\Phi^T_k(X) 
\end{align*}
where $\Phi_k(X)=\frac{1}{\sqrt{n}}(\Phi_k(x_1),..,\Phi_k(x_n))^T\in \Real^n$. We also define 
\begin{align*}
     K^{\leq R}_g &:= \sum_{k=1}^R  g(\lambda_k)\Phi_k(X)\Phi^T_k(X) \\
     K^{>R}_g &:= \sum_{k=R}^\infty  g(\lambda_k)\Phi_k(X)\Phi^T_k(X) 
\end{align*}
Let $\vv_1,..,\vv_n$ be the eigenvectors of $K$ (and therefore of $\tilde K_g$)  and fix $R>0$.
Then we have
\begin{align*}
     &\norm{\tilde K_g-K_g}_F\leq\norm{\tilde K_g-K^{\leq R}_g}_{F}+\norm{K^{>R}_g}_F=\norm{K^{>R}_g}_F+\sqrt{\sum_{j=1}^n\langle (\tilde K_g-K^{\leq R}_g)\vv_j,(\tilde K_g-K^{\leq R}_g)\vv_j \rangle }\\
     =&\norm{K^{>R}_g}_F+ \sqrt{\sum_{j=1}^n\langle g(\hat \lambda_j)\vv_j-K^{\leq R}_g\vv_j,g(\hat \lambda_j)\vv_j-K^{\leq R}_g\vv_j \rangle}\\
     =&\norm{K^{>R}_g}_F+\sqrt{\sum_{j=1}^n\left( g(\hat \lambda_j)^2-2 g(\hat \lambda_j)\langle \vv_j,K^{\leq R}_g\vv_j \rangle +\langle K^{\leq R}_g\vv_j,K^{\leq R}_g\vv_j \rangle\right)}\\
     =&\norm{K^{>R}_g}_F\\
     +&\left(\sum_{j=1}^n\left( g(\hat \lambda_j)^2-2 g(\hat \lambda_j)\sum_{k=1}^R g(\lambda_k)(\vv_j^T\Phi_k(X))^2 +\sum_{k,l=1}^R g(\lambda_k)g(\lambda_l)(\vv_j^T\Phi_k(X))(\vv_j^T\Phi_l(X))\Phi_l(X)^T\Phi_k(X)\right)\right)^\frac{1}{2}
\end{align*}
Since $\Phi_1,\Phi_2,...$ are orthonormal, and $\Phi_i(\x_1),..,\Phi_i(\x_n)$ are i.i.d, by the law of large numbers it holds that   $\Phi_l(X)^T\Phi_k(X)\rightarrow \mathbb{E}\Phi_l\Phi_k=\delta_{ij}$, therefore, the last term converges a.s   to 
\begin{align*}
\norm{K^{>R}_g}_F&
     +\left(\sum_{j=1}^n\left( g(\hat \lambda_j)^2-2 g(\hat \lambda_j)\sum_{k=1}^Rg(\lambda_k)(\vv_j^T\Phi_k(X))^2 +\sum_{k=1}^R g(\lambda_k)^2(\vv_j^T\Phi_k(X))^2\right)\right)^\frac{1}{2}\\
\end{align*}
Next we use \citet{braun2005spectral}[Theorem 4.10 and  Equation 4.15] to get that 
\begin{align*}
\sum_{i=1}^d(\vv_i^T\Phi(X))^2\rightarrow \begin{cases} 1~ \text{ if } \Phi \text{ is an eigenfunction of } \lambda_k\\
0 ~ \text{ else}
\end{cases}
\end{align*}
Which implies that
\begin{align*}
    \sum_{k=1}^R g(\lambda_k)\cdot(\vv_j^T\Phi_k(X))^2\rightarrow \delta_{j,k}g(\lambda_k)
\end{align*}.
Therefore we get that 
\begin{align*}
\norm{\tilde K_g-K_g}_F&\leq\norm{\tilde K_g-K^{\leq R}_g}_{F}+\norm{K^{>R}_g}_F \rightarrow \sqrt{\sum_{i=1}^R(g(\hat \lambda_j)-g( \lambda_j))^2+\sum_{i=R}^n g(\hat \lambda_j)^2}+\norm{K^{>R}_g}_F
\end{align*}
Finally, from the Lipchitzness, $\sum_{i=1}^R(g(\hat \lambda_j)-g( \lambda_j))^2\leq L^2\sum_{i=1}^R( \hat \lambda_j- \lambda_j)^2$ so by applying \citet{rosasco2010learning} (Proposition 10) we get 
\begin{align*}
\sqrt{\sum_{i=1}^R(g(\hat \lambda_j)-g( \lambda_j))^2+\sum_{i=R}^n g(\hat \lambda_j)^2}+\norm{K^{>R}_g}_F \rightarrow \norm{K^{>R}_g}_F+\sqrt{\sum_{i=R}^n g(\hat \lambda_j)^2} 
\end{align*}
Observing that $\norm{K^{>R}_g}_F+\sqrt{\sum_{i=R}^n g(\hat \lambda_j)^2 }\rightarrow_{R\rightarrow \infty} 0$ give us the desired result.
\end{proof}

\end{document}
