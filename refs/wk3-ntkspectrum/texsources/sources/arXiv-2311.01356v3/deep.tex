\newcommand{\D}{\mathscr{D}}
\label{sec:deep}
In the following, we treat the case of deep networks, meaning $L > 1$. The proofs of this section also apply in the case of shallow networks, but are only relevant in the case of deep networks, since in the case of shallow networks better bounds have been derived in the preceding subsection. Again, we first assume that the matrices $W^{(0)}, ..., W^{(L-1)}$ and the biases $b^{(0)}, ..., b^{(L-1)}$ are fixed and the matrix $W^{(L)}$ is initialized randomly according to \Cref{assum:1}. In this setting, we denote
\begin{equation*}
\Lambda \defeq \Vert W^{(L-1)} \Vert_2 \cdots \Vert W^{(0)} \Vert_2.
\end{equation*}

As the first step, we derive a bound on the cardinality of the set of all possible combinations $(D^{(L-1)}(x), ..., D^{(0)}(x))$ of the $D$-matrices occurring in the formula for $\nabla \Phi(x)$ from \Cref{prop:grad_relu}. The finiteness of this set is immediate since it consists of tuples of diagonal $N \times N$-matrices with zeros and ones on the diagonal. Naively, one can bound the cardinality by $2^{LN}$ but in fact, it is possible to prove a much stronger bound. 
\begin{lemma}\label{lem:D_card}
Assume $d+2 <N$. For fixed $W^{(0)},..., W^{(L-1)}$ and $b^{(0)}, ..., b^{(L-1)}$ we define
\begin{equation*}
\mathscr{D} \defeq \left\{ \left(D^{(L-1)}(x),..., D^{(0)}(x)\right): \ x \in \RR^d\right\},
\end{equation*}
with $D^{(0)}(x),..., D^{(L-1)}(x)$ as defined in \Cref{subsec:gradient}.
Then it holds that
\begin{equation*}
\vert \mathscr{D} \vert \leq \left(\frac{\ee N}{d+1}\right)^{L(d+1)}.
\end{equation*}
\end{lemma}
\begin{proof}
For any $0\leq \ell \leq L-1$ we define
\begin{equation*}
\D^{(\ell)} \defeq \left\{ \left(D^{(\ell)}(x), ..., D^{(0)}(x)\right): \ x \in \RR^d\right\}
\end{equation*}
and claim that it holds
\begin{equation}\label{eq:ind_d}
\vert \D^{(\ell)} \vert \leq \left(\frac{\ee N}{d+1}\right)^{(\ell + 1)(d+1)},
\end{equation}
which we will show by induction over $\ell$. 

To this end, we start with the case $\ell = 0$. Using the notation introduced in \cref{prop:vc_half_spaces_2}, we see for any $i \in \{1,...,N\}$ and $x \in \RR^d$ that 
\begin{equation*}
\left(D^{(0)}(x)\right)_{i,i} = f_{(x,1)^T}\left((W^{(0)}_{i,-}, b^{(0)}_{i})^T\right).
\end{equation*}
By definition of $D^{(0)}(x)$, this yields
\begin{align*}
\vert\D^{(0)} \vert &= \left\vert \left\{ \left(f_{(x,1)^T}\left((W^{(0)}_{1,-}, b^{(0)}_{1})^T\right),..., f_{(x,1)^T}\left((W^{(0)}_{N,-}, b^{(0)}_{N})^T\right)\right): \ x \in \RR^d\right\}\right\vert \\
&\leq \left\vert \left\{ \left(f_{\alpha}\left((W^{(0)}_{1,-}, b^{(0)}_{1})^T\right),..., f_{\alpha}\left((W^{(0)}_{N,-}, b^{(0)}_{N})^T\right)\right): \ \alpha \in \RR^{d+1}\right\}\right\vert \leq \left(\frac{\ee N}{d+1}\right)^{d+1}.
\end{align*}
To obtain the last inequality we employed Sauer's lemma (cf. \cite[Lemma 6.10]{shalev2014understanding}) and the estimate for the VC-dimension of halfspaces (cf. \Cref{prop:vc_half_spaces_2}) and the assumption $d+2 < N$.

We now assume that the claim holds for some fixed $0 \leq \ell \leq L-2$. We then see 
\begin{align*}
\D^{(\ell + 1)} &= \left\{ \left(D^{(\ell+1)}(x), ..., D^{(0)}(x)\right): \ x \in \RR^d\right\} \\
&= \! \!\biguplus_{(C^{(\ell)}, ..., C^{(0)}) \in \D^{(\ell)}} \! \left\{ \left(D^{(\ell + 1)}(x), C^{(\ell)}, ..., C^{(0)}\right): \! \ x \in \RR^d \!\text{ with } \! D^{(j)}(x)=C^{(j)} \ \!\text{for } \! \text{all } \! j\in \{0,...,\ell\}\right\}
\end{align*}
and hence
\begin{equation} \label{eq:D_bound}
\abs{\D^{(\ell + 1)}} = \sum_{(C^{(\ell)}, ..., C^{(0)}) \in \D^{(\ell)}} \abs{\left\{ D^{(\ell + 1)}(x): \ x \in \RR^d \text{ with } D^{(j)}(x)=C^{(j)} \ \text{for all } j\in \{0,...,\ell\}\right\}}.
\end{equation}
We thus fix $(C^{(\ell)}, ..., C^{(0)}) \in \D^{(\ell)}$ and seek to bound $\abs{\mathcal{A}}$ where
\begin{equation*}
\mathcal{A} \defeq \left\{ D^{(\ell + 1)}(x): \ x \in \RR^d, \  D^{(j)}(x) = C^{(j)} \ \text{for all } j\in \{0,...,\ell\}\right\}.
\end{equation*}
With $x^{(\ell)}$ as in \Cref{subsec:gradient} (for $1 \leq \ell \leq L$), it is immediate that
\begin{align*}
&\norel\abs{\mathcal{A}} \\
&= \abs{\left\{\left(\mathbbm{1}_{W^{(\ell + 1)}_{1,-}x^{(\ell+1)} + b^{(\ell + 1)}_1 > 0}, ..., \mathbbm{1}_{W^{(\ell + 1)}_{N,-}x^{(\ell+1)} + b^{(\ell + 1)}_N > 0}\right):  x \in \RR^d \!\text{ with }  \! \forall \  0\leq j \leq \ell\!:\!D^{(j)}(x)=C^{(j)}\right\}}.
\end{align*}
Fix $x \in \RR^d$ with $D^{(j)}(x)=C^{(j)}$ for every $j \in \{0,..., \ell\}$. From the definition of $x^{(\ell+1)}$ (see \Cref{subsec:gradient}) we may write
\begin{equation*}
x^{(\ell+1)} = C^{(\ell)}W^{(\ell)} \cdots C^{(0)}W^{(0)}x + \bar{c},
\end{equation*} 
where $\bar{c} = \bar{c}(C^{(0)}, ..., C^{(\ell)}, W^{(0)}, ..., W^{(\ell)}, b^{(0)}, ..., b^{(\ell)})\in \RR^N$ is a fixed vector. We thus get for any $i \in \{1,...,N\}$ that
\begin{equation*}
W^{(\ell + 1)}_{i,-}x^{(\ell+1)} + b^{(\ell+1)}_i = W^{(\ell + 1)}_{i,-}C^{(\ell)}W^{(\ell)} \cdots C^{(0)}W^{(0)}x + c_i,
\end{equation*}
where $c = c(C^{(0)}, ..., C^{(\ell)}, W^{(0)}, ..., W^{(\ell+1)}, b^{(0)}, ..., b^{(\ell+1)}) \in \RR^N$ is fixed. Writing 
\begin{equation*}
V \defeq W^{(\ell + 1)}C^{(\ell)}W^{(\ell)} \cdots C^{(0)}W^{(0)} \in \RR^{N \times d} 
\end{equation*}
we infer
\begin{align*}
\abs{\mathcal{A}} &= \left\vert \left\{ \left(f_{(x,1)^T}\left((V_{1,-}, c_{1})^T\right),..., f_{(x,1)^T}\left((V_{N,-}, c_{N})^T\right)\right): \ x \in \RR^d\right\}\right\vert \\
&\leq \left\vert \left\{ \left(f_{\alpha}\left((V_{1,-}, c_{1})^T\right),..., f_{\alpha}\left((V_{N,-}, c_{N})^T\right)\right): \ \alpha \in \RR^{d+1}\right\}\right\vert \leq \left(\frac{\ee N}{d+1}\right)^{d+1}
\end{align*}
where we again used \cite[Lemma 6.10]{shalev2014understanding} and \Cref{prop:vc_half_spaces_2} for the last inequality, again noting that we assumed $d+2 < N$. 

Combining this result with \eqref{eq:D_bound} and the induction hypothesis, we see
\begin{equation*}
\abs{\D^{(\ell+1)}} \leq \abs{\D^{(\ell)}} \cdot \left(\frac{\ee N}{d+1}\right)^{d+1} \leq \left(\frac{\ee N}{d+1}\right)^{(\ell + 2)(d+1)}.
\end{equation*}
By induction this shows that \eqref{eq:ind_d} holds for all $\ell \in \{0,...,L-1\}$. The statement of the lemma then follows by noting that $\D = \D^{(L-1)}$.
\end{proof}

Having established a bound for the cardinality of the set $\mathscr{D}$, we can now bound the covering numbers of $\mathcal{L}$ in the case of deep networks. 
\begin{lemma}\label{lem:cov_bound}
Let $N > d+2$ and $W^{(0)},..., W^{(L-1)}$ and $b^{(0)}, ..., b^{(L-1)}$ be fixed. Moreover, let
\begin{equation*}
\Lambda \defeq \Vert W^{(L-1)} \Vert_2 \cdots \Vert W^{(0)} \Vert_2.
\end{equation*}
Then for every arbitrary $\varepsilon \in (0, \Lambda)$ it holds that
\begin{equation*}
\mathcal{N}(\mathcal{L}, \Vert \cdot \Vert_2, \varepsilon) \leq \left(\frac{3 \Lambda}{\eps} \right)^d \cdot \left(\frac{\ee N}{d+1}\right)^{L(d+1)}.
\end{equation*}
Here, $\mathcal{L}$ is as defined in \Cref{prop:key}. 
\end{lemma}
\begin{proof}
We can assume that $\Lambda > 0$ since otherwise $(0, \Lambda) = \emptyset$. Using the notation $\mathscr{D}$ as introduced previously in \cref{lem:D_card}, we see immediately that
\begin{equation*}
\mathcal{L} = \bigcup_{(C^{(L-1)}, ..., C^{(0)}) \in \D} \left[ C^{(L-1)}W^{(L-1)} \cdots C^{(0)}W^{(0)} \B_d(0,1) \right]
\end{equation*}
and hence it holds for every $\eps > 0$ that
\begin{equation}\label{eq:cov_num}
\mathcal{N}(\mathcal{L}, \Vert \cdot \Vert_2, \eps) \leq \sum_{(C^{(L-1)}, ..., C^{(0)}) \in \D} \mathcal{N}\left(C^{(L-1)}W^{(L-1)} \cdots C^{(0)}W^{(0)} \B_d(0,1), \Vert \cdot \Vert_2, \eps \right).
\end{equation}
This can be seen from the fact that the union of $\eps$-nets of $C^{(L-1)}W^{(L-1)} \cdots C^{(0)}W^{(0)} \B_d(0,1)$, where $(C^{(L-1)}, ..., C^{(0)})$ runs through all elements of $\D$, is an $\eps$-net of $\mathcal{L}$.

Fix $(C^{(L-1)}, ..., C^{(0)}) \in \D$ and define $V \defeq C^{(L-1)}W^{(L-1)} \cdots C^{(0)}W^{(0)}$. 
From \Cref{prop:covering_ball} we infer that there are $w_1, ..., w_M \in \B_d(0,1)$ such that
\begin{equation*}
\B_d(0,1) \subseteq \bigcup_{i=1}^M \overline{B}_d\left(w_i,\frac{\eps}{\Lambda}\right)
\end{equation*}
with $M \leq \left(\frac{2 \Lambda}{\eps} + 1\right)^d$. 
Let $v_i \defeq Vw_i$ for every $i \in \{1,...,M\}$. 
Hence, it holds $v_i \in V\B_d(0,1)$ for every $i \in \{1,...,M\}$. Let $u \in V \B_d(0,1)$ be arbitrary and choose $u' \in \B_d(0,1)$ satisfying $ u = Vu'$. 
By choice of $w_1, ..., w_M$ there exists $i \in \{1,...,M\}$ with 
\begin{equation*}
\Vert u' - w_i \Vert_2 \leq \frac{\eps}{\Lambda}.
\end{equation*}
But then it holds
\begin{align*}
  \Vert u - v_i \Vert_2 =\Vert u - Vw_i \Vert_2 
&\leq \Vert V  \Vert_2 \cdot \Vert u' - w_i \Vert_2 \\
&\leq \underbrace{\Vert C^{(L-1)} \Vert_2}_{\leq 1} \cdot \Vert W^{(L-1)} \Vert_2 \cdots \underbrace{\Vert C^{(0)} \Vert_2}_{\leq 1} \cdot \Vert W^{(0)} \Vert_2 \cdot \frac{\eps}{\Vert W^{(L-1)} \Vert_2 \cdots \Vert W^{(0)} \Vert_2} \\
&\leq \eps.
\end{align*}
Hence, we conclude
\begin{equation*}
\mathcal{N}\left(V\B_d(0,1), \Vert \cdot \Vert_2, \eps \right) \leq \left(\frac{2 \Lambda}{\eps} + 1\right)^d \leq \left(\frac{3 \Lambda}{\eps}\right)^d,
\end{equation*}
since $\eps \leq \Lambda$. Combining this estimate with Equation \eqref{eq:cov_num} and \cref{lem:D_card} yields the claim.
\end{proof}

The following proposition establishes the final bound when the expectation is calculated only with respect to $W^{(L)}$, i.e., conditioning on $W^{(0)}, ..., W^{(L-1)}, b^{(0)}, ..., b^{(L-1)}$. 
\begin{proposition}\label{thm:deep_final}
Let the matrices $W^{(L-1)}, ..., W^{(0)}$ and the biases $b^{(L-1)}, ..., b^{(0)}$ be fixed and define $\Lambda \defeq \Vert W^{(L-1)} \Vert_2 \cdots \Vert W^{(0)} \Vert_2$. Moreover, assume that $d+2 <  N$. Then the following hold, with an absolute constant $C>0$:
\begin{enumerate}
\item{ \label{item:deep_highprob} For any $u \geq 0$, we have
\begin{equation*}
\underset{x \in \RR^d}{\sup} \Vert W^{(L)}D^{(L-1)}(x)W^{(L-1)}\cdots D^{(0)}(x) W^{(0)} \Vert_2 \leq C \cdot \Lambda \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot (\sqrt{d} + u)
\end{equation*}
with probability at least $(1 - 2\exp(-u^2))$ (with respect to the choice of $W^{(L)}$).
}
\item{
$\displaystyle
\underset{W^{(L)}}{\EE} \left[\underset{x \in \RR^d}{\sup} \Vert W^{(L)}D^{(L-1)}(x)W^{(L-1)}\cdots D^{(0)}(x) W^{(0)} \Vert_2 \right] \leq C \cdot \Lambda \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot \sqrt{d}.
$
}
\end{enumerate}
\end{proposition}
\begin{proof}
Using \Cref{lem:cov_bound} and the elementary inequality $\sqrt{x+y}  \leq \sqrt{x} + \sqrt{y}$ for $x,y \geq 0$ we infer
\begin{align*}
\sqrt{\ln \left(\mathcal{N}(\mathcal{L}, \Vert \cdot \Vert_2, \eps)\right)} &\leq \sqrt{\ln \left(\left(\frac{3 \Lambda}{\eps} \right)^d \cdot \left(\frac{\ee N}{d+1}\right)^{L(d+1)}\right)} \\
&\leq \sqrt{d} \cdot \sqrt{\ln \left(\frac{3 \Lambda}{\eps}\right)} + \sqrt{L}\cdot \sqrt{d+1} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)}
\end{align*}
for any $\eps \in (0, \Lambda)$ where $\mathcal{L}$ is as in \Cref{prop:key}. This yields
\begin{equation} \label{eq:proof1}
\int_0^\Lambda \sqrt{\ln \left(\mathcal{N}(\mathcal{L}, \Vert \cdot \Vert_2, \eps)\right)} \ \dd \eps \leq \sqrt{d} \cdot \int_0^\Lambda \sqrt{\ln \left(\frac{3 \Lambda}{\eps}\right)} \ \dd \eps + \Lambda \cdot \sqrt{L}\cdot \sqrt{d+1} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)}.
\end{equation}
Using the substitution $\sigma = \frac{\eps}{3\Lambda}$ we get
\begin{align} \label{eq:proof2}
\int_0^\Lambda \sqrt{\ln \left(\frac{3 \Lambda}{\eps}\right)} \ \dd \eps = 3\Lambda \cdot \int_0^{\frac{1}{3}} \sqrt{\ln \left( 1/\sigma\right)}\ \dd \sigma \leq C_1 \cdot \Lambda
\end{align}
with $C_1 \defeq 3 \cdot \int_0^{1/3} \sqrt{\ln \left( 1/\sigma\right)} \dd \sigma$. Overall, we thus see
\begin{align}
\int_0^\Lambda \sqrt{\ln \left(\mathcal{N}(\mathcal{L}, \Vert \cdot \Vert_2, \eps)\right)} \ \dd \eps \overset{\eqref{eq:proof1}, \eqref{eq:proof2}}&{\leq} \Lambda \cdot \left( C_1  \cdot \sqrt{d} + \sqrt{L}\cdot \sqrt{d+1} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)}\right) \nonumber\\
\overset{d < d+1 \leq 2d}&{\leq} \sqrt{2} \cdot \max\{1, C_1\}  \cdot \Lambda \cdot \left( \sqrt{d} + \sqrt{L} \cdot \sqrt{d} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)}\right) \nonumber\\
\label{alig:int}
\overset{L \geq 1, N \geq d+1}&{\leq} \underbrace{2\sqrt{2} \cdot \max\{1, C_1\}}_{=: C_2}  \cdot \Lambda \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot \sqrt{d}. 
\end{align}
We can now prove \eqref{item:deep_highprob}. From \Cref{prop:key} we obtain an absolute constant $C_3>0$ such that for any $u \geq 0$ the estimate
\begin{align*}
\underset{x \in \RR^d}{\sup} \Vert W^{(L)} \cdot D^{(L-1)}(x)W^{(L-1)} \cdots D^{(0)}(x)W^{(0)}\Vert_2 &\leq C_3 \left( C_2  \cdot \Lambda \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot \sqrt{d} + u \cdot \Lambda \right) \\
&\leq C_3C_2 \cdot \Lambda \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot (\sqrt{d} + u)
\end{align*}
holds with probability at least $(1 - 2 \exp(-u^2))$ with respect to $W^{(L)}$. 

For the expectation bound, simply note that 
\begin{align*}
\underset{W^{(L)}}{\EE} \left[ \underset{x \in \RR^d}{\sup} \Vert W^{(L)} \cdot D^{(L-1)}(x)W^{(L-1)} \cdots D^{(0)}(x)W^{(0)}\Vert_2\right] \leq C_3C_2 \cdot \Lambda \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot \sqrt{d}
\end{align*}
follows directly from \Cref{prop:key}. Hence, the claim follows letting $C \defeq C_2C_3$.
\end{proof}

Incorporating randomness in $W^{(0)}, ..., W^{(L-1)}$ and $b^{(0)}, ..., b^{(L-1)}$ leads to the following theorem.
\begin{theorem} \label{thm:deep_finall}
There exist absolute constants $C, c_1 > 0$ such that for $N > d+2$, random weight matrices $W^{(0)},...,W^{(L)}$ and random bias vectors $b^{(0)},...,b^{(L)}$ as in \Cref{assum:1} the following hold:
\begin{enumerate}
\item{
For every $u,t \geq 0$ we have 
\begin{align*}
&\norel \underset{x \in \RR^d}{\sup} \Vert W^{(L)}D^{(L-1)}(x)W^{(L-1)}\cdots D^{(0)}(x) W^{(0)} \Vert_2 \\
&\leq C \cdot \left(1 + \frac{\sqrt{d} + t}{\sqrt{N}}\right)\left(2\sqrt{2} + \frac{\sqrt{2} t}{\sqrt{N}}\right)^{L-1} \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot (\sqrt{d} + u)
\end{align*}
with probability at least $(1-2\exp(-u^2))_+\left((1-2\exp(-c_1 t^2))_+\right)^L$ with respect to $W^{(0)},..., W^{(L)}$ and $b^{(0)},...,b^{(L)}$.
}
\item{
$\displaystyle
 \norel\EE\left[\underset{x \in \RR^d}{\sup} \Vert W^{(L)}D^{(L-1)}(x)W^{(L-1)}\cdots D^{(0)}(x) W^{(0)} \Vert_2 \right] $ \\
$\displaystyle \leq C \cdot \left( 1 + \frac{\sqrt{d}}{\sqrt{N}}\right) \cdot (2 \sqrt{2})^{L-1} \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot \sqrt{d}.$
}
\end{enumerate}
\end{theorem}
\begin{proof}
Let $\widetilde{C}$ be the relabeled constant from \Cref{thm:deep_final} and define $C \defeq \sqrt{2} \widetilde{C}$. We start with (1): In view of \Cref{thm:deep_final,prop:highprob} it suffices to show that for every $t \geq 0$ the estimate
\begin{equation*}
\Lambda \leq \sqrt{2}\left(1 + \frac{\sqrt{d} + t}{\sqrt{N}}\right)\left(2\sqrt{2} + \frac{\sqrt{2}t}{\sqrt{N}}\right)^{L-1}
\end{equation*}
holds with probability at least $\left((1-2\exp(-c_1 t^2))_+\right)^L$, where
\begin{equation*}
\Lambda \defeq \Vert W^{(L-1)}\Vert_2 \cdots \Vert W^{(0)} \Vert_2.
\end{equation*}
To show this, note that \cite[Corollary 7.3.3]{vershynin_high-dimensional_2018} yields
\begin{equation*}
\Vert W^{(0)} \Vert_2 = \sqrt{\frac{2}{N}} \left\Vert \sqrt{\frac{N}{2}} W^{(0)}\right\Vert_2\leq \sqrt{2} \left(1 + \frac{\sqrt{d} + t}{\sqrt{N}}\right)
\end{equation*}
with probability at least $(1- 2\exp(-c_1 t^2))_+$, as well as
\begin{equation*}
\Vert W^{(\ell)} \Vert_2 = \sqrt{\frac{2}{N}} \left\Vert \sqrt{\frac{N}{2}} W^{(\ell)} \right\Vert_2 \leq \sqrt{\frac{2}{N}} (2 \sqrt{N} + t)= 2\sqrt{2} + \frac{\sqrt{2}t}{\sqrt{N}}
\end{equation*}
with probability at least $(1- 2\exp(-c_1 t^2))_+$ for any $1 \leq \ell \leq L-1$. Hence, the claim follows from an iterative application of \Cref{prop:highprob}.

To conclude (2) we apply \cite[Theorem 7.3.1]{vershynin_high-dimensional_2018} to the matrices $\sqrt{\frac{N}{2}} W^{(\ell)}$ for $0 \leq \ell \leq L-1$. This yields $\underset{W^{(\ell)}}{\EE} \Vert W^{(\ell)} \Vert_2 \leq 2 \sqrt{2}$ for every $1 \leq \ell \leq L-1$ and further $\underset{W^{(0)}}{\EE} \Vert W^{(0)}\Vert_2\leq \sqrt{2} \left(1 + \frac{\sqrt{d}}{\sqrt{N}} \right)$. The independence of the matrices $W^{(\ell)}$ combined with \Cref{thm:deep_final} then yields the claim.
\end{proof}

Now we obtain an upper bound on the Lipschitz constant directly using \Cref{thm:deep_finall} and \eqref{eq:lowbound}:
\begin{theorem}\label{thm:final_deep_lipschitz}
Let $\Phi: \RR^d \to \RR$ be a random ReLU network of width $N$ and with $L$ hidden layers satisfying \Cref{assum:1}. Moreover, let $d+2<N$. Then the following hold, for certain absolute constants $C, c_1 > 0:$
\begin{enumerate}
\item{For every $u,t \geq 0$, we have
\begin{align*}
\lip(\Phi)\leq C \cdot \left(1 + \frac{\sqrt{d} + t}{\sqrt{N}}\right)\left(2\sqrt{2} + \frac{\sqrt{2} t}{\sqrt{N}}\right)^{L-1} \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot (\sqrt{d} + u)
\end{align*}
with probability at least $(1-2\exp(-u^2))_+\left((1-2\exp(-c_1 t^2))_+\right)^L$.
}
\item{
$\displaystyle
\EE \left[\lip(\Phi) \right]
\leq C \cdot \left( 1 + \frac{\sqrt{d}}{\sqrt{N}}\right) \cdot (2 \sqrt{2})^{L-1} \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot \sqrt{d}.
$
}
\end{enumerate}
\end{theorem}
 By plugging in special values for $t$ and $u$ and using $d \leq N$ we can now prove \Cref{thm:main_2}.

\renewcommand*{\proofname}{Proof of \Cref{thm:main_2}}
\begin{proof}
Let $\widetilde{C}$ and $\widetilde{c_1}$ be the relabeled constants from \Cref{thm:final_deep_lipschitz} and let $C \defeq 6\widetilde{C}$ and $c_1 \defeq \widetilde{c_1}$. 
Part (1) follows from \Cref{thm:final_deep_lipschitz} by plugging in $u = \sqrt{d}$ and $t = \sqrt{N}$, which yields
\begin{equation*}
\lip(\Phi)\leq \widetilde{C} \cdot 3 \cdot (3\sqrt{2})^{L-1} \cdot \sqrt{L} \cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot 2\cdot \sqrt{d} = 6\widetilde{C} \cdot (3\sqrt{2})^{L-1} \cdot \sqrt{L}\cdot \sqrt{\ln \left(\frac{\ee N}{d+1}\right)} \cdot \sqrt{d}
\end{equation*}
with probability at least $(1-2\exp(-d))\left((1-2\exp(-c_1 N))_+\right)^L$,
where we also used $d \leq N$ and $1-2\exp(-u^2) = 1-2\exp(-d) \geq 0$.
Part (2) follows immediately from $d \leq N$ and part (2) of \Cref{thm:final_deep_lipschitz}.
\end{proof}
\renewcommand*{\proofname}{Proof}
