\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abramowitz \& Stegun(1972)Abramowitz and Stegun]{abramowitz_stegun}
Abramowitz, M. and Stegun, I.
\newblock \emph{Handbook of Mathematical Functions: With Formulas, Graphs, and
  Mathematical Tables}.
\newblock U.S. Department of Commerce, National Bureau of Standards, 1972.

\bibitem[Arfken(1985)]{garfken67:math}
Arfken, G.
\newblock \emph{Mathematical Methods for Physicists}.
\newblock Academic Press, {Inc.}, San Diego, third edition, 1985.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R.~R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8139--8148, 2019.

\bibitem[Belkin et~al.(2018{\natexlab{a}})Belkin, Hsu, and
  Mitra]{belkin_mitra_overfitting_perfect}
Belkin, M., Hsu, D.~J., and Mitra, P.
\newblock Overfitting or perfect fitting? risk bounds for classification and
  regression rules that interpolate.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2300--2311, 2018{\natexlab{a}}.

\bibitem[Belkin et~al.(2018{\natexlab{b}})Belkin, Ma, and
  Mandal]{belkin2018understand}
Belkin, M., Ma, S., and Mandal, S.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  541--549, 2018{\natexlab{b}}.

\bibitem[Belkin et~al.(2019{\natexlab{a}})Belkin, Hsu, Ma, and
  Mandal]{belkin2018reconciling}
Belkin, M., Hsu, D., Ma, S., and Mandal, S.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019{\natexlab{a}}.

\bibitem[Belkin et~al.(2019{\natexlab{b}})Belkin, Rakhlin, and
  Tsybakov]{belkin_interpolation_optimality}
Belkin, M., Rakhlin, A., and Tsybakov, A.~B.
\newblock Does data interpolation contradict statistical optimality?
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1611--1619, 2019{\natexlab{b}}.

\bibitem[Bietti \& Mairal(2019)Bietti and Mairal]{bietti2019inductive}
Bietti, A. and Mairal, J.
\newblock On the inductive bias of neural tangent kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  12873--12884, 2019.

\bibitem[Cao et~al.(2019)Cao, Fang, Wu, Zhou, and Gu]{cao2019understanding}
Cao, Y., Fang, Z., Wu, Y., Zhou, D.-X., and Gu, Q.
\newblock Towards understanding the spectral bias of deep learning, 2019.

\bibitem[Cohen et~al.(2019)Cohen, Malka, and Ringel]{cohen2019learning}
Cohen, O., Malka, O., and Ringel, Z.
\newblock Learning curves for deep neural networks: A gaussian field theory
  perspective, 2019.

\bibitem[Cucker \& Smale(2002)Cucker and Smale]{cucker}
Cucker, F. and Smale, S.
\newblock Best choices for regularization parameters in learning theory: On the
  bias-variance problem.
\newblock \emph{Foundations of Computational Mathematics}, 2:\penalty0
  413--428, 2002.

\bibitem[Dai \& Xu(2013)Dai and Xu]{Dai_2013}
Dai, F. and Xu, Y.
\newblock \emph{Approximation Theory and Harmonic Analysis on Spheres and
  Balls}.
\newblock Springer New York, 2013.

\bibitem[Dietrich et~al.(1999)Dietrich, Opper, and
  Sompolinsky]{sompolinsky1999svm}
Dietrich, R., Opper, M., and Sompolinsky, H.
\newblock Statistical mechanics of support vector networks.
\newblock \emph{Physical Review Letters}, 82\penalty0 (14):\penalty0
  2975–2978, 1999.

\bibitem[Efthimiou \& Frye(2014)Efthimiou and Frye]{costasspherical}
Efthimiou, C. and Frye, C.
\newblock \emph{Spherical Harmonics In P Dimensions}.
\newblock World Scientific Publishing Company, 2014.

\bibitem[Evgeniou et~al.(1999)Evgeniou, Pontil, and Poggio]{poggio_regnets}
Evgeniou, T., Pontil, M., and Poggio, T.
\newblock Regularization networks and support vector machines.
\newblock \emph{Advances in Computational Mathematics}, 13, 1999.

\bibitem[Gyorfi et~al.(2003)Gyorfi, Kohler, Krzyzak, and Walk]{gyorfi}
Gyorfi, L., Kohler, M., Krzyzak, A., and Walk, H.
\newblock A distribution-free theory of nonparametric regression.
\newblock \emph{Journal of the American Statistical Association}, 98\penalty0
  (464):\penalty0 1084--1084, 2003.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,
  Kianinejad, Patwary, Yang, and Zhou]{hestness2017deep}
Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H.,
  Patwary, M. M.~A., Yang, Y., and Zhou, Y.
\newblock Deep learning scaling is predictable, empirically, 2017.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8571--8580, 2018.

\bibitem[{Lecun} et~al.(1998){Lecun}, {Bottou}, {Bengio}, and {Haffner}]{MNIST}
{Lecun}, Y., {Bottou}, L., {Bengio}, Y., and {Haffner}, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8570--8581, 2019.

\bibitem[Liang \& Rakhlin(2018)Liang and Rakhlin]{liang2018just}
Liang, T. and Rakhlin, A.
\newblock Just interpolate: Kernel ``ridgeless" regression can generalize,
  2018.

\bibitem[Luo et~al.(2019)Luo, Ma, Xu, and Zhang]{luo2019theory}
Luo, T., Ma, Z., Xu, Z.-Q.~J., and Zhang, Y.
\newblock Theory of the frequency principle for general deep neural networks,
  2019.

\bibitem[Mercer(1909)]{mercer1909xvi}
Mercer, J.
\newblock Functions of positive and negative type, and their connection with
  the theory of integral equations.
\newblock \emph{Philosophical Transactions of the Royal Society of London.
  Series A}, 209:\penalty0 415--446, 1909.

\bibitem[M{\'e}zard et~al.(1987)M{\'e}zard, Parisi, and
  Virasoro]{mezard1987spin}
M{\'e}zard, M., Parisi, G., and Virasoro, M.
\newblock \emph{Spin glass theory and beyond: An Introduction to the Replica
  Method and Its Applications}, volume~9.
\newblock World Scientific Publishing Company, 1987.

\bibitem[Novak et~al.(2020)Novak, Xiao, Hron, Lee, Alemi, Sohl-Dickstein, and
  Schoenholz]{neuraltangents2020}
Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A.~A., Sohl-Dickstein, J., and
  Schoenholz, S.~S.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Opper \& Vivarelli(1998)Opper and Vivarelli]{OpperVivarelli}
Opper, M. and Vivarelli, F.
\newblock General bounds on bayes errors for regression with gaussian
  processes.
\newblock In \emph{Advances in Neural Information Processing Systems 11}, pp.\
  302--308, 1998.

\bibitem[Pehlevan et~al.(2018)Pehlevan, Ali, and
  {\"O}lveczky]{pehlevan2018flexibility}
Pehlevan, C., Ali, F., and {\"O}lveczky, B.~P.
\newblock Flexibility in motor timing constrains the topology and dynamics of
  pattern generator circuits.
\newblock \emph{Nature communications}, 9\penalty0 (1):\penalty0 1--15, 2018.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2018spectral}
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F.,
  Bengio, Y., and Courville, A.
\newblock On the spectral bias of neural networks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pp.\  5301--5310, 2019.

\bibitem[Rasmussen \& Williams(2005)Rasmussen and Williams]{GPMLRasmussen}
Rasmussen, C.~E. and Williams, C. K.~I.
\newblock \emph{Gaussian Processes for Machine Learning (Adaptive Computation
  and Machine Learning)}.
\newblock The MIT Press, 2005.

\bibitem[Sch{\"o}lkopf \& Smola(2001)Sch{\"o}lkopf and Smola]{scholkopf_smola}
Sch{\"o}lkopf, B. and Smola, A.~J.
\newblock \emph{Learning with Kernels: Support Vector Machines, Regularization,
  Optimization, and Beyond}.
\newblock MIT Press, 2001.

\bibitem[Sherrington \& Kirkpatrick(1975)Sherrington and
  Kirkpatrick]{sherrington_spinglass}
Sherrington, D. and Kirkpatrick, S.
\newblock Solvable model of a spin-glass.
\newblock \emph{Phys. Rev. Lett.}, 35:\penalty0 1792--1796, 1975.

\bibitem[Smola et~al.(2001)Smola, Ovari, and Williamson]{smola2001dotproduct}
Smola, A.~J., Ovari, Z.~L., and Williamson, R.~C.
\newblock Regularization with dot-product kernels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  308--314, 2001.

\bibitem[Sollich(1999)]{sollich1999learning}
Sollich, P.
\newblock Learning curves for gaussian processes.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  344--350, 1999.

\bibitem[Sollich(2002)]{sollich2001mismatch}
Sollich, P.
\newblock Gaussian process regression with mismatched models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  519--526, 2002.

\bibitem[Sollich \& Halees(2002)Sollich and Halees]{sollich2002approx}
Sollich, P. and Halees, A.
\newblock Learning curves for gaussian process regression: Approximations and
  bounds.
\newblock \emph{Neural Computation}, 14\penalty0 (6):\penalty0 1393–1428,
  2002.

\bibitem[{Spigler} et~al.(2019){Spigler}, {Geiger}, and {Wyart}]{spigler2019}
{Spigler}, S., {Geiger}, M., and {Wyart}, M.
\newblock {Asymptotic learning curves of kernel methods: empirical data v.s.
  Teacher-Student paradigm}.
\newblock \emph{arXiv e-prints}, art. arXiv:1905.10843, 2019.

\bibitem[Spigler et~al.(2019)Spigler, Geiger, and Wyart]{spigler2019asymptotic}
Spigler, S., Geiger, M., and Wyart, M.
\newblock Asymptotic learning curves of kernel methods: empirical data v.s.
  teacher-student paradigm, 2019.

\bibitem[Vapnik(1999)]{vapnik1999overview}
Vapnik, V.~N.
\newblock An overview of statistical learning theory.
\newblock \emph{IEEE transactions on neural networks}, 10\penalty0
  (5):\penalty0 988--999, 1999.

\bibitem[Wahba(1990)]{Wahba90a}
Wahba, G.
\newblock \emph{Spline Models for Observational Data}.
\newblock Society for Industrial and Applied Mathematics, Philadelphia, 1990.

\bibitem[Xu et~al.(2019{\natexlab{a}})Xu, Zhang, Luo, Xiao, and
  Ma]{xu2019frequency}
Xu, Z.-Q.~J., Zhang, Y., Luo, T., Xiao, Y., and Ma, Z.
\newblock Frequency principle: Fourier analysis sheds light on deep neural
  networks, 2019{\natexlab{a}}.

\bibitem[Xu et~al.(2019{\natexlab{b}})Xu, Zhang, and Xiao]{xu2018training}
Xu, Z.-Q.~J., Zhang, Y., and Xiao, Y.
\newblock Training behavior of deep neural network in frequency domain.
\newblock In Gedeon, T., Wong, K.~W., and Lee, M. (eds.), \emph{Neural
  Information Processing}, pp.\  264--274, 2019{\natexlab{b}}.

\bibitem[Yang \& Salman(2019)Yang and Salman]{yang2019finegrained}
Yang, G. and Salman, H.
\newblock A fine-grained spectral perspective on neural networks, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{ZHANGDeep}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Xu, Luo, and Ma]{zhang2019explicitizing}
Zhang, Y., Xu, Z.-Q.~J., Luo, T., and Ma, Z.
\newblock Explicitizing an implicit bias of the frequency principle in
  two-layer neural networks, 2019.

\end{thebibliography}
