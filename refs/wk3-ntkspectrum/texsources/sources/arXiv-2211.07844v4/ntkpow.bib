@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{arora_exact_comp, 
 author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Exact Computation with an Infinitely Wide Neural Net},
 url = {https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{arora_exact_comp-OLD,
title = "On exact computation with an infinitely wide neural net",
abstract = "How well does a classic deep net architecture like AlexNet or VGG19 classify on a standard dataset such as CIFAR-10 when its “width”- namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers - is allowed to increase to infinity? Such questions have come to the forefront in the quest to theoretically understand deep learning and its mysteries about optimization and generalization. They also connect deep learning to notions such as Gaussian processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures the behavior of fully-connected deep nets in the infinite width limit trained by gradient descent; this object was implicit in some other recent papers. An attraction of such ideas is that a pure kernel-based method is used to capture the power of a fully-trained deep net of infinite width. The current paper gives the first efficient exact algorithm for computing the extension of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as well as an efficient GPU implementation of this algorithm. This results in a significant new benchmark for performance of a pure kernel-based method on CIFAR-10, being 10\% higher than the methods reported in [Novak et al., 2019], and only 6\% lower than the performance of the corresponding finite deep net architecture (once batch normalization etc. are turned off). Theoretically, we also give the first non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed equivalent to the kernel regression predictor using NTK.",
author = "Sanjeev Arora and Du, {Simon S.} and Wei Hu and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang",
note = "",
year = "2019",
language = "English (US)",
volume = "32",
journal = "Advances in Neural Information Processing Systems",
noissn = "1049-5258",
}



@InProceedings{nguyen_tight_bounds,
  title = 	 {Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep {ReLU} Networks},
  author =       {Nguyen, Quynh and Mondelli, Marco and Mont\'ufar, Guido},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8119--8129},
  year = 	 {2021},
  noeditor =	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nguyen21g/nguyen21g.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nguyen21g.html},
  abstract = 	 {A recent line of work has analyzed the theoretical properties of deep neural networks via the Neural Tangent Kernel (NTK). In particular, the smallest eigenvalue of the NTK has been related to the memorization capacity, the global convergence of gradient descent algorithms and the generalization of deep nets. However, existing results either provide bounds in the two-layer setting or assume that the spectrum of the NTK matrices is bounded away from 0 for multi-layer networks. In this paper, we provide tight bounds on the smallest eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of infinite widths and for finite widths. In the finite-width setting, the network architectures we consider are fairly general: we require the existence of a wide layer with roughly order of $N$ neurons, $N$ being the number of data samples; and the scaling of the remaining layer widths is arbitrary (up to logarithmic factors). To obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of hidden feature matrices, and upper bounds on the Lipschitz constant of input-output feature maps.}
}




@book{donnellbook,
  title={Analysis of {B}oolean functions},
  author={O'Donnell, Ryan},
  year={2014},
  publisher={Cambridge University Press}
}

@inproceedings{Panigrahi2020Effect,
title={Effect of Activation Functions on the Training of Overparametrized Neural Nets},
author={Abhishek Panigrahi and Abhishek Shetty and Navin Goyal},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgfdeBYvH}
}

@article{hacohen2021principal,
  nodoi = {10.48550/ARXIV.2105.05553},
  url = {https://arxiv.org/abs/2105.05553},
  author = {Hacohen, Guy and Weinshall, Daphna},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Principal Components Bias in Over-parameterized Linear Models, and its Manifestation in Deep Neural Networks},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@book{girko2001theory,
  title={Theory of Stochastic Canonical Equations},
  author={Girko, V.L.},
  number={v. 2},
  noisbn={9781402000744},
  lccn={01045954},
  series={Mathematics and its applications},
  url={https://books.google.com/books?id=fsMamZXZwIsC},
  year={2001},
  publisher={Kluwer Academic}
}

@article{Marchenko1967diistribution,
	nodoi = {10.1070/sm1967v001n04abeh001994},
	url = {https://doi.org/10.1070/sm1967v001n04abeh001994},
	year = 1967,
	nomonth = {apr},
	publisher = {{IOP} Publishing},
	volume = {1},
	number = {4},
	pages = {457--483},
	author = {V A Mar{\v{c}}enko and L A Pastur},
	title = {{DISTRIBUTION} {OF} {EIGENVALUES} {FOR} {SOME} {SETS} {OF} {RANDOM} {MATRICES}},
	journal = {Mathematics of the {USSR}-Sbornik},
	abstract = {}
}

@article{REBROVA201840,
title = {Norms of random matrices: Local and global problems},
journal = {Advances in Mathematics},
volume = {324},
pages = {40-83},
year = {2018},
noissn = {0001-8708},
nodoi = {https://doi.org/10.1016/j.aim.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0001870816310878},
author = {Elizaveta Rebrova and Roman Vershynin},
keywords = {Random matrices, Operator norm, Heavy tails, Bai–Yin law},
abstract = {Can the behavior of a random matrix be improved by modifying a small fraction of its entries? Consider a random matrix A with i.i.d. entries. We show that the operator norm of A can be reduced to the optimal order O(n) by zeroing out a small submatrix of A if and only if the entries have zero mean and finite variance. Moreover, we obtain an almost optimal dependence between the size of the removed submatrix and the resulting operator norm. Our approach utilizes the cut norm and Grothendieck–Pietsch factorization for matrices, and it combines the methods developed recently by C. Le and R. Vershynin and by E. Rebrova and K. Tikhomirov.}
}

@article{wigner1957characteristics,
 noissn = {0003486X},
 URL = {http://www.jstor.org/stable/1969956},
 author = {Eugene P. Wigner},
 journal = {Annals of Mathematics},
 number = {2},
 pages = {203--207},
 publisher = {Annals of Mathematics},
 title = {Characteristics Vectors of Bordered Matrices with Infinite Dimensions II},
 urldate = {2022-06-06},
 volume = {65},
 year = {1957}
}



@article{wishart1928generalised,
    author = {WISHART, JOHN},
    title = "{THE GENERALISED PRODUCT MOMENT DISTRIBUTION IN SAMPLES FROM A NORMAL MULTIVARIATE POPULATION}",
    journal = {Biometrika},
    volume = {20A},
    number = {1-2},
    pages = {32-52},
    year = {1928},
    nomonth = {12},
    noissn = {0006-3444},
    nodoi = {10.1093/biomet/20A.1-2.32},
    url = {https://doi.org/10.1093/biomet/20A.1-2.32},
    eprint = {https://academic.oup.com/biomet/article-pdf/20A/1-2/32/530655/20A-1-2-32.pdf},
}

@article{alt217local,
author = {Johannes Alt and László Erdős and Torben Krüger},
title = {{Local law for random Gram matrices}},
volume = {22},
journal = {Electronic Journal of Probability},
number = {none},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {1 -- 41},
keywords = {capacity of MIMO channels, general variance profile, hard edge, Marchenko-Pastur law, soft edge},
year = {2017},
nodoi = {10.1214/17-EJP42},
URL = {https://doi.org/10.1214/17-EJP42}
}

@misc{bahri2022explaining,
title={Explaining Scaling Laws of Neural Network Generalization},
author={Yasaman Bahri and Ethan Dyer and Jared Kaplan and Jaehoon Lee and Utkarsh Sharma},
year={2022},
url={https://openreview.net/forum?id=FvfV64rovnY}
}

@article{canatar2021spectral,
	abstract = {A theoretical understanding of generalization remains an open problem for many machine learning models, including deep networks where overparameterization leads to better performance, contradicting the conventional wisdom from classical statistics. Here, we investigate generalization error for kernel regression, which, besides being a popular machine learning method, also describes certain infinitely overparameterized neural networks. We use techniques from statistical mechanics to derive an analytical expression for generalization error applicable to any kernel and data distribution. We present applications of our theory to real and synthetic datasets, and for many kernels including those that arise from training deep networks in the infinite-width limit. We elucidate an inductive bias of kernel regression to explain data with simple functions, characterize whether a kernel is compatible with a learning task, and show that more data may impair generalization when noisy or not expressible by the kernel, leading to non-monotonic learning curves with possibly many peaks.},
	author = {Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
	date = {2021/05/18},
	date-added = {2022-06-02 18:09:46 -0700},
	date-modified = {2022-06-02 18:09:46 -0700},
	nodoi = {10.1038/s41467-021-23103-1},
	id = {Canatar2021},
	noisbn = {2041-1723},
	journal = {Nature Communications},
	number = {1},
	pages = {2914},
	title = {Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks},
	url = {https://doi.org/10.1038/s41467-021-23103-1},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1038/s41467-021-23103-1}}


@inproceedings{nitanda2021optimal,
title={Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime},
author={Atsushi Nitanda and Taiji Suzuki},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PULSD5qI2N1}
}

@misc{tsai2022side,
  nodoi = {10.48550/ARXIV.2203.00614},
  url = {https://arxiv.org/abs/2203.00614},
  author = {He, Juncai and Tsai, Richard and Ward, Rachel},
  keywords = {Machine Learning (cs.LG), Numerical Analysis (math.NA), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  title = {Side-effects of Learning from Low Dimensional Data Embedded in an {E}uclidean Space},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{du2018gradient,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}

@InProceedings{du2019gradient,
  title = 	 {Gradient Descent Finds Global Minima of Deep Neural Networks},
  author =       {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1675--1685},
  year = 	 {2019},
  noeditor =	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/du19c/du19c.pdf},
  url = 	 {https://proceedings.mlr.press/v97/du19c.html},
  abstract = 	 {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.}
}

@inproceedings{bowman2022implicit,
title={Implicit Bias of {MSE} Gradient Optimization in Underparameterized Neural Networks},
author={Benjamin Bowman and
               Guido Mont{\'{u}}far}, 
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=VLgmhQDVBV}
}

@inproceedings{
bowman2022spectral,
title={Spectral Bias Outside the Training Set for Deep Networks in the Kernel Regime},
author={Benjamin Bowman and Guido Montufar},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=a01PL2gb7W5}
}


@article{Yang2021DoesTD,
  title={Does the Data Induce Capacity Control in Deep Learning?},
  author={Rubing Yang and Jialin Mao and Pratik Chaudhari},
  journal={ArXiv},
  year={2021},
  url={https://arxiv.org/abs/2110.14163}, 
  volume={abs/2110.14163}
}

@article{JMLR:v21:20-933,
  author  = {Vardan Papyan},
  title   = {Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {252},
  pages   = {1--64},
  url     = {http://jmlr.org/papers/v21/20-933.html}
}


@inproceedings{jacot_ntk,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{exact_comp,
title = "On exact computation with an infinitely wide neural net",
abstract = "How well does a classic deep net architecture like AlexNet or VGG19 classify on a standard dataset such as CIFAR-10 when its “width”- namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers - is allowed to increase to infinity? Such questions have come to the forefront in the quest to theoretically understand deep learning and its mysteries about optimization and generalization. They also connect deep learning to notions such as Gaussian processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures the behavior of fully-connected deep nets in the infinite width limit trained by gradient descent; this object was implicit in some other recent papers. An attraction of such ideas is that a pure kernel-based method is used to capture the power of a fully-trained deep net of infinite width. The current paper gives the first efficient exact algorithm for computing the extension of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as well as an efficient GPU implementation of this algorithm. This results in a significant new benchmark for performance of a pure kernel-based method on CIFAR-10, being 10\% higher than the methods reported in [Novak et al., 2019], and only 6\% lower than the performance of the corresponding finite deep net architecture (once batch normalization etc. are turned off). Theoretically, we also give the first non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed equivalent to the kernel regression predictor using NTK.",
author = "Sanjeev Arora and Du, {Simon S.} and Wei Hu and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang",
booktitle = {Advances in Neural Information Processing Systems},
nonote = "; 33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019 ; Conference date: 08-12-2019 Through 14-12-2019",
year = "2019",
language = "English (US)",
volume = "32",
noissn = "1049-5258",
}


@inproceedings{LeeBNSPS18,
title={Deep Neural Networks as {G}aussian Processes},
author={Jaehoon Lee and Yasaman Bahri and  Roman Novak and
               Samuel S. Schoenholz and Jeffrey Pennington and
               Jascha Sohl{-}Dickstein},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1EA-M-0Z},
}

@inproceedings{LeeBNSPS18-OLD,
  author    = {Jaehoon Lee and Yasaman Bahri and  Roman Novak and
               Samuel S. Schoenholz and Jeffrey Pennington and
               Jascha Sohl{-}Dickstein},
  title     = {Deep Neural Networks as {G}aussian Processes},
  booktitle = {International Conference on Learning Representations (ICLR)},
  pages = {1--11},
  year  = {2018},
  nomonth ={April},
  }
  
@inproceedings{
matthews2018gaussian,
title={Gaussian Process Behaviour in Wide Deep Neural Networks},
author={Alexander G. {de G. Matthews} and Jiri Hron and Mark Rowland and Richard E. Turner and Zoubin Ghahramani},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=H1-nGgWC-},
}

@InProceedings{fine_grain_arora,
  title = 	 {Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author =       {Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {322--332},
  year = 	 {2019},
  noeditor =	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/arora19a/arora19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/arora19a.html},
  abstract = 	 {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR’17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.}
}

@inproceedings{fine_grain_arora-OLD,
title = "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks",
abstract = "Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR' 17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Leamability of a broad class of smooth functions by 2-laycr ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.",
author = "Sanjeev Arora and Du, {Simon S.} and Wei Hu and Zhiyuan Li and Ruosong Wang",
year = "2019",
nomonth = jan,
day = "1",
language = "English (US)",
noseries = "36th International Conference on Machine Learning, ICML 2019",
publisher = "International Machine Learning Society (IMLS)",
pages = "477--502",
booktitle = "36th International Conference on Machine Learning, ICML 2019",
nonote = "36th International Conference on Machine Learning, ICML 2019 ; Conference date: 09-06-2019 Through 15-06-2019",
}


@InProceedings{pmlr-v139-nguyen21g,
  title = 	 {Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep {ReLU} Networks},
  author =       {Nguyen, Quynh and Mondelli, Marco and Mont\'ufar, Guido},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8119--8129},
  year = 	 {2021},
  noeditor =	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nguyen21g/nguyen21g.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nguyen21g.html},
  abstract = 	 {A recent line of work has analyzed the theoretical properties of deep neural networks via the Neural Tangent Kernel (NTK). In particular, the smallest eigenvalue of the NTK has been related to the memorization capacity, the global convergence of gradient descent algorithms and the generalization of deep nets. However, existing results either provide bounds in the two-layer setting or assume that the spectrum of the NTK matrices is bounded away from 0 for multi-layer networks. In this paper, we provide tight bounds on the smallest eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of infinite widths and for finite widths. In the finite-width setting, the network architectures we consider are fairly general: we require the existence of a wide layer with roughly order of $N$ neurons, $N$ being the number of data samples; and the scaling of the remaining layer widths is arbitrary (up to logarithmic factors). To obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of hidden feature matrices, and upper bounds on the Lipschitz constant of input-output feature maps.}
}

@inproceedings{uniform_sphere_data,
  author    = {Ronen Basri and
               David W. Jacobs and
               Yoni Kasten and
               Shira Kritchman},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {The Convergence Rate of Neural Networks for Learned Functions of Different
               Frequencies},
  booktitle = {Advances in Neural Information Processing Systems 32},
  nonote ={Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada}, 
  pages     = {4763--4772},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/5ac8bb8a7d745102a978c5f8ccdb61b8-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:19 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/BasriJKK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{wavlet_marginals, author = {Wainwright, Martin J. and Simoncelli, Eero P.}, 
title = {Scale Mixtures of Gaussians and the Statistics of Natural Images}, 
year = {1999}, 
publisher = {MIT Press}, 
address = {Cambridge, MA, USA}, 
abstract = {The statistics of photographic images, when represented using multiscale (wavelet) bases, exhibit two striking types of non-Gaussian behavior. First, the marginal densities of the coefficients have extended heavy tails. Second, the joint densities exhibit variance dependencies not captured by second-order models. We examine properties of the class of Gaussian scale mixtures, and show that these densities can accurately characterize both the marginal and joint distributions of natural image wavelet coefficients. This class of model suggests a Markov structure, in which wavelet coefficients are linked by hidden scaling variables corresponding to local image structure. We derive an estimator for these hidden variables, and show that a nonlinear "normalization" procedure can be used to Gaussianize the coefficients.}, 
booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems}, 
pages = {855–861}, 
numpages = {7}, 
location = {Denver, CO}, 
series = {NIPS'99} 
}

@article {Papyan24652,
	author = {Papyan, Vardan and Han, X. Y. and Donoho, David L.},
	title = {Prevalence of neural collapse during the terminal phase of deep learning training},
	volume = {117},
	number = {40},
	pages = {24652--24663},
	year = {2020},
	nodoi = {10.1073/pnas.2015509117},
	publisher = {National Academy of Sciences},
	abstract = {Modern deep neural networks for image classification have achieved superhuman performance. Yet, the complex details of trained networks have forced most practitioners and researchers to regard them as black boxes with little that could be understood. This paper considers in detail a now-standard training methodology: driving the cross-entropy loss to zero, continuing long after the classification error is already zero. Applying this methodology to an authoritative collection of standard deepnets and datasets, we observe the emergence of a simple and highly symmetric geometry of the deepnet features and of the deepnet classifier, and we document important benefits that the geometry conveys{\textemdash}thereby helping us understand an important component of the modern deep learning training paradigm.Modern practice for training classification deepnets involves a terminal phase of training (TPT), which begins at the epoch where training error first vanishes. During TPT, the training error stays effectively zero, while training loss is pushed toward zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call neural collapse (NC), involving four deeply interconnected phenomena. (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class means. (NC2) The class means collapse to the vertices of a simplex equiangular tight frame (ETF). (NC3) Up to rescaling, the last-layer classifiers collapse to the class means or in other words, to the simplex ETF (i.e., to a self-dual configuration). (NC4) For a given activation, the classifier{\textquoteright}s decision collapses to simply choosing whichever class has the closest train class mean (i.e., the nearest class center [NCC] decision rule). The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.Experimental measurements have been deposited in the Stanford Digital Repository, https://purl.stanford.edu/ng812mz4543. An animation can be found at https://purl.stanford.edu/br193mh4244.},
	noissn = {0027-8424},
	URL = {https://www.pnas.org/content/117/40/24652},
	eprint = {https://www.pnas.org/content/117/40/24652.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@misc{bordelon2021learning,
      title={Learning Curves for SGD on Structured Features}, 
      author={Blake Bordelon and Cengiz Pehlevan},
      year={2021},
      eprint={2106.02713},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{pmlr-v54-xie17a,
  title = 	 {{Diverse Neural Network Learns True Target Functions}},
  author = 	 {Xie, Bo and Liang, Yingyu and Song, Le},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1216--1224},
  year = 	 {2017},
  noeditor =	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth =	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/xie17a/xie17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/xie17a.html},
  abstract = 	 {Neural networks are a powerful class of functions that can be trained with simple gradient descent to achieve state-of-the-art performance on a variety of applications. Despite their practical success, there is a paucity of results that provide theoretical guarantees on why they are so effective.  Lying in the center of the problem is the difficulty of analyzing the non-convex loss function with potentially numerous local minima and saddle points. Can neural networks corresponding to the stationary points of the loss function learn the true target function? If yes, what are the key factors contributing to such nice optimization properties?   In this paper, we answer these questions by analyzing one-hidden-layer neural networks with ReLU activation, and show that despite the non-convexity, neural networks with diverse units have no spurious local minima. We bypass the non-convexity issue by directly analyzing the first order optimality condition, and show that the loss can be made arbitrarily small if the minimum singular value of the “extended feature matrix” is large enough. We make novel use of techniques from kernel methods and geometric discrepancy, and identify a new relation linking the smallest singular value to the spectrum of a kernel function associated with the activation function and to the diversity of the units. Our results also suggest a novel regularization function to promote unit diversity for potentially better generalization ability.}
}


@misc{tropp2015introduction,
      title={An Introduction to Matrix Concentration Inequalities}, 
      author={Joel A. Tropp},
      year={2015},
      eprint={1501.01571},
      archivePrefix={arXiv},
      primaryClass={math.PR}
}

@article{big-data-low-rank,
author = {Udell, Madeleine and Townsend, Alex},
title = {Why Are Big Data Matrices Approximately Low Rank?},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {1},
number = {1},
pages = {144-160},
year = {2019},
nodoi = {10.1137/18M1183480},

URL = { 
        https://doi.org/10.1137/18M1183480
    
},
eprint = { 
        https://doi.org/10.1137/18M1183480
    
}

}

@misc{zhou2014lowrank,
      title={Low-Rank Modeling and Its Applications in Image Analysis}, 
      author={Xiaowei Zhou and Can Yang and Hongyu Zhao and Weichuan Yu},
      year={2014},
      eprint={1401.3409},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article {Giusti13455,
	author = {Giusti, Chad and Pastalkova, Eva and Curto, Carina and Itskov, Vladimir},
	title = {Clique topology reveals intrinsic geometric structure in neural correlations},
	volume = {112},
	number = {44},
	pages = {13455--13460},
	year = {2015},
	nodoi = {10.1073/pnas.1506407112},
	publisher = {National Academy of Sciences},
	abstract = {Detecting structure in neural activity is critical for understanding the function of neural circuits. The coding properties of neurons are typically investigated by correlating their responses to external stimuli. It is not clear, however, if the structure of neural activity can be inferred intrinsically, without a priori knowledge of the relevant stimuli. We introduce a novel method, called clique topology, that detects intrinsic structure in neural activity that is invariant under nonlinear monotone transformations. Using pairwise correlations of neurons in the hippocampus, we demonstrate that our method is capable of detecting geometric structure from neural activity alone, without appealing to external stimuli or receptive fields.Detecting meaningful structure in neural activity and connectivity data is challenging in the presence of hidden nonlinearities, where traditional eigenvalue-based methods may be misleading. We introduce a novel approach to matrix analysis, called clique topology, that extracts features of the data invariant under nonlinear monotone transformations. These features can be used to detect both random and geometric structure, and depend only on the relative ordering of matrix entries. We then analyzed the activity of pyramidal neurons in rat hippocampus, recorded while the animal was exploring a 2D environment, and confirmed that our method is able to detect geometric organization using only the intrinsic pattern of neural correlations. Remarkably, we found similar results during nonspatial behaviors such as wheel running and rapid eye movement (REM) sleep. This suggests that the geometric structure of correlations is shaped by the underlying hippocampal circuits and is not merely a consequence of position coding. We propose that clique topology is a powerful new tool for matrix analysis in biological settings, where the relationship of observed quantities to more meaningful variables is often nonlinear and unknown.},
	noissn = {0027-8424},
	URL = {https://www.pnas.org/content/112/44/13455},
	eprint = {https://www.pnas.org/content/112/44/13455.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@INPROCEEDINGS{Williams00theeffect,
    author = {Christopher Williams and Matthias Seeger},
    title = {The Effect of the Input Density Distribution on Kernel-based Classifiers},
    booktitle = {Proceedings of the 17th International Conference on Machine Learning},
    year = {2000},
    pages = {1159--1166},
    publisher = {Morgan Kaufmann}
}

@article{El_Karoui_2010,
	nodoi = {10.1214/08-aos648},
  
	url = {https://doi.org/10.1214%2F08-aos648},
  
	year = 2010,
	nomonth ={feb},
  
	publisher = {Institute of Mathematical Statistics},
  
	volume = {38},
  
	number = {1},
  
	author = {Noureddine El Karoui},
  
	title = {The spectrum of kernel random matrices},
  
	journal = {The Annals of Statistics}
}

@article{10.2307/3318636,
 noissn = {13507265},
 URL = {http://www.jstor.org/stable/3318636},
 abstract = {Let $H\colon L_{2}(S,\scr{I},P)\mapsto L_{2}(S,\scr{I},P)$ be a compact integral operator with a symmetric kernel h. Let Xi, i∈ N, be independent S-valued random variables with common probability law P. Consider the n × n matrix H̃n with entries n-1h(Xi,Xj), 1 ≤ i, j ≤ n (this is the matrix of an empirical version of the operator H with P replaced by the empirical measure Pn), and let Hn denote the modification of H̃n, obtained by deleting its diagonal. It is proved that the l2 distance between the ordered spectrum of Hn and the ordered spectrum of H tends to zero a.s. if and only if H is Hilbert-Schmidt. Rates of convergence and distributional limit theorems for the difference between the ordered spectra of the operators Hn (or H̃n) and H are also obtained under somewhat stronger conditions. These results apply in particular to the kernels of certain functions H = φ(L) of partial differential operators L (heat kernels, Green functions).},
 author = {Vladimir Koltchinskii and Evarist Giné},
 journal = {Bernoulli},
 number = {1},
 pages = {113--167},
 publisher = {International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
 title = {Random Matrix Approximation of Spectra of Integral Operators},
 volume = {6},
 year = {2000}
}


@INPROCEEDINGS{7098875,
  author={Roy, Olivier and Vetterli, Martin},
  booktitle={2007 15th European Signal Processing Conference}, 
  title={The effective rank: A measure of effective dimensionality}, 
  year={2007},
  volume={},
  number={},
  pages={606-610},
  nodoi ={}}

@inproceedings{NEURIPS2021_14faf969,
 author = {Velikanov, Maksim and Yarotsky, Dmitry},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {2570--2582},
 publisher = {Curran Associates, Inc.},
 title = {Explicit loss asymptotics in the gradient descent training of neural networks},
 url = {https://proceedings.neurips.cc/paper/2021/file/14faf969228fc18fcd4fcf59437b0c97-Paper.pdf},
 volume = {34},
 year = {2021}
}



@inproceedings{marco,
 author = {Nguyen, Quynh and Mondelli, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11961--11972},
 publisher = {Curran Associates, Inc.},
 title = {Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology},
 url = {https://proceedings.neurips.cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{marco-OLD,
author = {Nguyen, Quynh and Mondelli, Marco},
title = {Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology},
year = {2020},
noisbn = {9781713829546},
publisher = {Curran Associates Inc.},
noaddress = {Red Hook, NY, USA},
abstract = {Recent works have shown that gradient descent can find a global minimum for over-parameterized neural networks where the widths of all the hidden layers scale polynomially with N (N being the number of training samples). In this paper, we prove that, for deep networks, a single layer of width N following the input layer suffices to ensure a similar guarantee. In particular, all the remaining layers are allowed to have constant widths, and form a pyramidal topology. We show an application of our result to the widely used LeCun's initialization and obtain an over-parameterization requirement for the single wide layer of order N2.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1003},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}


@book{bhatia97,
  added-at = {2013-06-15T01:58:38.000+0200},
  author = {Bhatia, Rajendra},
  biburl = {https://www.bibsonomy.org/bibtex/269934a372db92a018132c5880987691e/ytyoun},
  interhash = {a52e63731d9a0e304c29b795ed54cf94},
  intrahash = {69934a372db92a018132c5880987691e},
  noisbn = {0387948465},
  keywords = {courant-fischer eigenvalues linear.algebra majorization matrix textbook},
  publisher = {Springer},
  timestamp = {2017-02-13T08:18:47.000+0100},
  title = {Matrix Analysis},
  volume = 169,
  year = 1997
}


@InProceedings{10.5555/3524938.3525002,
  title = 	 {Frequency Bias in Neural Networks for Input of Non-Uniform Density},
  author =       {Basri, Ronen and Galun, Meirav and Geifman, Amnon and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {685--694},
  year = 	 {2020},
  noeditor =	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/basri20a/basri20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/basri20a.html},
  abstract = 	 {Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias – networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency $\kappa$, convergence at a point $x \in \S^{d-1}$ occurs in time $O(\kappa^d/p(x))$ where $p(x)$ denotes the local density at $x$. Specifically, for data in $\S^1$ we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.}
}

@inproceedings{10.5555/3524938.3525002-OLD,
author = {Basri, Ronen and Galun, Meirav and Geifman, Amnon and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
title = {Frequency Bias in Neural Networks for Input of Non-Uniform Density},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias - networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency κ, convergence at a point x ∈ Sd-1 occurs in time O(κd/p(x)) where p(x) denotes the local density at x. Specifically, for data in S1 we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {64},
numpages = {10},
series = {ICML'20}
}

@misc{laplace_ntk,
  nodoi = {10.48550/ARXIV.2007.01580},
  
  url = {https://arxiv.org/abs/2007.01580},
  
  author = {Geifman, Amnon and Yadav, Abhay and Kasten, Yoni and Galun, Meirav and Jacobs, David and Basri, Ronen},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Similarity between the Laplace and Neural Tangent Kernels},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{freq_bias_non_uniform,
  nodoi = {10.48550/ARXIV.2003.04560},
  
  url = {https://arxiv.org/abs/2003.04560},
  
  author = {Basri, Ronen and Galun, Meirav and Geifman, Amnon and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Frequency Bias in Neural Networks for Input of Non-Uniform Density},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{neal1996,
author = {Neal, Radford M.},
title = {Bayesian Learning for Neural Networks},
year = {1996},
noisbn = {0387947248},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

  
  
@inproceedings{Poole2016,
 author = {Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Exponential expressivity in deep neural networks through transient chaos},
 url = {https://proceedings.neurips.cc/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf},
 volume = {29},
 year = {2016}
}


@inproceedings{samuel2017,
title={Deep Information Propagation},
author={Samuel S. Schoenholz and Justin Gilmer and Surya Ganguli and Jascha Sohl-Dickstein},
booktitle={International Conference on Learning Representations (ICLR)},
year={2017},
nomonth={April},
url ={https://openreview.net/pdf?id=H1W1UN9gg},
}


@article{MURRAY2022117,
title = {Activation function design for deep networks: linearity and effective initialisation},
journal = {Applied and Computational Harmonic Analysis},
volume = {59},
pages = {117-154},
year = {2022},
note = {Special Issue on Harmonic Analysis and Machine Learning},
noissn = {1063-5203},
nodoi = {https://doi.org/10.1016/j.acha.2021.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S1063520321001111},
author = {M. Murray and V. Abrol and J. Tanner},
keywords = {Activation function design, Deep learning, Initialisation, Random networks},
abstract = {The activation function deployed in a deep neural network has great influence on the performance of the network at initialisation, which in turn has implications for training. In this paper we study how to avoid two problems at initialisation identified in prior works: rapid convergence of pairwise input correlations, and vanishing and exploding gradients. We prove that both these problems can be avoided by choosing an activation function possessing a sufficiently large linear region around the origin, relative to the bias variance σb2 of the network's random initialisation. We demonstrate empirically that using such activation functions leads to tangible benefits in practice, both in terms of test and training accuracy and in terms of training time. Furthermore, we observe that the shape of the nonlinear activation outside the linear region appears to have a relatively limited impact on training. Finally, our results also allow us to train networks in a new hyperparameter regime, with a much larger bias variance than has previously been possible.}
}

@book{PDLT-2022,
    title = "The Principles of Deep Learning Theory",
    author = "Roberts, Daniel A. and Yaida, Sho and Hanin, Boris",
    publisher = "Cambridge University Press",
    year = "2022",
    eprint = "2106.10165",
    archivePrefix = "arXiv",
    primaryClass = "cs.LG",
    note={\url{https://deeplearningtheory.com}},
}

@inproceedings{Hayou2019OnTI,
  title={On the Impact of the Activation Function on Deep Neural Networks Training},
  author={Soufiane Hayou and A. Doucet and Judith Rousseau},
  booktitle={ICML},
  year={2019}
}


@InProceedings{pmlr-v125-woodworth20a,
  title = 	 {Kernel and Rich Regimes in Overparametrized Models},
  author =       {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3635--3673},
  year = 	 {2020},
  noeditor =	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth =	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/woodworth20a/woodworth20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/woodworth20a.html},
  abstract = 	 { A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e.&nbsp;when  during training the network behaves as a kernelized linear predictor, and thus, training with gradient descent has the effect of finding the corresponding minimum RKHS norm solution.  This stands in contrast to other studies which demonstrate how gradient descent on overparametrized  networks can induce rich implicit biases that are not RKHS norms.  Building on an observation by \citet{chizat2018note}, we show how the \textbf{\textit{scale of the initialization}} controls the transition between the “kernel” (aka lazy) and “rich” (aka active) regimes and affects generalization properties in multilayer homogeneous models. We provide a complete and detailed analysis for a family of simple depth-$D$ linear networks that exhibit an interesting and meaningful transition between the kernel and rich regimes, and highlight an interesting role for the \emph{width}  of the models. We further demonstrate this transition empirically for matrix factorization and multilayer non-linear networks.}
}


@InProceedings{pmlr-v9-glorot10a,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  noeditor =	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  noaddress = 	 {Chia Laguna Resort, Sardinia, Italy},
  nomonth =	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@InBook{LeCuBottOrrMull9812,
  author    = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  editor    = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  pages     = {9--48},
  publisher = {Springer Berlin Heidelberg},
  title     = {Efficient BackProp},
  year      = {2012},
  address   = {Berlin, Heidelberg},
  noisbn      = {978-3-642-35289-8},
  abstract  = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.},
  booktitle = {Neural Networks: Tricks of the Trade: Second Edition},
  nodoi       = {10.1007/978-3-642-35289-8_3},
  url       = {https://doi.org/10.1007/978-3-642-35289-8_3},
}


@INPROCEEDINGS{7410480,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
  year={2015},
  volume={},
  number={},
  pages={1026-1034},
  nodoi ={10.1109/ICCV.2015.123}}
  
 @inproceedings{NEURIPS2020_b7ae8fec,
 author = {Liu, Chaoyue and Zhu, Libin and Belkin, Misha},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15954--15964},
 publisher = {Curran Associates, Inc.},
 title = {On the linearity of large non-linear models: when and why the tangent kernel is constant},
 url = {https://proceedings.neurips.cc/paper/2020/file/b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{DBLP:journals/corr/MishkinM15,
  author    = {Dmytro Mishkin and
               Jiri Matas},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {All you need is a good init},
  booktitle = {4th International Conference on Learning Representations, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.06422},
  timestamp = {Thu, 25 Jul 2019 14:25:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MishkinM15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Lee2019WideNN-SHORT,
 author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{Lee2019WideNN-SHORT-OLD, 
  title={Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
  author={Jaehoon Lee and Lechao Xiao and Samuel S. Schoenholz and Yasaman Bahri and Roman Novak and Jascha Sohl-Dickstein and Jascha Sohl-Dickstein},
  booktitle={NeurIPS},
  year={2019}
}

@article{solt_mod_over,
place = {}, 
title = {Toward Moderate Overparameterization: Global Convergence Guarantees for Training Shallow Neural Networks}, 
url = {https://par.nsf.gov/biblio/10200049}, 
nodoi = {10.1109/JSAIT.2020.2991332}, 
abstractNote = {}, 
journal = {IEEE Journal on Selected Areas in Information Theory}, 
volume = {1}, 
number = {1}, 
author = {Oymak, Samet and Soltanolkotabi, Mahdi}, 
year = {2020}}


@book {rudin,
    AUTHOR = {Rudin, Walter},
     TITLE = {Principles of mathematical analysis},
 PUBLISHER = {McGraw-Hill Book Company, Inc., New York-Toronto-London},
      YEAR = {1953},
     PAGES = {ix+227},
   MRCLASS = {27.2X},
  MRNUMBER = {0055409 (14,1070c)},
MRREVIEWER = {U. S. Haslam-Jones},
  BOEKCODE = {26-01},
}

@article{hille,
 noissn = {0003486X},
 URL = {http://www.jstor.org/stable/1967695},
 author = {Einar Hille},
 journal = {Annals of Mathematics},
 number = {4},
 pages = {427--464},
 publisher = {Annals of Mathematics},
 title = {A Class of Reciprocal Functions},
 urldate = {2022-08-09},
 volume = {27},
 year = {1926}
}

@article{hillecontributions,
  author  = {Einar Hille},
  title   = {Contributions to the theory of {H}ermitian series. II. The Representation Problem},
  journal = {Transactions of the American Mathematical Society},
  year    = {1940},
  volume = {47},
  number  = {1},
  pages   = {80-94},
}


@article{rosasco10a,
  author  = {Lorenzo Rosasco and Mikhail Belkin and Ernesto De Vito},
  title   = {On Learning with Integral Operators},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {30},
  pages   = {905--934},
  url     = {http://jmlr.org/papers/v11/rosasco10a.html}
}


@article{doi:10.1137/1007133,
author = {Wimp, Jet},
title = {Special Functions and Their Applications (N. N. Lebedev)},
journal = {SIAM Review},
volume = {7},
number = {4},
pages = {577-580},
year = {1965},
nodoi = {10.1137/1007133},

URL = { 
        https://doi.org/10.1137/1007133
    
},
eprint = { 
        https://doi.org/10.1137/1007133
    
}

}


@article{Weyl,
	Author = {Weyl, Hermann},
	Da = {1912/12/01},
	Date-Added = {2022-07-26 07:35:53 +0000},
	Date-Modified = {2022-07-26 07:35:53 +0000},
	nodoi = {10.1007/BF01456804},
	Id = {Weyl1912},
	noisbn = {1432-1807},
	Journal = {Mathematische Annalen},
	Number = {4},
	Pages = {441--479},
	Title = {Das asymptotische {V}erteilungsgesetz der {E}igenwerte linearer partieller {D}ifferentialgleichungen (mit einer {A}nwendung auf die {T}heorie der {H}ohlraumstrahlung)},
	Ty = {JOUR},
	Url = {https://doi.org/10.1007/BF01456804},
	Volume = {71},
	Year = {1912},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF01456804}}


@book{folland1999,
  title     = "Real analysis: Modern techniques and their applications",
  author    = "Folland, G. B.",
  year      = 1999,
  publisher = "Wiley",
  address   = "New York"
}

@article{face_split_prod,
author = {Slyusar, Vadym},
year = {1998},
nomonth ={03},
pages = {71-75},
title = {End matrix products in radar applications},
volume = {41},
journal = {Izvestiya Vysshikh Uchebnykh Zavedenij. Radioelektronika}
}


@article{Schur1911,
author = {Schur, J.},
journal = {Journal für die reine und angewandte Mathematik},
pages = {1-28},
title = {Bemerkungen zur {T}heorie der beschränkten {B}ilinearformen mit unendlich vielen {V}eränderlichen.},
url = {http://eudml.org/doc/149352},
volume = {140},
year = {1911},
}

@article{torchNTK,
author = {Engel, Andrew and Wang, Zhichao and Sarwate, Anand and Choudhury, Sutanay and Chiang, Tony},
year = {2022},
nomonth ={05},
pages = {},
title = {{TorchNTK}: A Library for Calculation of Neural Tangent Kernels of {PyTorch} Models},
nodoi = {10.48550/arXiv.2205.12372}
}


@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.}
}


@InProceedings{pmlr-v162-novak22a,
  title = 	 {Fast Finite Width Neural Tangent Kernel},
  author =       {Novak, Roman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {17018--17044},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/novak22a/novak22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/novak22a.html},
  abstract = 	 {The Neural Tangent Kernel (NTK), defined as the outer product of the neural network (NN) Jacobians, has emerged as a central object of study in deep learning. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. At finite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architecture search, and do meta-learning. Unfortunately, the finite width NTK is notoriously expensive to compute, which severely limits its practical utility. We perform the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks. Leveraging the structure of neural networks, we further propose two novel algorithms that change the exponent of the compute and memory requirements of the finite width NTK, dramatically improving efficiency. Our algorithms can be applied in a black box fashion to any differentiable function, including those implementing neural networks. We open-source our implementations within the Neural Tangents package at https://github.com/google/neural-tangents.}
}


@misc{
novak2022fast,
title={Fast Finite Width Neural Tangent Kernel},
author={Roman Novak and Jascha Sohl-Dickstein and Samuel Stern Schoenholz},
year={2022},
url={https://openreview.net/forum?id=zLb9oSWy933}
}


@inproceedings{dual_view,
 author = {Daniely, Amit and Frostig, Roy and Singer, Yoram},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity},
 url = {https://proceedings.neurips.cc/paper/2016/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf},
 volume = {29},
 year = {2016}
}

@book{lebedev1972special,
  title={Special Functions and Their Applications},
  author={Lebedev, N.N. and Silverman, R.A.},
  noisbn={9780486606248},
  lccn={lc72086228},
  series={Dover Books on Mathematics},
  url={https://books.google.co.uk/books?id=po-6Yxz851MC},
  year={1972},
  publisher={Dover Publications}
}

@article{wallis_ratio,
title = {Completely monotone functions and the {W}allis ratio},
journal = {Applied Mathematics Letters},
volume = {25},
number = {4},
pages = {717-722},
year = {2012},
noissn = {0893-9659},
nodoi = {https://doi.org/10.1016/j.aml.2011.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893965911004964},
author = {Cristinel Mortici},
keywords = {Gamma function, Wallis ratio, Ratio of gamma functions, Polygamma functions, Completely monotonic functions, Kazarinoff’s inequality},
abstract = {The aim of the paper is to improve known estimates of the Wallis ratio. Moreover, we show that these improvements are valid, because certain functions involving the continuous version of the Wallis ratio are completely monotone.}
}


@article{kazarinoff_1956, title={On {W}allis' formula}, volume={40}, nodoi={10.1017/S095018430000029X}, journal={Edinburgh Mathematical Notes}, publisher={Cambridge University Press}, author={Kazarinoff, Donat K.}, year={1956}, pages={19–21}}

@InProceedings{allenzhu2019convergence,
  title = 	 {A Convergence Theory for Deep Learning via Over-Parameterization},
  author =       {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {242--252},
  year = 	 {2019},
  noeditor =	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth =	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/allen-zhu19a/allen-zhu19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/allen-zhu19a.html},
  abstract = 	 {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps &nbsp; e^{-T}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).}
}

@InProceedings{nguyenrelu,
  title = 	 {On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths},
  author =       {Nguyen, Quynh},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8056--8062},
  year = 	 {2021},
  noeditor =	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth =	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nguyen21a/nguyen21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nguyen21a.html},
  abstract = 	 {We give a simple proof for the global convergence of gradient descent in training deep ReLU networks with the standard square loss, and show some of its improvements over the state-of-the-art. In particular, while prior works require all the hidden layers to be wide with width at least $\Omega(N^8)$ ($N$ being the number of training samples), we require a single wide layer of linear, quadratic or cubic width depending on the type of initialization. Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof need not track the evolution of the entire NTK matrix, or more generally, any quantities related to the changes of activation patterns during training. Instead, we only need to track the evolution of the output at the last hidden layer, which can be done much more easily thanks to the Lipschitz property of ReLU. Some highlights of our setting: (i) all the layers are trained with standard gradient descent, (ii) the network has standard parameterization as opposed to the NTK one, and (iii) the network has a single wide layer as opposed to having all wide hidden layers as in most of NTK-related results.}
}


@inproceedings{zou2019improved,
 author = {Zou, Difan and Gu, Quanquan},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {An Improved Analysis of Training Over-parameterized Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{bayesianconv,
  author    = {Roman Novak and
               Lechao Xiao and
               Yasaman Bahri and
               Jaehoon Lee and
               Greg Yang and
               Jiri Hron and
               Daniel A. Abolafia and
               Jeffrey Pennington and
               Jascha Sohl{-}Dickstein},
  title     = {Bayesian Deep Convolutional Networks with Many Channels are {G}aussian Processes},
  booktitle = {7th International Conference on Learning Representations},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=B1g30j0qF7},
  timestamp = {Mon, 17 Jan 2022 07:45:14 +0100},
  biburl    = {https://dblp.org/rec/conf/iclr/NovakXBLYHAPS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2020_572201a4,
 author = {Fan, Zhou and Wang, Zhichao},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7710--7721},
 publisher = {Curran Associates, Inc.},
 title = {Spectra of the Conjugate Kernel and Neural Tangent Kernel for linear-width neural networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/572201a4497b0b9f02d4f279b09ec30d-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{https://doi.org/10.48550/arxiv.1907.10599,
  doi = {10.48550/ARXIV.1907.10599},
  
  url = {https://arxiv.org/abs/1907.10599},
  
  author = {Yang, Greg and Salman, Hadi},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Fine-Grained Spectral Perspective on Neural Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{Sontag89backpropagationcan,
    author = {Eduardo D. Sontag and Héctor J. Sussmann},
    title = {Backpropagation Can Give Rise To Spurious Local Minima Even For Networks Without Hidden Layers},
    journal = {Complex Systems},
    year = {1989},
    volume = {3},
    pages = {91--106}
}

@book{DBLP:books/daglib/0025992,
  author    = {Martin Anthony and
               Peter L. Bartlett},
  title     = {Neural Network Learning - Theoretical Foundations},
  publisher = {Cambridge University Press},
  year      = {2002},
  url       = {http://www.cambridge.org/gb/knowledge/isbn/item1154061/?site\_locale=en\_GB},
  noisbn      = {978-0-521-57353-5},
  timestamp = {Thu, 05 May 2011 16:48:09 +0200},
  biburl    = {https://dblp.org/rec/books/daglib/0025992.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{journals/jmlr/BartlettM02,
  added-at = {2019-07-10T00:00:00.000+0200},
  author = {Bartlett, Peter L. and Mendelson, Shahar},
  biburl = {https://www.bibsonomy.org/bibtex/2cabe2eb75c513a9eb8dabe7bedc0ab7f/dblp},
  ee = {http://jmlr.org/papers/v3/bartlett02a.html},
  interhash = {dcb2762041bdff7b5d4db5058ee347e3},
  intrahash = {cabe2eb75c513a9eb8dabe7bedc0ab7f},
  journal = {J. Mach. Learn. Res.},
  keywords = {dblp},
  pages = {463-482},
  timestamp = {2019-07-11T11:42:31.000+0200},
  title = {Rademacher and Gaussian Complexities: Risk Bounds and Structural Results.},
  url = {http://dblp.uni-trier.de/db/journals/jmlr/jmlr3.html#BartlettM02},
  volume = 3,
  year = 2002
}





@inproceedings{10.5555/3495724.3496995,
 author = {Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15156--15172},
 publisher = {Curran Associates, Inc.},
 title = {Finite Versus Infinite Neural Networks: an Empirical Study},
 url = {https://proceedings.neurips.cc/paper/2020/file/ad086f59924fffe0773f8d0ca22ea712-Paper.pdf},
 volume = {33},
 year = {2020}
}



@inproceedings{10.5555/3495724.3496995-OLD,
author = {Lee, Jaehoon and Schoenholz, Samuel S. and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
title = {Finite versus Infinite Neural Networks: An Empirical Study},
year = {2020},
noisbn = {9781713829546},
publisher = {Curran Associates Inc.},
noaddress = {Red Hook, NY, USA},
abstract = {We perform a careful, thorough, and large scale empirical study of the correspondence between wide neural networks and kernel methods. By doing so, we resolve a variety of open questions related to the study of infinitely wide neural networks. Our experimental results include: kernel methods outperform fully-connected finite-width networks, but underperform convolutional finite width networks; neural network Gaussian process (NNGP) kernels frequently outperform neural tangent (NT) kernels; centered and ensembled finite networks have reduced posterior variance and behave more similarly to infinite networks; weight decay and the use of a large learning rate break the correspondence between finite and infinite networks; the NTK parameterization outperforms the standard parameterization for finite width networks; diagonal regularization of kernels acts similarly to early stopping; floating point precision limits kernel performance beyond a critical dataset size; regularized ZCA whitening improves accuracy; finite network performance depends non-monotonically on width in ways not captured by double descent phenomena; equivariance of CNNs is only beneficial for narrow networks far from the kernel regime. Our experiments additionally motivate an improved layer-wise scaling for weight decay which improves generalization in finite-width networks. Finally, we develop improved best practices for using NNGP and NT kernels for prediction, including a novel ensembling technique. Using these best practices we achieve state-of-the-art results on CIFAR-10 classification for kernels corresponding to each architecture class we consider.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1271},
numpages = {17},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}


@InProceedings{pmlr-v108-li20j,
  title = 	 {Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks},
  author =       {Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4313--4324},
  year = 	 {2020},
  noeditor =	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth =	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/li20j/li20j.pdf},
  url = 	 {https://proceedings.mlr.press/v108/li20j.html},
  abstract = 	 {Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including significantly corrupted ones. Despite this (over)fitting capacity in this paper we demonstrate that such overparameterized networks have an intriguing robustness capability: they are surprisingly robust to label noise when first order methods with early stopping is used to train them. This paper also takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) To start to overfit to the noisy labels network must stray rather far from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.}
}

@inbook{vershynin2011introduction, 
place={Cambridge}, 
title={Introduction to the non-asymptotic analysis of random matrices}, 
noDOI={10.1017/CBO9780511794308.006}, 
booktitle={Compressed Sensing: Theory and Applications}, 
publisher={Cambridge University Press}, 
author={Vershynin, Roman}, 
noeditor ={Eldar, Yonina C. and Kutyniok, GittaEditors}, 
year={2012}, 
pages={210–268}}

@misc{vershynin2011introduction-OLD,
      title={Introduction to the non-asymptotic analysis of random matrices}, 
      author={Roman Vershynin},
      year={2011},
      eprint={1011.3027},
      archivePrefix={arXiv},
      primaryClass={math.PR}
}

@article{DBLP:journals/simods/HuangHV22,
  author    = {Ningyuan Teresa Huang and
               David W. Hogg and
               Soledad Villar},
  title     = {Dimensionality Reduction, Regularization, and Generalization in Overparameterized Regressions},
  journal   = {{SIAM} J. Math. Data Sci.},
  volume    = {4},
  number    = {1},
  pages     = {126--152},
  year      = {2022},
  url       = {https://doi.org/10.1137/20m1387821},
  nodoi       = {10.1137/20m1387821},
  timestamp = {Thu, 30 Jun 2022 16:57:50 +0200},
  biburl    = {https://dblp.org/rec/journals/simods/HuangHV22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{davis_hermite,
author = {Davis, Tom},
year = {2021},
nomonth = {06},
pages = {},
title = {A GENERAL EXPRESSION FOR {H}ERMITE EXPANSIONS WITH APPLICATIONS},
doi = {10.13140/RG.2.2.30843.44325},
url = {https://www.researchgate.net/profile/Tom-Davis-2/publication/352374514_A_GENERAL_EXPRESSION_FOR_HERMITE_EXPANSIONS_WITH_APPLICATIONS/links/60c873c5a6fdcc8267cf74d4/A-GENERAL-EXPRESSION-FOR-HERMITE-EXPANSIONS-WITH-APPLICATIONS.pdf}
}

@inproceedings{pennington_shallow,
 author = {Pennington, Jeffrey and Worah, Pratik},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Spectrum of the {F}isher Information Matrix of a Single-Hidden-Layer Neural Network},
 url = {https://proceedings.neurips.cc/paper/2018/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf},
 volume = {31},
 year = {2018}
}


@article{10.2307/26542333,
 noissn = {10505164, 21688737},
 URL = {https://www.jstor.org/stable/26542333},
 abstract = {This article studies the Gram random matrix model G= 1 T Σ ⊺ Σ,Σ=σ(WX) , classically found in the analysis of random feature maps and random neural networks, where X = [x₁,...,xT] ∈ ℝp×T is a (data) matrix of bounded norm, W ∈ ℝn×p is a matrix of independent zero-mean unit variance entries and σ : ℝ → ℝ is a Lipschitz continuous (activation) function—σ(WX) being understood entry-wise. By means of a key concentration of measure lemma arising from nonasymptotic random matrix arguments, we prove that, as n, p, T grow large at the same rate, the resolvent Q = (G + γIT)−1, for γ > 0, has a similar behavior as that met in sample covariance matrix models, involving notably the moment Φ= T n E[G] , which provides in passing a deterministic equivalent for the empirical spectral measure of G. Application-wise, this result enables the estimation of the asymptotic performance of single-layer random neural networks. This in turn provides practical insights into the underlying mechanisms into play in random neural networks, entailing several unexpected consequences, as well as a fast practical means to tune the network hyperparameters.},
 author = {Cosme Louart and Zhenyu Liao and Romain Couillet},
 journal = {The Annals of Applied Probability},
 number = {2},
 pages = {1190--1248},
 publisher = {Institute of Mathematical Statistics},
 title = {A RANDOM MATRIX APPROACH TO NEURAL NETWORKS},
 urldate = {2022-09-22},
 volume = {28},
 year = {2018}
}



@inproceedings{pennington_nonlinear,
 author = {Pennington, Jeffrey and Worah, Pratik},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Nonlinear random matrix theory for deep learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{DBLP:journals/corr/abs-1906-05392,
  author    = {Samet Oymak and
               Zalan Fabian and
               Mingchen Li and
               Mahdi Soltanolkotabi},
  title     = {Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the {J}acobian},
  journal   = {CoRR},
  volume    = {abs/1906.05392},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05392},
  eprinttype = {arXiv},
  eprint    = {1906.05392},
  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-05392.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Karakida_2020,
	nodoi = {10.1088/1742-5468/abc62e},
	url = {https://doi.org/10.1088/1742-5468/abc62e},
	year = 2020,
	nomonth = {dec},
	publisher = {{IOP} Publishing},
	volume = {2020},
	number = {12},
	pages = {124005},
	author = {Ryo Karakida and Shotaro Akaho and Shun-ichi Amari},
	title = {Universal statistics of {F}isher information in deep neural networks: mean field approach},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	abstract = {The {F}isher information matrix (FIM) is a fundamental quantity to represent the characteristics of a stochastic model, including deep neural networks (DNNs). The present study reveals novel statistics of FIM that are universal among a wide class of DNNs. To this end, we use random weights and large width limits, which enables us to utilize mean field theories. We investigate the asymptotic statistics of the FIM’s eigenvalues and reveal that most of them are close to zero while the maximum eigenvalue takes a huge value. Because the landscape of the parameter space is defined by the FIM, it is locally flat in most dimensions, but strongly distorted in others. Moreover, we demonstrate the potential usage of the derived statistics in learning strategies. First, small eigenvalues that induce flatness can be connected to a norm-based capacity measure of generalization ability. Second, the maximum eigenvalue that induces the distortion enables us to quantitatively estimate an appropriately sized learning rate for gradient methods to converge.}
}

@inproceedings{
jin2022learning,
title={Learning Curves for Gaussian Process Regression with Power-Law Priors and Targets},
author={Hui Jin and Pradeep Kr. Banerjee and Guido Mont\'ufar},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=KeI9E-gsoB}
}

@book{book_RBF,
author = {Chen, Wen and Fu, Zhuo-Jia and Chen, Ching-Shyang},
year = {2013},
month = {12},
pages = {},
title = {Recent Advances in Radial Basis Function Collocation Methods},
noisbn = {978-3-642-39571-0},
doi = {10.13140/2.1.2862.1602}
}

@inproceedings{
bietti2021deep,
title={Deep Equals Shallow for {ReLU} Networks in Kernel Regimes},
author={Alberto Bietti and Francis Bach},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=aDjoksTpXOP}
}

@inproceedings{geifman2020similarity,
 author = {Geifman, Amnon and Yadav, Abhay and Kasten, Yoni and Galun, Meirav and Jacobs, David and Ronen, Basri},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1451--1461},
 publisher = {Curran Associates, Inc.},
 title = {On the Similarity between the {L}aplace and Neural Tangent Kernels},
 url = {https://proceedings.neurips.cc/paper/2020/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{geifman2020similarity-OLD,
  title={On the similarity between the laplace and neural tangent kernels},
  author={Geifman, Amnon and Yadav, Abhay and Kasten, Yoni and Galun, Meirav and Jacobs, David and Ronen, Basri},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1451--1461},
  year={2020}
}


@inproceedings{cui2021generalization,
title={Generalization Error Rates in Kernel Regression: The Crossover from the Noiseless to Noisy Regime},
author={Hugo Cui and Bruno Loureiro and Florent Krzakala and Lenka Zdeborov{\'a}},
booktitle={Advances in Neural Information Processing Systems},
noeditor ={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=Da_EHrAcfwd}
}

@article{cui2021generalization-OLD,
  title={Generalization Error Rates in Kernel Regression: {T}he Crossover from the Noiseless to Noisy Regime},
  author={Cui, Hugo and Loureiro, Bruno and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:2105.15004},
  year={2021}
}

@article{caponnetto2007optimal,
  title={Optimal rates for the regularized least-squares algorithm},
  author={Caponnetto, Andrea and De Vito, Ernesto},
  journal={Foundations of Computational Mathematics},
  volume={7},
  number={3},
  pages={331--368},
  year={2007}
}


@inproceedings{bietti2019inductive,
 author = {Bietti, Alberto and Mairal, Julien},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the Inductive Bias of Neural Tangent Kernels},
 url = {https://proceedings.neurips.cc/paper/2019/file/c4ef9c39b300931b69a36fb3dbb8d60e-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{bietti2019inductive-OLD,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@InProceedings{pmlr-v162-simon22a,
  title = 	 {Reverse Engineering the Neural Tangent Kernel},
  author =       {Simon, James Benjamin and Anand, Sajant and Deweese, Mike},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {20215--20231},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/simon22a/simon22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/simon22a.html},
  abstract = 	 {The development of methods to guide the design of neural networks is an important open challenge for deep learning theory. As a paradigm for principled neural architecture design, we propose the translation of high-performing kernels, which are better-understood and amenable to first-principles design, into equivalent network architectures, which have superior efficiency, flexibility, and feature learning. To this end, we constructively prove that, with just an appropriate choice of activation function, any positive-semidefinite dot-product kernel can be realized as either the NNGP or neural tangent kernel of a fully-connected neural network with only one hidden layer. We verify our construction numerically and demonstrate its utility as a design tool for finite fully-connected networks in several experiments.}
}

@inproceedings{
han2022fast,
title={Fast Neural Kernel Embeddings for General Activations},
author={Insu Han and Amir Zandieh and Jaehoon Lee and Roman Novak and Lechao Xiao and Amin Karbasi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=yLilJ1vZgMe}
}

@misc{li_andreeto_ranzato_perona_2022, title={Caltech 101}, DOI={10.22002/D1.20086}, abstractNote={Pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco Andreetto, and Marc'Aurelio Ranzato. The size of each image is roughly 300 x 200 pixels. We have carefully clicked outlines of each object in these pictures, these are included under the 'Annotations.tar'. There is also a MATLAB script to view the annotations, 'show_annotations.m'.}, publisher={CaltechDATA}, author={Li and Andreeto and Ranzato and Perona}, year={2022}, month={Apr} }


@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep {ReLU} networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020},
  publisher={Springer}
}

@inproceedings{
chen2021deep,
title={Deep Neural Tangent Kernel and Laplace Kernel Have the Same {RKHS}},
author={Lin Chen and Sheng Xu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=vK9WrZ0QYQ}
}

@inproceedings{
geifman2022on,
title={On the Spectral Bias of Convolutional Neural Tangent and Gaussian Process Kernels},
author={Amnon Geifman and Meirav Galun and David Jacobs and Ronen Basri},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=gthKzdymDu2}
}

@inproceedings{scetbon2021spectral,
  title={A spectral analysis of dot-product kernels},
  author={Scetbon, Meyer and Harchaoui, Zaid},
  booktitle={International conference on artificial intelligence and statistics},
  pages={3394--3402},
  year={2021},
  organization={PMLR}
}

@article{azevedo2015eigenvalues,
  title={Eigenvalues of dot-product kernels on the sphere},
  author={Azevedo, Douglas and Menegatto, Valdir A},
  journal={Proceeding Series of the Brazilian Society of Computational and Applied Mathematics},
  volume={3},
  number={1},
  year={2015}
}