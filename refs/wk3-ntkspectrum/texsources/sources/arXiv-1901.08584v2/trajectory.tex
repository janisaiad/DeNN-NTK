In this section we present our main characterization of the trajectory.
Our key finding is that both trajectories $\left\{\mat{W}(k)\right\}_{k=0}^\infty$ and $\left\{\vect{u}(k)\right\}_{k=0}^\infty$ can be described by perturbed linear dynamical systems where the perturbation goes to $0$ as the width $m$ goes to infinity.

In the following two subsections, we will give the reference linear dynamical system, then give a perturbation bound that characterize the difference between the actual trajectory generated by the gradient descent algorithm and the reference linear dynamical system.

\subsection{Trajectory of Predictions}
\label{sec:traj_pred}
By some standard calculation, one can find the trajectory of predictions is defined as 
\begin{align*}
	\vect{u}(k+1) = \vect{u}(k) - \eta \mat{H}(k)\left(\vect{u}(k)-\vect{y}\right).
\end{align*}
Now we define our ``unperturbed" linear dynamical system of the trajectory of predictions $\{\tilde{\vect{u}}(k)\}_{k=0}^\infty$
\begin{align}
	\tilde{\vect{u}}(0) = &\vect{u}(0), \nonumber\\
	\tilde{\vect{u}}(k+1) = &\tilde{\vect{u}}(k) - \eta \mat{H}(0)\left(\tilde{\vect{u}}(k)-\vect{y}\right). \label{eqn:u_tilde_dynamics}
\end{align}
Recall $\mat{H}_{ij}(0) = \frac{1}{m}\sum_{r=1}^{m} \indict_{r,i}\indict_{r,j}$ for $(i,j) \in [n] \times [n]$.
%\[\mat{H}^\infty_{ij} = \vect{x}_i^\top \vect{x}_j \expect_{\vect{w}\sim N(\vect{0},\mat{I})}\left[\indict\left\{\vect{w}^\top \vect{x}_i \ge 0, \vect{w}^\top \vect{x}_j \ge 0\right\}\right].\]
This trajectory $\{\tilde{u}(k)\}_{k=1}^{\infty}$ has the same initialization as $\left\{\vect{u}(k)\right\}_{k=0}^\infty$ but at each iteration, $\tilde{\vect{u}}(k)$ uses the initial Gram matrix $\mat{H}(0)$ to do the update and $\vect{u}(k)$ uses Gram matrix $\mat{H}(k)$ which varies at each iteration.
In \cite{du2018provably}, it was shown $\mat{H}(k)$ is close to $\mat{H}^\infty$ if $m$ is large.
Based on this observation, we can show the following perturbation theorem.
\begin{thm}\label{thm:traj_u}
\begin{align}
\norm{\tilde{\vect{u}}(k) - \vect{u}(k)}_2 \le \frac{\poly(n,1/\lambda_0)}{\sqrt{m}}, \forall k \ge 0. \label{eqn:traj_u_perturb}
\end{align}
\end{thm}
This theorem shows, to study the dynamics of $\vect{u}(k)$, we can instead study $\tilde{\vect{u}}(k)$.
Since $\tilde{\vect{u}}(k)$ is a \emph{linear} dynamical system, all information is contained in $\mat{H}(0)$ and initialization $\vect{u}(0)$, we can characterize $\tilde{\vect{u}}(k)$ in a quantitatively precise manner.
We will use this trajectory to explain why true labels converge faster than random labels in Section~\ref{sec:rate}. 






\subsection{Trajectory of Parameters}
\label{sec:traj_W}
Now we consider the dynamics of parameters.
We define the ``unperturbed" linear dynamical system as \begin{align*}
	\widetilde{\mat{W}}(k+1) &= \widetilde{\mat{W}}(k) - \eta\mat{Z}(0)\left(\tilde{\vect{u}}(k)-\vect{y}\right),\\ \widetilde{\mat{W}}(0) &= \mat{W}(0). 
\end{align*}
Recall $\mat{Z}(0)$ is the initial \emph{extended feature matrix}.
Similar to the trajectory of predictions, the following theorem shows $\widetilde{\mat{W}}(k)$ is close to $\mat{W}(k)$ for all$k = 0,1,\ldots,$. 
\begin{thm}\label{thm:traj_W}
For $k=0,1,\ldots$, we have \begin{align}
	\norm{\mat{W}(k)-\widetilde{\mat{W}}(k)}_F \le \frac{\poly(n,1/\lambda_0,\log(m))}{\sqrt{m}} \label{eqn:traj_W_perturb}
\end{align}
\end{thm}

This theorem establishes that if the width is sufficiently large, the linear dynamical system $\{\widehat{\mat{W}}(k)\}_{k=0}^\infty$ identifies the main characteristics of the parameter trajectory $\{\mat{W}(k)\}_{k=0}^\infty$ and therefore we can use this liner dynamical system to analyze the properties of the learned neural network.
In Section~\ref{sec:generalization}, we will exploit this idea to derive a generalization bound.









