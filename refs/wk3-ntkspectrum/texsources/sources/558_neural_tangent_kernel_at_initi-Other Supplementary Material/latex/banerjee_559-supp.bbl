\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{ZAZ-YL-ZS:19}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research},
  pages 242--252. PMLR, 2019.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and
  Wang]{SA-SD-WH-ZL-RW:19}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{SA-SSD-WH-ZL-RS-RW:19}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019{\natexlab{b}}.

\bibitem[Banerjee et~al.(2023)Banerjee, Cisneros-Velarde, Zhu, and
  Belkin]{AB-PCV-LZ-MB:22}
Arindam Banerjee, Pedro Cisneros-Velarde, Libin Zhu, and Misha Belkin.
\newblock Restricted strong convexity of deep learning models with smooth
  activations.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and Rakhlin]{PB-AM-AR:21}
Peter~L. Bartlett, Andrea Montanari, and Alexander Rakhlin.
\newblock Deep learning: a statistical viewpoint.
\newblock \emph{Acta Numerica}, 30:\penalty0 87â€“201, 2021.

\bibitem[Bombari et~al.(2022)Bombari, Amani, and
  Mondelli]{bombari2022memorization}
Simone Bombari, Mohammad~Hossein Amani, and Marco Mondelli.
\newblock Memorization and optimization in deep neural networks with minimum
  over-parameterization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{SD-JL-HL-LW-XZ:19}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research},
  pages 1675--1685. PMLR, 09--15 Jun 2019.

\bibitem[Fan et~al.(2021)Fan, Ma, and Zhong]{JF-CM-YZ:21}
Jianqing Fan, Cong Ma, and Yiqiao Zhong.
\newblock A selective overview of deep learning.
\newblock \emph{Statistical Science}, 36\penalty0 (2):\penalty0 264 -- 290,
  2021.

\bibitem[Fang et~al.(2021)Fang, Dong, and Zhang]{CF-HD-TZ:21}
Cong Fang, Hanze Dong, and Tong Zhang.
\newblock Mathematical models of overparameterized neural networks.
\newblock \emph{Proceedings of the IEEE}, 109\penalty0 (5):\penalty0 683--703,
  2021.

\bibitem[Huang and Yau(2020)]{JH-HY:20}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock In \emph{International conference on machine learning}, pages
  4542--4551. PMLR, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{AJ-FG-CH:18}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31. Curran Associates, Inc., 2018.

\bibitem[Ji and Telgarsky(2019)]{ZJ-MT:19}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{L}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{JL-LX-SS-YB-RN-JSD-JP:19}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{CL-LZ-MB:21}
C.~Liu, L.~Zhu, and M.~Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0
  85--116, 2022.

\bibitem[Liu et~al.(2020)Liu, Zhu, and Belkin]{CL-LZ-MB:20}
Chaoyue Liu, Libin Zhu, and Misha Belkin.
\newblock On the linearity of large non-linear models: when and why the tangent
  kernel is constant.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Montanari and Zhong(2020)]{AM-YZ:20}
Andrea Montanari and Yiqiao Zhong.
\newblock The interpolation phase transition in neural networks: Memorization
  and generalization under lazy training.
\newblock \emph{arXiv preprint arXiv:2007.12826}, 2020.

\bibitem[Nguyen and Mondelli(2020)]{ng2020hermite1}
Q.~Nguyen and M.~Mondelli.
\newblock Global convergence of deep networks with one wide layer followed by
  pyramidal topology.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Nguyen(2021)]{ng2021opt}
Quynh Nguyen.
\newblock On the proof of global convergence of gradient descent for deep relu
  networks with linear widths.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Nguyen et~al.(2021{\natexlab{a}})Nguyen, Br\'{e}chet, and
  Mondelli]{QN-PB-MM:21}
Quynh Nguyen, Pierre Br\'{e}chet, and Marco Mondelli.
\newblock When are solutions connected in deep networks?
\newblock In \emph{Advances in Neural Information Processing Systems}. Curran
  Associates, Inc., 2021{\natexlab{a}}.

\bibitem[Nguyen et~al.(2021{\natexlab{b}})Nguyen, Mondelli, and
  Montufar]{ng2021hermite2}
Quynh Nguyen, Marco Mondelli, and Guido Montufar.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel
  for deep relu networks.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2021{\natexlab{b}}.

\bibitem[Oymak and Soltanolkotabi(2020)]{oymak2020hermite}
S.~Oymak and M.~Soltanolkotabi.
\newblock Towards moderate overparameterization: global convergence guarantees
  for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 2020.

\bibitem[Zou and Gu(2019)]{DZ-QG:19}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{DZ-YC-DZ-QG:20}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{Machine Learning}, 109:\penalty0 467--492, 2020.

\end{thebibliography}
