\begin{thebibliography}{10}

\bibitem{arora2019:NNGP}
S.~Arora, S.~S. Du, W.~hu, Z.~Li, R.~R. Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  8141--8150, 2019.

\bibitem{arora2019:NTK}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, and R.~Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, pages 322--332, 2019.

\bibitem{avidan2023:connecting}
Y.~Avidan, Q.~Li, and H.~Sompolinsky.
\newblock Connecting {NTK} and {NNGP}: A unified theoretical framework for
  neural network learning dynamics in the kernel regime.
\newblock {\em arXiv:2309.04522}, 2023.

\bibitem{billingsley2013convergence}
P.~Billingsley.
\newblock {\em Convergence of Probability Measures}.
\newblock John Wiley \& Sons, 2013.

\bibitem{bracale2020:asymptotic}
D.~Bracale, S.~Favaro, S.~Fortini, and S.~Peluchetti.
\newblock Large-width functional asymptotics for deep gaussian neural networks.
\newblock In {\em Proceedings of the 8th International Conference on Learning
  Representations}, 2020.

\bibitem{cho2009:GP}
Y.~Cho and L.~Saul.
\newblock Kernel methods for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems 22}, pages
  342--350, 2009.

\bibitem{du2019:GNTK}
S.~S. Du, K.~Hou, R.~R. Salakhutdinov, B.~Poczos, R.~Wang, and K.~Xu.
\newblock Graph neural tangent kernel: Fusing graph neural networks with graph
  kernels.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  5723 -- 5733, 2019.

\bibitem{garriga2019:GP}
A.~Garriga-Alonso, C.~Rasmussen, and L.~Aitchison.
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations}, 2019.

\bibitem{hron2020:attention}
J.~Hron, Y.~Bahri, J.~Sohl-Dickstein, and R.~Novak.
\newblock Infinite attention: {NNGP} and {NTK} for deep attention networks.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, pages 4376--4386, 2020.

\bibitem{huang2021:NTK}
B.~Huang, X.~Li, Z.~Song, and X.~Yang.
\newblock {FL-NTK}: A neural tangent kernel-based framework for federated
  learning analysis.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, pages 4423--4434, 2021.

\bibitem{jacot2018:NTK}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  8580 -- 8589, 2018.

\bibitem{lee2018:NNGP}
J.~Lee, Y.~Bahri, R.~Novak, S.~S. Schoenholz, J.~Pennington, and
  J.~Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock In {\em Proceedings of the 6th International Conference on Learning
  Representations}, 2018.

\bibitem{lee2020finite}
J.~Lee, S.~Schoenholz, J.~Pennington, B.~Adlam, L.~Xiao, R.~Novak, and
  J.~Sohl-Dickstein.
\newblock Finite versus infinite neural networks: An empirical study.
\newblock In {\em Advances in Neural Information Processing Systems 33}, pages
  15156--15172, 2020.

\bibitem{mahankali2023:NTK}
A.~Mahankali, J.~Z. Haochen, K.~Dong, M.~Glasgow, and T.~Ma.
\newblock Beyond {NTK} with vanilla gradient descent: A mean-field analysis of
  neural networks with polynomial width, samples, and time.
\newblock {\em arXiv:2306.16361}, 2023.

\bibitem{malladi2023:NTK}
S.~Malladi, A.~Wettig, D.~Yu, D.~Chen, and S.~Arora.
\newblock A kernel-based view of language model fine-tuning.
\newblock In {\em Proceedings of the 40th International Conference on Machine
  Learning}, pages 23610--23641, 2023.

\bibitem{mezard1987:Replica}
M.~M{\'e}zard, G.~Parisi, and M.~A. Virasoro.
\newblock {\em Spin glass theory and beyond: An Introduction to the Replica
  Method and Its Applications}.
\newblock World Scientific Publishing Company, 1987.

\bibitem{neal1996:GP}
R.~M. Neal.
\newblock Priors for infinite networks.
\newblock {\em Bayesian Learning for Neural Networks}, pages 29--53, 1996.

\bibitem{nguyen2021:eigenvalues}
Q.~Nguyen, M.~Mondelli, and G.~Montufar.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel
  for deep relu networks.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, pages 8119--8129, 2021.

\bibitem{novak2018:GP}
R.~Novak, L.~Xiao, Y.~Bahri, J.~Lee, G.~Yang, J.~Hron, D.~A. Abolafia,
  J.~Pennington, and J.~Sohl-dickstein.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In {\em Proceedings of the 6th International Conference on Learning
  Representations}, 2018.

\bibitem{pang2019:NNGP}
G.~Pang, L.~Yang, and G.~E. Karniadakis.
\newblock Neural-net-induced gaussian process regression for function
  approximation and {PDE} solution.
\newblock {\em Journal of Computational Physics}, 384:270--288, 2019.

\bibitem{park2020:NNGP}
D.~S. Park, J.~Lee, D.~Peng, Y.~Cao, and J.~Sohl-Dickstein.
\newblock Towards {NNGP}-guided neural architecture search.
\newblock {\em arXiv:2011.06006}, 2020.

\bibitem{pleiss2022:NNGP}
G.~Pleiss and J.~P. Cunningham.
\newblock The limitations of large width in neural networks: A deep gaussian
  process perspective.
\newblock In {\em Advances in Neural Information Processing Systems 34}, pages
  3349--3363, 2021.

\bibitem{poggio2020theoretical}
T.~Poggio, A.~Banburski, and Q.~Liao.
\newblock Theoretical issues in deep networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(48):30039--30045, 2020.

\bibitem{salas1999gershgorin}
Hector~N Salas.
\newblock Gershgorin's theorem for matrices of operators.
\newblock {\em Linear Algebra and its Applications}, 291(1-3):15--36, 1999.

\bibitem{stroock1997:Kolmogorov}
D.~Stroock and S.~Varadhan.
\newblock {\em Multidimensional Diffusion Processes}.
\newblock Springer Science \& Business Media, 1997.

\bibitem{van2000asymptotic}
A.~W. Van~der Vaart.
\newblock {\em Asymptotic Statistics}.
\newblock Cambridge University Press, 2000.

\bibitem{yang2019:GP}
G.~Yang.
\newblock Tensor programs {I}: Wide feedforward or recurrent neural networks of
  any architecture are gaussian processes.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  9951--9960, 2019.

\bibitem{zhang2022:NNGP}
S.-Q. Zhang, F.~Wang, and F.-L. Fan.
\newblock Neural network gaussian processes by increasing depth.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem{zhang2021:arise}
S.-Q. Zhang and Z.-H. Zhou.
\newblock Arise: Aperiodic semi-parametric process for efficient markets
  without periodogram and gaussianity assumptions.
\newblock {\em arXiv:2111.06222}, 2021.

\end{thebibliography}
