\begin{thebibliography}{10}

\bibitem{adamczak2015note}
Radoslaw Adamczak.
\newblock A note on the {H}anson-{W}right inequality for random vectors with
  dependencies.
\newblock {\em Electronic Communications in Probability}, 20, 2015.

\bibitem{adamczak2015concentration}
Rados{\l}aw Adamczak and Pawe{\l} Wolff.
\newblock Concentration inequalities for non-{L}ipschitz functions with bounded
  derivatives of higher order.
\newblock {\em Probability Theory and Related Fields}, 162(3-4):531--586, 2015.

\bibitem{adlam2019random}
Ben Adlam, Jake Levinson, and Jeffrey Pennington.
\newblock A random matrix perspective on mixtures of nonlinearities for deep
  learning.
\newblock {\em arXiv preprint arXiv:1912.00827}, 2019.

\bibitem{adlam2020neural}
Ben Adlam and Jeffrey Pennington.
\newblock The neural tangent kernel in high dimensions: Triple descent and a
  multi-scale theory of generalization.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{advani2017high}
Madhu~S Advani and Andrew~M Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock {\em arXiv preprint arXiv:1710.03667}, 2017.

\bibitem{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  242--252, 2019.

\bibitem{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8139--8148, 2019.

\bibitem{benaych2010surprising}
Florent Benaych-Georges.
\newblock On a surprising relation between the marchenko-pastur law,
  rectangular and square free convolutions.
\newblock {\em Annales de l'IHP Probabilit{\'e}s et statistiques},
  46(3):644--652, 2010.

\bibitem{benigni2019eigenvalue}
Lucas Benigni and Sandrine P{\'e}ch{\'e}.
\newblock Eigenvalue distribution of nonlinear models of random matrices.
\newblock {\em arXiv preprint arXiv:1904.03090}, 2019.

\bibitem{boucheron2013concentration}
S.~Boucheron, G.~Lugosi, and P.~Massart.
\newblock {\em Concentration Inequalities: A Nonasymptotic Theory of
  Independence}.
\newblock OUP Oxford, 2013.

\bibitem{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2933--2943, 2019.

\bibitem{cho2009kernel}
Youngmin Cho and Lawrence~K Saul.
\newblock Kernel methods for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  342--350, 2009.

\bibitem{couillet2016kernel}
Romain Couillet and Florent Benaych-Georges.
\newblock Kernel spectral clustering of large dimensional data.
\newblock {\em Electronic Journal of Statistics}, 10(1):1393--1454, 2016.

\bibitem{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In {\em Advances In Neural Information Processing Systems}, pages
  2253--2261, 2016.

\bibitem{dascoli2020double}
St{\'e}phane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala.
\newblock Double trouble in double descent: {B}ias and variance(s) in the lazy
  regime.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{dicker2016ridge}
Lee~H Dicker.
\newblock Ridge regression and asymptotic minimax estimation over spheres of
  growing dimension.
\newblock {\em Bernoulli}, 22(1):1--37, 2016.

\bibitem{dobriban2018high}
Edgar Dobriban and Stefan Wager.
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock {\em The Annals of Statistics}, 46(1):247--279, 2018.

\bibitem{du2019gradientb}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em International Conference on Machine Learning}, 2019.

\bibitem{du2019gradienta}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{dyer2019asymptotics}
Ethan Dyer and Guy Gur-Ari.
\newblock Asymptotics of wide networks from {F}eynman diagrams.
\newblock {\em arXiv preprint arXiv:1909.11304}, 2019.

\bibitem{geiger2019jamming}
Mario Geiger, Stefano Spigler, St{\'e}phane d'Ascoli, Levent Sagun, Marco
  Baity-Jesi, Giulio Biroli, and Matthieu Wyart.
\newblock Jamming transition as a paradigm to understand the loss landscape of
  deep neural networks.
\newblock {\em Physical Review E}, 100(1):012115, 2019.

\bibitem{ghorbani2019limitations}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Limitations of lazy training of two-layers neural network.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9108--9118, 2019.

\bibitem{ghorbani2019linearized}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock {\em arXiv preprint arXiv:1904.12191}, 2019.

\bibitem{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em arXiv preprint arXiv:1903.08560}, 2019.

\bibitem{huang2019dynamics}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock {\em arXiv preprint arXiv:1909.08156}, 2019.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International Conference on Machine Learning}, pages
  448--456, 2015.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: {C}onvergence and generalization in neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8571--8580, 2018.

\bibitem{jacot2019asymptotic}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock The asymptotic spectrum of the hessian of dnn throughout training.
\newblock {\em arXiv preprint arXiv:1910.02875}, 2019.

\bibitem{johnson1990matrix}
C.R. Johnson.
\newblock {\em Matrix Theory and Applications}.
\newblock AMS Short Course Lecture Notes. American Mathematical Society, 1990.

\bibitem{karakida2019universal}
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari.
\newblock Universal statistics of {F}isher information in deep neural networks:
  {M}ean field approach.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1032--1041, 2019.

\bibitem{kasiviswanathan2019restricted}
Shiva~Prasad Kasiviswanathan and Mark Rudelson.
\newblock Restricted isometry property under high correlations.
\newblock {\em arXiv preprint arXiv:1904.05510}, 2019.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lee2018deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as {G}aussian processes.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8570--8581, 2019.

\bibitem{liang2019risk}
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai.
\newblock On the risk of minimum-norm interpolants and restricted lower
  isometry of kernels.
\newblock {\em arXiv preprint arXiv:1908.10292}, 2019.

\bibitem{liao2018dynamics}
Zhenyu Liao and Romain Couillet.
\newblock The dynamics of learning: A random matrix approach.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{liao2018spectrum}
Zhenyu Liao and Romain Couillet.
\newblock On the spectrum of random features maps of high dimensional data.
\newblock In {\em International Conference on Machine Learning}, pages
  3063--3071, 2018.

\bibitem{liao2019inner}
Zhenyu Liao and Romain Couillet.
\newblock On inner-product kernels of high dimensional data.
\newblock In {\em 2019 IEEE 8th International Workshop on Computational
  Advances in Multi-Sensor Adaptive Processing (CAMSAP)}, pages 579--583. IEEE,
  2019.

\bibitem{louart2018random}
Cosme Louart, Zhenyu Liao, and Romain Couillet.
\newblock A random matrix approach to neural networks.
\newblock {\em The Annals of Applied Probability}, 28(2):1190--1248, 2018.

\bibitem{marchenko1967distribution}
Vladimir~Alexandrovich Marchenko and Leonid~Andreevich Pastur.
\newblock Distribution of eigenvalues for some sets of random matrices.
\newblock {\em Matematicheskii Sbornik}, 114(4):507--536, 1967.

\bibitem{matthews2018gaussian}
Alexander G de~G Matthews, Jiri Hron, Mark Rowland, Richard~E Turner, and
  Zoubin Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{mei2019generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock {\em arXiv preprint arXiv:1908.05355}, 2019.

\bibitem{neal1995bayesian}
Radford~M Neal.
\newblock {\em Bayesian learning for neural networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem{peche2019note}
S~P{\'e}ch{\'e}.
\newblock A note on the pennington-worah distribution.
\newblock {\em Electronic Communications in Probability}, 24, 2019.

\bibitem{pennington2017geometry}
Jeffrey Pennington and Yasaman Bahri.
\newblock Geometry of neural network loss surfaces via random matrix theory.
\newblock In {\em International Conference on Machine Learning}, pages
  2798--2806, 2017.

\bibitem{pennington2017nonlinear}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2637--2646, 2017.

\bibitem{pennington2018spectrum}
Jeffrey Pennington and Pratik Worah.
\newblock The spectrum of the {F}isher information matrix of a
  single-hidden-layer neural network.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5410--5419, 2018.

\bibitem{poole2016exponential}
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya
  Ganguli.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3360--3368, 2016.

\bibitem{rahimi2008random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1177--1184, 2008.

\bibitem{rudelson2013hanson}
Mark Rudelson and Roman Vershynin.
\newblock Hanson-{W}right inequality and sub-gaussian concentration.
\newblock {\em Electronic Communications in Probability}, 18, 2013.

\bibitem{sagun2018empirical}
Levent Sagun, Utku Evci, V~Ugur Guney, Yann Dauphin, and Leon Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{schoenholz2017deep}
Samuel~S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{vershynin2018high}
R.~Vershynin.
\newblock {\em High-Dimensional Probability: An Introduction with Applications
  in Data Science}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2018.

\bibitem{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock {\em arXiv preprint arXiv:1011.3027}, 2010.

\bibitem{vu2015random}
Van Vu and Ke~Wang.
\newblock Random weighted projections, random quadratic forms and random
  eigenvectors.
\newblock {\em Random Structures \& Algorithms}, 47(4):792--821, 2015.

\bibitem{williams1997computing}
Christopher~KI Williams.
\newblock Computing with infinite networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  295--301, 1997.

\bibitem{xiao2019disentangling}
Lechao Xiao, Jeffrey Pennington, and Samuel~S Schoenholz.
\newblock Disentangling trainability and generalization in deep learning.
\newblock {\em arXiv preprint arXiv:1912.13053}, 2019.

\bibitem{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing:
  {G}aussian process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock {\em arXiv preprint arXiv:1902.04760}, 2019.

\bibitem{yang2019fine}
Greg Yang and Hadi Salman.
\newblock A fine-grained spectral perspective on neural networks.
\newblock {\em arXiv preprint arXiv:1907.10599}, 2019.

\bibitem{yehudai2019power}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6594--6604, 2019.

\end{thebibliography}
