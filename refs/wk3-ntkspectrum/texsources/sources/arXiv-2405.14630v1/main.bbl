\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allenzhu2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  242--252. PMLR, 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/allen-zhu19a.html}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and Wang]{pmlr-v97-arora19a}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  322--332. PMLR, 09--15 Jun 2019{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v97/arora19a.html}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and Wang]{arora_exact_comp}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019{\natexlab{b}}.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf}.

\bibitem[Axler et~al.(2013)Axler, Bourdon, and Wade]{axler2013harmonic}
Sheldon Axler, Paul Bourdon, and Ramey Wade.
\newblock \emph{Harmonic function theory}, volume 137.
\newblock Springer Science \& Business Media, 2013.
\newblock URL \url{https://doi.org/10.1007/978-1-4757-8137-3}.

\bibitem[Ball(1997)]{ball1997elementary}
Keith Ball.
\newblock An elementary introduction to modern convex geometry.
\newblock \emph{Flavors of geometry}, 31:\penalty0 1--58, 1997.

\bibitem[Banerjee et~al.(2023)Banerjee, Cisneros-Velarde, Zhu, and Belkin]{banerjee2023neural}
Arindam Banerjee, Pedro Cisneros-Velarde, Libin Zhu, and Mikhail Belkin.
\newblock Neural tangent kernel at initialization: Linear width suffices.
\newblock In \emph{The 39th Conference on Uncertainty in Artificial Intelligence}, 2023.
\newblock URL \url{https://openreview.net/forum?id=VJaoe7Rp9tZ}.

\bibitem[Basri et~al.(2019)Basri, Jacobs, Kasten, and Kritchman]{uniform_sphere_data}
Ronen Basri, David~W. Jacobs, Yoni Kasten, and Shira Kritchman.
\newblock The convergence rate of neural networks for learned functions of different frequencies.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\  4763--4772, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/5ac8bb8a7d745102a978c5f8ccdb61b8-Abstract.html}.

\bibitem[Basri et~al.(2020)Basri, Galun, Geifman, Jacobs, Kasten, and Kritchman]{basri2020frequency}
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  685--694. PMLR, 13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/basri20a.html}.

\bibitem[Bietti \& Bach(2021)Bietti and Bach]{bietti2021deep}
Alberto Bietti and Francis Bach.
\newblock Deep equals shallow for {ReLU} networks in kernel regimes.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=aDjoksTpXOP}.

\bibitem[Bombari et~al.(2022)Bombari, Amani, and Mondelli]{bombari2022memorization}
Simone Bombari, Mohammad~Hossein Amani, and Marco Mondelli.
\newblock Memorization and optimization in deep neural networks with minimum over-parameterization.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, pp.\  7628--7640. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/323746f0ae2fbd8b6f500dc2d5c5f898-Paper-Conference.pdf}.

\bibitem[Bowman \& Mont\'ufar(2022)Bowman and Mont\'ufar]{bowman2022spectral}
Benjamin Bowman and Guido Mont\'ufar.
\newblock Spectral bias outside the training set for deep networks in the kernel regime.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=a01PL2gb7W5}.

\bibitem[Cao et~al.(2021)Cao, Fang, Wu, Zhou, and Gu]{cao2019towards}
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu.
\newblock Towards understanding the spectral bias of deep learning.
\newblock In \emph{Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}}, pp.\  2205--2211, August 2021.
\newblock URL \url{https://doi.org/10.24963/ijcai.2021/304}.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{lazy_training}
L\'{e}na\"{\i}c Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf}.

\bibitem[Cui et~al.(2021)Cui, Loureiro, Krzakala, and Zdeborov{\'a}]{cui2021generalization}
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Da_EHrAcfwd}.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  1675--1685. PMLR, 2019{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v97/du19c.html}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Efthimiou \& Frye(2014)Efthimiou and Frye]{efthimiou2014spherical}
Costas Efthimiou and Christopher Frye.
\newblock \emph{Spherical harmonics in p dimensions}.
\newblock World Scientific, 2014.
\newblock URL \url{https://doi.org/10.1142/9134}.

\bibitem[Fan \& Wang(2020)Fan and Wang]{NEURIPS2020_572201a4}
Zhou Fan and Zhichao Wang.
\newblock Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  7710--7721. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/file/572201a4497b0b9f02d4f279b09ec30d-Paper.pdf}.

\bibitem[Geifman et~al.(2020)Geifman, Yadav, Kasten, Galun, Jacobs, and Ronen]{geifman2020similarity}
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen.
\newblock On the similarity between the {L}aplace and neural tangent kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  1451--1461. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf}.

\bibitem[Gradshteyn \& Ryzhik(2014)Gradshteyn and Ryzhik]{gradshteyn2014table}
Izrail~Solomonovich Gradshteyn and Iosif~Moiseevich Ryzhik.
\newblock \emph{Table of integrals, series, and products}.
\newblock Academic press, 2014.
\newblock URL \url{https://doi.org/10.1016/C2010-0-64839-5}.

\bibitem[Horn \& Johnson(2012)Horn and Johnson]{Horn_Johnson_2012}
Roger~A. Horn and Charles~R. Johnson.
\newblock \emph{Matrix Analysis}.
\newblock Cambridge University Press, 2 edition, 2012.
\newblock URL \url{https://doi.org/10.1017/CBO9780511810817}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}.

\bibitem[Jin et~al.(2022)Jin, Banerjee, and Mont\'ufar]{jin2022learning}
Hui Jin, Pradeep~Kr. Banerjee, and Guido Mont\'ufar.
\newblock Learning curves for {G}aussian process regression with power-law priors and targets.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=KeI9E-gsoB}.

\bibitem[Karhadkar et~al.(2023)Karhadkar, Murray, Tseran, and Mont\'ufar]{karhadkar2023mildly}
Kedar Karhadkar, Michael Murray, Hanna Tseran, and Guido Mont\'ufar.
\newblock Mildly overparameterized {ReLU} networks have a favorable loss landscape.
\newblock \emph{arXiv:2305.19510}, 2023.

\bibitem[Laurent \& Massart(2000)Laurent and Massart]{10.1214/aos/1015957395}
B.~Laurent and P.~Massart.
\newblock {Adaptive estimation of a quadratic functional by model selection}.
\newblock \emph{The Annals of Statistics}, 28\penalty0 (5):\penalty0 1302--1338, 2000.
\newblock URL \url{https://doi.org/10.1214/aos/1015957395}.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein, and Pennington]{Lee2019WideNN-SHORT}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf}.

\bibitem[Lee et~al.(2020{\natexlab{a}})Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and Sohl-Dickstein]{10.5555/3495724.3496995}
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  15156--15172. Curran Associates, Inc., 2020{\natexlab{a}}.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/file/ad086f59924fffe0773f8d0ca22ea712-Paper.pdf}.

\bibitem[Lee et~al.(2020{\natexlab{b}})Lee, Yu, Rival, and Yang]{10.5555/3495724.3496288}
Wonyeol Lee, Hangyeol Yu, Xavier Rival, and Hongseok Yang.
\newblock On correctness of automatic differentiation for non-differentiable functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  6719--6730. Curran Associates, Inc., 2020{\natexlab{b}}.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/4aaa76178f8567e05c8e8295c96171d8-Paper.pdf}.

\bibitem[Li(2010)]{li2010concise}
Shengqiao Li.
\newblock Concise formulas for the area and volume of a hyperspherical cap.
\newblock \emph{Asian Journal of Mathematics \& Statistics}, 4\penalty0 (1):\penalty0 66--70, 2010.

\bibitem[Li \& Liang(2018)Li and Liang]{NEURIPS2018_54fe976b}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf}.

\bibitem[Liu et~al.(2020)Liu, Zhu, and Belkin]{NEURIPS2020_b7ae8fec}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock On the linearity of large non-linear models: when and why the tangent kernel is constant.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  15954--15964. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf}.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{LIU202285}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0 85--116, 2022.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S106352032100110X}.
\newblock Special Issue on Harmonic Analysis and Machine Learning.

\bibitem[Montanari \& Zhong(2022)Montanari and Zhong]{montanari2022interpolation}
Andrea Montanari and Yiqiao Zhong.
\newblock The interpolation phase transition in neural networks: Memorization and generalization under lazy training.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (5):\penalty0 2816--2847, 2022.
\newblock URL \url{https://doi.org/10.1214/22-AOS2211}.

\bibitem[Murray et~al.(2023)Murray, Jin, Bowman, and Mont\'ufar]{murray2023characterizing}
Michael Murray, Hui Jin, Benjamin Bowman, and Guido Mont\'ufar.
\newblock Characterizing the spectrum of the {NTK} via a power series expansion.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Tvms8xrZHyR}.

\bibitem[Nevai et~al.(1994)Nevai, Erd{\'e}lyi, and Magnus]{nevai1994generalized}
Paul Nevai, Tam{\'a}s Erd{\'e}lyi, and Alphonse~P Magnus.
\newblock Generalized jacobi weights, christoffel functions, and jacobi polynomials.
\newblock \emph{SIAM Journal on Mathematical Analysis}, 25\penalty0 (2):\penalty0 602--614, 1994.

\bibitem[Nguyen(2021)]{nguyenrelu}
Quynh Nguyen.
\newblock On the proof of global convergence of gradient descent for deep {ReLU} networks with linear widths.
\newblock In \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  8056--8062. PMLR, 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/nguyen21a.html}.

\bibitem[Nguyen \& Mondelli(2020)Nguyen and Mondelli]{marco}
Quynh Nguyen and Marco Mondelli.
\newblock Global convergence of deep networks with one wide layer followed by pyramidal topology.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  11961--11972. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf}.

\bibitem[Nguyen et~al.(2021)Nguyen, Mondelli, and Mont\'ufar]{nguyen2021tight}
Quynh Nguyen, Marco Mondelli, and Guido Mont\'ufar.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep {ReLU} networks.
\newblock In \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  8119--8129. PMLR, 18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/nguyen21g.html}.

\bibitem[Oymak \& Soltanolkotabi(2020)Oymak and Soltanolkotabi]{Oymak2019TowardMO}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 1\penalty0 (1):\penalty0 84--105, 2020.
\newblock URL \url{https://doi.org/10.1109/JSAIT.2020.2991332}.

\bibitem[Rubin(1999)]{rubin1999inversion}
Boris Rubin.
\newblock Inversion and characterization of the hemispherical transform.
\newblock \emph{Journal d’Analyse Math{\'e}matique}, 77:\penalty0 105--128, 1999.
\newblock URL \url{https://doi.org/10.1007/BF02791259}.

\bibitem[Schur(1911)]{Schur1911}
J.~Schur.
\newblock Bemerkungen zur {T}heorie der beschränkten {B}ilinearformen mit unendlich vielen {V}eränderlichen.
\newblock \emph{Journal für die reine und angewandte Mathematik}, 140:\penalty0 1--28, 1911.
\newblock URL \url{http://eudml.org/doc/149352}.

\bibitem[Seeley(1966)]{seeley1966spherical}
Robert~T Seeley.
\newblock Spherical harmonics.
\newblock \emph{The American Mathematical Monthly}, 73\penalty0 (4P2):\penalty0 115--121, 1966.
\newblock URL \url{https://doi.org/10.1080/00029890.1966.11970927}.

\bibitem[Tropp(2012)]{tropp2012user}
Joel~A. Tropp.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{Foundations of computational mathematics}, 12:\penalty0 389--434, 2012.
\newblock URL \url{https://doi.org/10.1007/s10208-011-9099-z}.

\bibitem[Velikanov \& Yarotsky(2021)Velikanov and Yarotsky]{velikanov2021explicit}
Maksim Velikanov and Dmitry Yarotsky.
\newblock Explicit loss asymptotics in the gradient descent training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  2570--2582. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/14faf969228fc18fcd4fcf59437b0c97-Paper.pdf}.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications in data science}, volume~47.
\newblock Cambridge university press, 2018.
\newblock URL \url{https://doi.org/10.1017/9781108231596}.

\bibitem[Xie et~al.(2017)Xie, Liang, and Song]{xie2017diverse}
Bo~Xie, Yingyu Liang, and Le~Song.
\newblock Diverse neural network learns true target functions.
\newblock In \emph{Proceedings of the 20th International Conference on Artificial Intelligence and Statistics}, pp.\  1216--1224. PMLR, 2017.
\newblock URL \url{https://proceedings.mlr.press/v54/xie17a.html}.

\bibitem[Xie et~al.(2013)Xie, Wang, and Zhao]{xie2013exponential}
Ziqing Xie, Li-Lian Wang, and Xiaodan Zhao.
\newblock On exponential convergence of {G}egenbauer interpolation and spectral differentiation.
\newblock \emph{Mathematics of Computation}, 82\penalty0 (282):\penalty0 1017--1036, 2013.
\newblock URL \url{https://doi.org/10.1090/S0025-5718-2012-02645-7}.

\bibitem[Zou \& Gu(2019)Zou and Gu]{zou2019improved}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf}.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Gradient descent optimizes over-parameterized deep {ReLU} networks.
\newblock \emph{Machine learning}, 109\penalty0 (3):\penalty0 467--492, 2020.
\newblock URL \url{https://doi.org/10.1007/s10994-019-05839-6}.

\end{thebibliography}
