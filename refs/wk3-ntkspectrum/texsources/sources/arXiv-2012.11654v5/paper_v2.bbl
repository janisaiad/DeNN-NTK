\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adlam et~al.(2019)Adlam, Levinson, and Pennington]{adlam2019random}
Adlam, B., Levinson, J., and Pennington, J.
\newblock A random matrix perspective on mixtures of nonlinearities for deep
  learning, 2019.
\newblock \texttt{arXiv:1912.00827}.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{AllenZhuEtal2018}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{AroraEtal2019}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)},
  2019{\natexlab{b}}.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{bartlett2019nearly}
Bartlett, P., Harvey, N., Liaw, C., and Mehrabian, A.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 20\penalty0
  (63):\penalty0 1--17, 2019.

\bibitem[Benigni \& P{\'e}ch{\'e}(2019)Benigni and
  P{\'e}ch{\'e}]{benigni2019eigenvalue}
Benigni, L. and P{\'e}ch{\'e}, S.
\newblock Eigenvalue distribution of nonlinear models of random matrices, 2019.
\newblock \texttt{arXiv:1904.03090}.

\bibitem[Bubeck et~al.(2020)Bubeck, Eldan, Lee, and
  Mikulincer]{bubeck2020network}
Bubeck, S., Eldan, R., Lee, Y.~T., and Mikulincer, D.
\newblock Network size and weights size for memorization with two-layers neural
  networks.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Buchanan et~al.(2021)Buchanan, Gilboa, and Wrighta]{Sam2021}
Buchanan, S., Gilboa, D., and Wrighta, J.
\newblock Deep networks and the multiple manifold problem.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Chen et~al.(2020)Chen, Cao, Gu, and Zhang]{ChenEtal2020}
Chen, Z., Cao, Y., Gu, Q., and Zhang, T.
\newblock A generalized neural tangent kernel analysis fortwo-layer neural
  networks.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{ChizatEtc2019}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Davidson \& Szarek(2001)Davidson and Szarek]{Davidson2001}
Davidson, K.~R. and Szarek, S.~J.
\newblock Local operator theory, random matrices and banach spaces.
\newblock \emph{Handbook of the geometry of Banach spaces}, 1\penalty0
  (140):\penalty0 317--366, 2001.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{DuEtal2019}
Du, S.~S., Lee, J.~D., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{DuEtal2018_ICLR}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019{\natexlab{b}}.

\bibitem[Fan \& Wang(2020)Fan and Wang]{fan2020spectra}
Fan, Z. and Wang, Z.
\newblock Spectra of the conjugate kernel and neural tangent kernel for
  linear-width neural networks.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Ge et~al.(2019)Ge, Wang, and Zhao]{ge2019mildly}
Ge, R., Wang, R., and Zhao, H.
\newblock Mildly overparametrized neural nets can memorize training data
  efficiently, 2019.
\newblock \texttt{arXiv:1909.11837}.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{GhorbaniEtal2020}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Linearized two-layers neural networks in high dimension, 2020.
\newblock \texttt{arXiv:1904.12191}.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{XavierBengio2010}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2010.

\bibitem[Gorokhovik(2011)]{Gorokhovik2011}
Gorokhovik, V.~V.
\newblock Geometrical and analytical characteristic properties of piecewise
  affine mappings, 2011.
\newblock \texttt{arXiv:1111.1389}.

\bibitem[Hanin \& Nica(2019)Hanin and Nica]{hanin2019products}
Hanin, B. and Nica, M.
\newblock Products of many large random matrices and gradients in deep neural
  networks.
\newblock \emph{Communications in Mathematical Physics}, pp.\  1--36, 2019.

\bibitem[Hanin \& Rolnick(2019)Hanin and Rolnick]{HaninRolnick2019}
Hanin, B. and Rolnick, D.
\newblock Deep relu networks have surprisingly few activation patterns.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2019surprises}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J.
\newblock Surprises in high-dimensional ridgeless least squares interpolation,
  2019.
\newblock \texttt{arXiv:1903.08560}.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015.

\bibitem[Huang \& Yau(2020)Huang and Yau]{HuangYau2020}
Huang, J. and Yau, H.-T.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{JacotEtc2018}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun2012efficient}
LeCun, Y.~A., Bottou, L., Orr, G.~B., and M{\"u}ller, K.-R.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  9--48.
  Springer, 2012.

\bibitem[Liao \& Couillet(2018)Liao and Couillet]{liao2018spectrum}
Liao, Z. and Couillet, R.
\newblock On the spectrum of random features maps of high dimensional data.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Liao et~al.(2020)Liao, Couillet, and Mahoney]{liao2020random}
Liao, Z., Couillet, R., and Mahoney, M.~W.
\newblock {A random matrix analysis of random Fourier features: beyond the
  Gaussian kernel, a precise phase transition, and the corresponding double
  descent}.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Louart et~al.(2018)Louart, Liao, and Couillet]{louart2018random}
Louart, C., Liao, Z., and Couillet, R.
\newblock A random matrix approach to neural networks.
\newblock \emph{The Annals of Applied Probability}, 28\penalty0 (2):\penalty0
  1190--1248, 2018.

\bibitem[Mei \& Montanari(2019)Mei and Montanari]{mei2019generalization}
Mei, S. and Montanari, A.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve, 2019.
\newblock \texttt{arXiv:1908.05355}.

\bibitem[Montanari \& Zhong(2020)Montanari and Zhong]{Andrea2020}
Montanari, A. and Zhong, Y.
\newblock The interpolation phase transition in neural networks: Memorization
  and generalization under lazy training, 2020.
\newblock \texttt{arXiv:2007.12826}.

\bibitem[Montufar et~al.(2014)Montufar, Pascanu, Cho, and
  Bengio]{MontufarEtal2014}
Montufar, G.~F., Pascanu, R., Cho, K., and Bengio, Y.
\newblock On the number of linear regions of deep neural networks.
\newblock In \emph{Neural Information Processing Systems (NIPS)}, 2014.

\bibitem[Nguyen(2019)]{QuynhICML2019}
Nguyen, Q.
\newblock On connected sublevel sets in deep learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Nguyen \& Hein(2017)Nguyen and Hein]{QuynhICML2017}
Nguyen, Q. and Hein, M.
\newblock The loss surface of deep and wide neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Nguyen \& Mondelli(2020)Nguyen and Mondelli]{QuynhMarco2020}
Nguyen, Q. and Mondelli, M.
\newblock Global convergence of deep networks with one wide layer followed by
  pyramidal topology.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Oymak \& Soltanolkotabi(2020)Oymak and Soltanolkotabi]{OymakMahdi2019}
Oymak, S. and Soltanolkotabi, M.
\newblock Towards moderate overparameterization: global convergence guarantees
  for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 2020.

\bibitem[Pennington \& Bahri(2017)Pennington and Bahri]{pennington2017geometry}
Pennington, J. and Bahri, Y.
\newblock Geometry of neural network loss surfaces via random matrix theory.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Pennington \& Worah(2017)Pennington and
  Worah]{pennington2017nonlinear}
Pennington, J. and Worah, P.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Pennington \& Worah(2018)Pennington and Worah]{pennington2018spectrum}
Pennington, J. and Worah, P.
\newblock The spectrum of the fisher information matrix of a
  single-hidden-layer neural network.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Pennington et~al.(2018)Pennington, Schoenholz, and
  Ganguli]{pennington2018emergence}
Pennington, J., Schoenholz, S., and Ganguli, S.
\newblock The emergence of spectral universality in deep networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2018.

\bibitem[Schur(1911)]{schur1911bemerkungen}
Schur, J.
\newblock Bemerkungen zur theorie der beschr{\"a}nkten bilinearformen mit
  unendlich vielen ver{\"a}nderlichen.
\newblock \emph{Journal f{\"u}r die reine und angewandte Mathematik (Crelles
  Journal)}, 1911\penalty0 (140):\penalty0 1--28, 1911.

\bibitem[Seddik et~al.(2020)Seddik, Louart, Tamaazousti, and
  Couillet]{seddik2020random}
Seddik, M. E.~A., Louart, C., Tamaazousti, M., and Couillet, R.
\newblock Random matrix theory proves that deep learning representations of
  gan-data behave as gaussian mixtures.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Serra et~al.(2018)Serra, Tjandraatmadja, and
  Ramalingam]{SerraEtal2018}
Serra, T., Tjandraatmadja, C., and Ramalingam, S.
\newblock Bounding and counting linear regions of deep neural networks.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Song \& Yang(2020)Song and Yang]{SongYang2020}
Song, Z. and Yang, X.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound, 2020.
\newblock \texttt{arXiv:1906.03593}.

\bibitem[Tropp(2012)]{Tropp2011}
Tropp, J.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{Foundations of Computational Mathematics}, pp.\  389â€“434,
  2012.

\bibitem[Vershynin(2018)]{vershynin2018high}
Vershynin, R.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}.
\newblock Cambridge university press, 2018.

\bibitem[Vershynin(2020)]{vershynin2020memory}
Vershynin, R.
\newblock Memory capacity of neural networks with threshold and rectified
  linear unit activations.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1004--1033, 2020.

\bibitem[Wu et~al.(2019)Wu, Du, and Ward]{wu2019global}
Wu, X., Du, S.~S., and Ward, R.
\newblock Global convergence of adaptive gradient methods for an
  over-parameterized neural network, 2019.
\newblock \texttt{arXiv:1902.07111}.

\bibitem[Yun et~al.(2019)Yun, Sra, and Jadbabaie]{yun2019small}
Yun, C., Sra, S., and Jadbabaie, A.
\newblock Small relu networks are powerful memorizers: a tight analysis of
  memorization capacity.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Zou \& Gu(2019)Zou and Gu]{ZouGu2019}
Zou, D. and Gu, Q.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{Machine Learning}, 109\penalty0 (3):\penalty0 467--492, 2020.

\end{thebibliography}
