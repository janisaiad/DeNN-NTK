@article{Palesetal2007a,
  title={Generalized {J}acobian for Functions with Infinite Dimensional Range and Domain},
  author={Zsolt P{\'a}les and Vera Zeidan},
  journal={Set-Valued Analysis},
  year={2007},
  volume={15},
  pages={331-375}
}

@article{Palesetal2007b,
  title={Infinite dimensional {C}larke generalized {J}acobian},
  author={Zsolt P{\'a}les and Vera Zeidan},
  journal={Journal of Convex Analysis},
  year={2007},
  volume={14},
  number={2},
  pages={433-454}
}

@article{Palesetal2008,
  title={Infinite dimensional generalized {J}acobian: Properties and calculus rules},
  author={Zsolt P{\'a}les and Vera Zeidan},
  journal={Journal of Mathematical Analysis and Applications},
  year={2008},
  volume={344},
  pages={55-75}
}

@article{Palesetal2009,
  title={The core of the infinite dimensional generalized {J}acobian},
  author={Zsolt P{\'a}les and Vera Zeidan},
  journal={Journal of Convex Analysis},
  year={2009},
  volume={16},
  number={2},
  pages={321-349}
}

@article{Palesetal2010,
  title={Co-{J}acobian for {L}ipschitzian Maps},
  author={Zsolt P{\'a}les and Vera Zeidan},
  journal={Set-Valued and Variational Analysis},
  year={2010},
  volume={18},
  pages={57-78}
}

@article{Palesetal2011,
  title={V-{J}acobian and {V}-co-{J}acobian for {L}ipschitzian maps},
  author={Zsolt P{\'a}les and Vera Zeidan},
  journal={Discrete and Continuous Dynamical Systems},
  year={2011},
  volume={29},
  number={2},
  pages={623-646}
}

@article{Ambrosioetal2000a,
  title={Rectifiable sets in metric and {B}anach spaces},
  author={Luigi Ambrosio and Bernd Kirchheim},
  journal={Mathematische Annalen},
  year={2000},
  volume={318},
  pages={527-555}
}

@article{Ambrosioetal2000b,
  title={Currents in metric spaces},
  author={Luigi Ambrosio and Bernd Kirchheim},
  journal={Acta Mathematica},
  year={2000},
  volume={185},
  pages={1-80}
}

@InProceedings{Rockafellar1976,
author={R. Tyrrell Rockafellar},
editor={Jean Pierre Gossez and 
		Enrique Jos{\'{e}} Lami Dozo and
		Jean Mawhin and
		Lucien Waelbroeck},
title={Integral functionals, normal integrands and measurable selections},
booktitle={Nonlinear Operators and the Calculus of Variations},
year={1976},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={157--207},
isbn={978-3-540-38075-7}
}

@article{Rockafellar1971,
author={R. Tyrrell Rockafellar},
publisher={E. Zarantonello},
title={Convex integral functionals and duality},
journal={Contributlons to Nonlinear Functional Analysis},
year={1971},
publisher={Academic Press},
pages={215--236}
}

@article{Polyak1963,
  title={Gradient methods for minimizing functionals},
  author={Polyak, Boris Teodorovich},
  journal={Zhurnal vychislitel'noi matematiki i matematicheskoi fiziki},
  volume={3},
  number={4},
  pages={643--653},
  year={1963},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}

@article{Karimietal2016,
   title={Linear Convergence of Gradient and Proximal-Gradient Methods Under the {P}olyak-{Ł}ojasiewicz Condition},
   ISBN={9783319461281},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-319-46128-1_50},
   DOI={10.1007/978-3-319-46128-1_50},
   journal={Lecture Notes in Computer Science},
   publisher={Springer International Publishing},
   author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
   year={2016},
   pages={795–811}
}

@inproceedings{Jacotetal2018,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
 volume = {31},
 year = {2018}
}

@book{Rockafellaretal1998,
  title={Variational Analysis},
  author={R. Tyrrell Rockafellar and Roger J.-B. Wets},
  series={Grundlehren der mathematischen Wissenschaften},
  year={1998}
}

@book{Zalinescu2002,
  title={Convex Analysis for general vector spaces},
  author={C. Z\u{a}linescu},
  publisher={World Scientific Publishing Co. Pte. Ltd.},
  year={2002}
}

@article{Fournieretal2015,
  TITLE = {On the rate of convergence in {W}asserstein distance of the empirical measure},
  AUTHOR = {Fournier, N. and Guillin, A.},
  URL = {https://hal.archives-ouvertes.fr/hal-00915365},
  JOURNAL = {{Probability Theory and Related Fields}},
  PUBLISHER = {{Springer Verlag}},
  VOLUME = {162},
  NUMBER = {3-4},
  PAGES = {707},
  YEAR = {2015},
  MONTH = Aug,
  KEYWORDS = {Empirical measure ; Sequence of i.i.d. random variables ; Wasserstein distance ; Concentration inequalities ; Quantization ; Markov chains},
  PDF = {https://hal.archives-ouvertes.fr/hal-00915365/file/fournier-guillin-wass.pdf},
  HAL_ID = {hal-00915365},
  HAL_VERSION = {v1},
}

@article{Lei2020,
  title={Convergence and concentration of empirical measures under {W}asserstein distance in unbounded functional spaces},
  author={Lei, J.},
  journal={Bernoulli},
  year={2020},
  volume={26},
  pages={767-798}
}

@InProceedings{Oymaketal2019a,
  title = 	 {Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?},
  author =       {Oymak, Samet and Soltanolkotabi, Mahdi},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4951--4960},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/oymak19a/oymak19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/oymak19a.html},
}

@book{Cobzasetal2019,
  title={Lipschitz Functions},
  author={Cobza{\c{s}}, {\c{S}}tefan and Miculescu, Radu and Nicolae, Adriana},
  isbn={9783030164881},
  lccn={2019936365},
  series={Lecture Notes in Mathematics},
  year={2019},
  publisher={Springer International Publishing}
}

@misc{Gaoetal2017,
    title={On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning},
    author={Gao, B. and Pavel, L.},
    year={2017},
    eprint={1704.00805},
    archivePrefix={arXiv},
    primaryClass={math.OC}
}

@book{Hajeketal2014,
author = {Hájek, Petr and Johanis, Michal},
doi = {doi:10.1515/9783110258998},
url = {https://doi.org/10.1515/9783110258998},
title = {Smooth analysis in Banach spaces},
year = {2014},
publisher = {De Gruyter},
ISBN = {9783110258998}
}

@book{Clarke2013,
  title={Functional Analysis, Calculus of Variations and Optimal Control},
  author={Clarke, F.},
  isbn={9781447148203},
  lccn={2013931980},
  series={Graduate Texts in Mathematics},
  year={2013},
  publisher={Springer London}
}

@article{Liuetal2022,
title = {Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
journal = {Applied and Computational Harmonic Analysis},
volume = {59},
pages = {85-116},
year = {2022},
note = {Special Issue on Harmonic Analysis and Machine Learning},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2021.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S106352032100110X},
author = {Chaoyue Liu and Libin Zhu and Mikhail Belkin}
}

@book{Kallenberg2002,
  title={Foundations of Modern Probability},
  author={Kallenberg, O.},
  isbn={9780387953137},
  lccn={2001032816},
  series={Probability and Its Applications},
  year={2002},
  publisher={Springer New York}
}

@inbook{Lietal2020,
author = {Xinyan Li and Qilong Gu and Yingxue Zhou and Tiancong Chen and Arindam Banerjee},
title = {Hessian based analysis of SGD for Deep Nets: Dynamics and Generalization},
booktitle = {Proceedings of the 2020 SIAM International Conference on Data Mining (SDM)},
chapter = {},
pages = {190-198},
doi = {10.1137/1.9781611976236.22},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976236.22},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611976236.22}
}

@inproceedings{Williamsetal2019,
 author = {Williams, Francis and Trager, Matthew and Panozzo, Daniele and Silva, Claudio and Zorin, Denis and Bruna, Joan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Gradient Dynamics of Shallow Univariate {R}e{LU} Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/1f6419b1cbe79c71410cb320fc094775-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Razinetal2020,
 author = {Razin, Noam and Cohen, Nadav},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21174--21187},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Regularization in Deep Learning May Not Be Explainable by Norms},
 url = {https://proceedings.neurips.cc/paper/2020/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{Lietal2020b,
  title = 	 {Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks},
  author =       {Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4313--4324},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/li20j/li20j.pdf},
  url = 	 {https://proceedings.mlr.press/v108/li20j.html},
  abstract = 	 {Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including significantly corrupted ones. Despite this (over)fitting capacity in this paper we demonstrate that such overparameterized networks have an intriguing robustness capability: they are surprisingly robust to label noise when first order methods with early stopping is used to train them. This paper also takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) To start to overfit to the noisy labels network must stray rather far from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.}
}

@ARTICLE{Oymaketal2020,

  author={Oymak, Samet and Soltanolkotabi, Mahdi},

  journal={IEEE Journal on Selected Areas in Information Theory}, 

  title={Toward Moderate Overparameterization: Global Convergence Guarantees for Training Shallow Neural Networks}, 

  year={2020},

  volume={1},

  number={1},

  pages={84-105},

  doi={10.1109/JSAIT.2020.2991332}
}

@inproceedings{Chizatetal2019,
 author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Lazy Training in Differentiable Programming},
 url = {https://proceedings.neurips.cc/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Neyshaburetal2017,
 author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Exploring Generalization in Deep Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/10ce03a1ed01077e3e289f3e53c72813-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{Longetal2020,
title={Generalization bounds for deep convolutional neural networks},
author={Philip M. Long and Hanie Sedghi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1e_FpNFDr}
}

@inproceedings{Gunasekaretal2017,
 author = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Regularization in Matrix Factorization},
 url = {https://proceedings.neurips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{Azizanetal2018,
title={Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization},
author={Navid Azizan and Babak Hassibi},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJf9ZhC9FX},
}


@InProceedings{Nacsonetal2019,
  title = 	 {Stochastic Gradient Descent on Separable Data: Exact Convergence with a Fixed Learning Rate},
  author =       {Nacson, Mor Shpigel and Srebro, Nathan and Soudry, Daniel},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3051--3059},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/nacson19a/nacson19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/nacson19a.html},
  abstract = 	 {Stochastic Gradient Descent (SGD) is a central tool in machine learning. We prove that SGD converges to zero loss, even with a fixed (non-vanishing) learning rate — in the special case of homogeneous linear classifiers with smooth monotone loss functions, optimized on linearly separable data. Previous works assumed either a vanishing learning rate, iterate averaging, or loss assumptions that do not hold for monotone loss functions used for classification, such as the logistic loss. We prove our result on a fixed dataset, both for sampling with or without replacement. Furthermore, for logistic loss (and similar exponentially-tailed losses), we prove that with SGD the weight vector converges in direction to the $L_2$ max margin vector as $O(1/\log(t))$ for almost all separable datasets, and the loss converges as $O(1/t)$ — similarly to gradient descent. Lastly, we examine the case of a fixed learning rate proportional to the minibatch size. We prove that in this case, the asymptotic convergence rate of SGD (with replacement) does not depend on the minibatch size in terms of epochs, if the support vectors span the data. These results may suggest an explanation to similar behaviors observed in deep networks, when trained with SGD.}
}

@inproceedings{Neyshaburetal2015,
  author={Behnam Neyshabur and Ryota Tomioka and Nathan Srebro},
  title={In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning.},
  year={2015},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6614},
  booktitle={ICLR (Workshop)},
}

@inproceedings{Soudryetal2018,
title={The Implicit Bias of Gradient Descent on Separable Data},
author={Daniel Soudry and Elad Hoffer and Nathan Srebro},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1q7n9gAb},
}

@inproceedings{Wilsonetal2017,
 author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Marginal Value of Adaptive Gradient Methods in Machine Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{Allen-Zhuetal2019a,
author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
title = {Learning and Generalization in Overparameterized Neural Networks, Going beyond Two Layers},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The fundamental learning theory behind neural networks remains largely open. What classes of functions can neural networks actually learn? Why doesn't the trained network overfit when it is overparameterized?In this work, we prove that overparameterized neural networks can learn some notable concept classes, including two and three-layer networks with fewer parameters and smooth activations. Moreover, the learning can be simply done by SGD (stochastic gradient descent) or its variants in polynomial time using poly-nomially many samples. The sample complexity can also be almost independent of the number of parameters in the network.On the technique side, our analysis goes beyond the so-called NTK (neural tangent kernel) linearization of neural networks in prior works. We establish a new notion of quadratic approximation of the neural network, and connect it to the SGD theory of escaping saddle points.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {553},
numpages = {12}
}


@InProceedings{Allen-Zhuetal2019b,
  title = 	 {A Convergence Theory for Deep Learning via Over-Parameterization},
  author =       {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {242--252},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/allen-zhu19a/allen-zhu19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/allen-zhu19a.html},
  abstract = 	 {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps &nbsp; e^{-T}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).}
}


@InProceedings{Duetal2019,
  title = 	 {Gradient Descent Finds Global Minima of Deep Neural Networks},
  author =       {Du, Simon S. and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1675--1685},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/du19c/du19c.pdf},
  url = 	 {https://proceedings.mlr.press/v97/du19c.html},
  abstract = 	 {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.}
}

@inproceedings{Duetal2018,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}


@article{Soltanolkotabietal2019,
title = "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks",
keywords = "Nonconvex optimization, over-parametrized neural networks, random matrix theory",
author = "Mahdi Soltanolkotabi and Adel Javanmard and Lee, {Jason D.}",
year = "2019",
month = feb,
doi = "10.1109/TIT.2018.2854560",
language = "English (US)",
volume = "65",
pages = "742--769",
journal = "IRE Professional Group on Information Theory",
issn = "0018-9448",
publisher = "Institute of Electrical and Electronics Engineers Inc.",
number = "2",
}

@inproceedings{Brutzkusetal2018,
title={{SGD} Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data},
author={Alon Brutzkus and Amir Globerson and Eran Malach and Shai Shalev-Shwartz},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJ33wwxRb},
}

@article{Sagunetal2018,
  title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
  author={Levent Sagun and Utku Evci and V. Ugur G{\"u}ney and Yann Dauphin and L{\'e}on Bottou},
  journal={ArXiv},
  year={2018},
  volume={abs/1706.04454}
}

@inproceedings{Chizatetal2018,
 author = {Chizat, L\'{e}na\"{\i}c and Bach, Francis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport},
 url = {https://proceedings.neurips.cc/paper/2018/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf},
 volume = {31},
 year = {2018}
}


@InProceedings{Aroraetal2018,
  title = 	 {On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
  author =       {Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {244--253},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/arora18a/arora18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/arora18a.html},
  abstract = 	 {Conventional wisdom in deep learning states that increasing depth improves expressiveness but complicates optimization. This paper suggests that, sometimes, increasing depth can speed up optimization. The effect of depth on optimization is decoupled from expressiveness by focusing on settings where additional layers amount to overparameterization – linear neural networks, a well-studied model. Theoretical analysis, as well as experiments, show that here depth acts as a preconditioner which may accelerate convergence. Even on simple convex problems such as linear regression with $\ell_p$ loss, $p&gt;2$, gradient descent can benefit from transitioning to a non-convex overparameterized objective, more than it would from some common acceleration schemes. We also prove that it is mathematically impossible to obtain the acceleration effect of overparametrization via gradients of any regularizer.}
}

@article{Geigeretal2021,
title = {Landscape and training regimes in deep learning},
journal = {Physics Reports},
volume = {924},
pages = {1-18},
year = {2021},
note = {Landscape and training regimes in deep learning},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2021.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0370157321001290},
author = {Mario Geiger and Leonardo Petrini and Matthieu Wyart},
keywords = {Deep learning, Jamming, Lazy training, Feature learning, Neural networks, Loss landscape, Curse of dimensionality, Neural tangent kernel},
abstract = {Deep learning algorithms are responsible for a technological revolution in a variety of tasks including image recognition or Go playing. Yet, why they work is not understood. Ultimately, they manage to classify data lying in high dimension – a feat generically impossible due to the geometry of high dimensional space and the associated curse of dimensionality. Understanding what kind of structure, symmetry or invariance makes data such as images learnable is a fundamental challenge. Other puzzles include that (i) learning corresponds to minimizing a loss in high dimension, which is in general not convex and could well get stuck bad minima. (ii) Deep learning predicting power increases with the number of fitting parameters, even in a regime where data are perfectly fitted. In this manuscript, we review recent results elucidating (i, ii) and the perspective they offer on the (still unexplained) curse of dimensionality paradox. We base our theoretical discussion on the (h,α) plane where h controls the number of parameters and α the scale of the output of the network at initialization, and provide new systematic measures of performance in that plane for two common image classification datasets. We argue that different learning regimes can be organized into a phase diagram. A line of critical points sharply delimits an under-parametrized phase from an over-parametrized one. In over-parametrized nets, learning can operate in two regimes separated by a smooth cross-over. At large initialization, it corresponds to a kernel method, whereas for small initializations features can be learnt, together with invariants in the data. We review the properties of these different phases, of the transition separating them and some open questions. Our treatment emphasizes analogies with physical systems, scaling arguments and the development of numerical observables to quantitatively test these results empirically. Practical implications are also discussed, including the benefit of averaging nets with distinct initial weights, or the choice of parameters (h,α) optimizing performance.}
}

@inproceedings{Lietal2019,
 author = {Li, Shuang and Tang, Gongguo and Wakin, Michael B},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Landscape of Non-convex Empirical Risk with Degenerate Population Risk},
 url = {https://proceedings.neurips.cc/paper/2019/file/65fc52ed8f88c81323a418ca94cec2ed-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Geetal2018,
title={Learning One-hidden-layer Neural Networks with Landscape Design},
author={Rong Ge and Jason D. Lee and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BkwHObbRZ},
}


@InProceedings{Safranetal2018,
  title = 	 {Spurious Local Minima are Common in Two-Layer {R}e{LU} Neural Networks},
  author =       {Safran, Itay and Shamir, Ohad},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4433--4441},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/safran18a/safran18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/safran18a.html},
  abstract = 	 {We consider the optimization problem associated with training simple ReLU neural networks of the form $\mathbf{x}\mapsto \sum_{i=1}^{k}\max\{0,\mathbf{w}_i^\top \mathbf{x}\}$ with respect to the squared loss. We provide a computer-assisted proof that even if the input distribution is standard Gaussian, even if the dimension is arbitrarily large, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once $6\le k\le 20$. By a concentration of measure argument, this implies that in high input dimensions, <em>nearly all</em> target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an over-parameterization assumption is necessary to get a positive result in this setting.}
}

@article{Meietal2018,
author = {Song Mei  and Andrea Montanari  and Phan-Minh Nguyen },
title = {A mean field view of the landscape of two-layer neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {33},
pages = {E7665-E7671},
year = {2018},
doi = {10.1073/pnas.1806579115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1806579115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1806579115},
abstract = {Multilayer neural networks have proven extremely successful in a variety of tasks, from image classification to robotics. However, the reasons for this practical success and its precise domain of applicability are unknown. Learning a neural network from data requires solving a complex optimization problem with millions of variables. This is done by stochastic gradient descent (SGD) algorithms. We study the case of two-layer networks and derive a compact description of the SGD dynamics in terms of a limiting partial differential equation. Among other consequences, this shows that SGD dynamics does not become more complex when the network size increases. Multilayer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires optimizing a nonconvex high-dimensional objective (risk function), a problem that is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the former case, does this happen because local minima are absent or because SGD somehow avoids them? In the latter, why do local minima reached by SGD have good generalization properties? In this paper, we consider a simple case, namely two-layer neural networks, and prove that—in a suitable scaling limit—SGD dynamics is captured by a certain nonlinear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows for “averaging out” some of the complexities of the landscape of neural networks and can be used to prove a general convergence result for noisy SGD.}}


@article{Chaudharietal2019,
	doi = {10.1088/1742-5468/ab39d9},
	url = {https://doi.org/10.1088/1742-5468/ab39d9},
	year = 2019,
	month = {dec},
	publisher = {{IOP} Publishing},
	volume = {2019},
	number = {12},
	pages = {124018},
	author = {Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
	title = {Entropy-{SGD}: biasing gradient descent into wide valleys},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	abstract = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.}
}

@article{Zhuetal2019,
  title={The Global Optimization Geometry of Shallow Linear Neural Networks},
  author={Zhihui Zhu and Daniel Soudry and Yonina C. Eldar and Michael B. Wakin},
  journal={Journal of Mathematical Imaging and Vision},
  year={2019},
  volume={62},
  pages={279-292}
}
@inproceedings{Kingmaetal2014,
  author    = {Diederik P. Kingma and
               Max Welling},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Rezendeetal2014,
  author    = {Danilo Jimenez Rezende and
               Shakir Mohamed and
               Daan Wierstra},
  title     = {Stochastic Backpropagation and Approximate Inference in Deep Generative
               Models},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning,
               {ICML} 2014, Beijing, China, 21-26 June 2014},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {32},
  pages     = {1278--1286},
  publisher = {JMLR.org},
  year      = {2014},
  url       = {http://proceedings.mlr.press/v32/rezende14.html},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/RezendeMW14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Jooetal2020,
  author    = {Weonyoung Joo and
               Wonsung Lee and
               Sungrae Park and
               Il{-}Chul Moon},
  title     = {Dirichlet Variational Autoencoder},
  journal   = {Pattern Recognit.},
  volume    = {107},
  pages     = {107514},
  year      = {2020},
  url       = {https://doi.org/10.1016/j.patcog.2020.107514},
  doi       = {10.1016/j.patcog.2020.107514},
  timestamp = {Wed, 26 Aug 2020 11:04:13 +0200},
  biburl    = {https://dblp.org/rec/journals/pr/JooLPM20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Higginsetal2017,
  author    = {Irina Higgins and
               Lo{\"{\i}}c Matthey and
               Arka Pal and
               Christopher P. Burgess and
               Xavier Glorot and
               Matthew M. Botvinick and
               Shakir Mohamed and
               Alexander Lerchner},
  title     = {beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational
               Framework},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Sy2fzU9gl},
  timestamp = {Tue, 26 Apr 2022 19:45:27 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HigginsMPBGBML17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Gulrajanietal2017,
  author    = {Ishaan Gulrajani and
               Faruk Ahmed and
               Mart{\'{\i}}n Arjovsky and
               Vincent Dumoulin and
               Aaron C. Courville},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Improved Training of {W}asserstein {GAN}s},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {5767--5777},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/GulrajaniAADC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Goodfellowetal2014,
  author    = {Ian J. Goodfellow and
               Jean Pouget{-}Abadie and
               Mehdi Mirza and
               Bing Xu and
               David Warde{-}Farley and
               Sherjil Ozair and
               Aaron C. Courville and
               Yoshua Bengio},
  editor    = {Zoubin Ghahramani and
               Max Welling and
               Corinna Cortes and
               Neil D. Lawrence and
               Kilian Q. Weinberger},
  title     = {Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference
               on Neural Information Processing Systems 2014, December 8-13 2014,
               Montreal, Quebec, Canada},
  pages     = {2672--2680},
  year      = {2014},
  url       = {https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:23 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/GoodfellowPMXWOCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Meschederetal2018,
  author    = {Lars M. Mescheder and
               Andreas Geiger and
               Sebastian Nowozin},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {Which Training Methods for GANs do actually Converge?},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {3478--3487},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/mescheder18a.html},
  timestamp = {Fri, 20 Nov 2020 13:22:22 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/MeschederGN18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Karrasetal2019,
  author    = {Tero Karras and
               Samuli Laine and
               Timo Aila},
  title     = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019},
  pages     = {4401--4410},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2019},
  url       = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Karras\_A\_Style-Based\_Generator\_Architecture\_for\_Generative\_Adversarial\_Networks\_CVPR\_2019\_paper.html},
  doi       = {10.1109/CVPR.2019.00453},
  timestamp = {Mon, 30 Aug 2021 17:01:14 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/KarrasLA19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Guptaetal2021,
  title={Path Length Bounds for Gradient Descent and Flow},
  author={Chirag Gupta and Sivaraman Balakrishnan and Aaditya Ramdas},
  journal={J. Mach. Learn. Res.},
  year={2021},
  volume={22},
  pages={68:1-68:63}
}

@article{Azizanetal2021,
  title={Stochastic Mirror Descent on Overparameterized Nonlinear Models.},
  author={Navid Azizan and Sahin Lale and Babak Hassibi},
  journal={IEEE transactions on neural networks and learning systems},
  year={2021},
  volume={PP}
}

@inproceedings{Zhangetal2020,
  title={A type of generalization error induced by initialization in deep neural networks},
  author={Yaoyu Zhang and Zhi-Qin John Xu and Tao Luo and Zheng Ma},
  booktitle={MSML},
  year={2020}
}

@article{Nitandaetal2019,
  title={Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems},
  author={Atsushi Nitanda and Geoffrey Chinot and Taiji Suzuki},
  journal={arXiv: Machine Learning},
  year={2019}
}

@article{Oymaketal2019b,
  title={Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian},
  author={Samet Oymak and Zalan Fabian and Mingchen Li and Mahdi Soltanolkotabi},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.05392}
}

@article{Belkin2021,
  title={Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation},
  author={Mikhail Belkin},
  journal={Acta Numerica},
  year={2021},
  volume={30},
  pages={203 - 248}
}

@article{Belkinetal2019,
  title={Reconciling modern machine-learning practice and the classical bias–variance trade-off},
  author={Mikhail Belkin and Daniel J. Hsu and Siyuan Ma and Soumik Mandal},
  journal={Proceedings of the National Academy of Sciences},
  year={2019},
  volume={116},
  pages={15849 - 15854}
}

@book{Vershynin2018, 
  place={Cambridge}, 
  series={Cambridge Series in Statistical and Probabilistic Mathematics}, 
  title={High-Dimensional Probability: An Introduction with Applications in Data Science}, 
  DOI={10.1017/9781108231596}, 
  publisher={Cambridge University Press}, 
  author={Vershynin, Roman}, 
  year={2018}, 
  collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}

@article{Vladimirovaetal2020,
author = {Vladimirova, Mariia and Girard, Stéphane and Nguyen, Hien and Arbel, Julyan},
title = {Sub-Weibull distributions: Generalizing sub-Gaussian and sub-Exponential properties to heavier tailed distributions},
journal = {Stat},
volume = {9},
number = {1},
pages = {e318},
keywords = {Bayesian neural network, deep learning, sub-Exponential, sub-Gaussian, sub-Weibull},
doi = {https://doi.org/10.1002/sta4.318},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.318},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.318},
note = {e318 sta4.318},
abstract = {We propose the notion of sub-Weibull distributions, which are characterized by tails lighter than (or equally light as) the right tail of a Weibull distribution. This novel class generalizes the sub-Gaussian and sub-Exponential families to potentially heavier tailed distributions. Sub-Weibull distributions are parameterized by a positive tail index θ and reduce to sub-Gaussian distributions for θ=1/2 and to sub-Exponential distributions for θ=1. A characterization of the sub-Weibull property based on moments and on the moment generating function is provided and properties of the class are studied. An estimation procedure for the tail parameter is proposed and is applied to an example stemming from Bayesian deep learning.},
year = {2020}
}

@article{Kuchibhotlaetal2022,
    author = {Kuchibhotla, Arun Kumar and Chakrabortty, Abhishek},
    title = "{Moving beyond sub-Gaussianity in high-dimensional statistics: applications in covariance estimation and linear regression}",
    journal = {Information and Inference: A Journal of the IMA},
    year = {2022},
    month = {06},
    abstract = "{Concentration inequalities form an essential toolkit in the study of high-dimensional statistical methods. Most of the relevant statistics literature in this regard is, however, based on the assumptions of sub-Gaussian or sub-exponential random variables/vectors. In this paper, we first bring together, through a unified exposition, various probabilistic inequalities for sums of independent random variables under much more general exponential type (namely sub-Weibull) tail assumptions. These results extract a part sub-Gaussian tail behavior of the sum in finite samples, matching the asymptotics governed by the central limit theorem, and are compactly represented in terms of a new Orlicz quasi-norm—the Generalized Bernstein–Orlicz norm—that typifies such kind of tail behaviors.We illustrate the usefulness of these inequalities through the analysis of four fundamental problems in high-dimensional statistics. In the first two problems, we study the rate of convergence of the sample covariance matrix in terms of the maximum elementwise norm and the maximum \\$k\\$-sub-matrix operator norm that are key quantities of interest in bootstrap procedures and high-dimensional structured covariance matrix estimation, as well as in high-dimensional and post-selection inference. The third example concerns the restricted eigenvalue condition, required in high-dimensional linear regression, which we verify for all sub-Weibull random vectors through a unified analysis, and also prove a more general result related to restricted strong convexity in the process. In the final example, we consider the Lasso estimator for linear regression and establish its rate of convergence to be generally \\$\\sqrt\\{k\\log p/n\\}\\$, for \\$k\\$-sparse signals, under much weaker than usual tail assumptions (on the errors as well as the covariates), while also allowing for misspecified models and both fixed and random design. To our knowledge, these are the first such results for Lasso obtained in this generality. The common feature in all our results over all the examples is that the convergence rates under most exponential tails match the usual (optimal) ones obtained under sub-Gaussian assumptions. Finally, we also establish some complementary results on analogous tail bounds for the suprema of empirical processes indexed by sub-Weibull variables. All our results are finite samples.}",
    issn = {2049-8772},
    doi = {10.1093/imaiai/iaac012},
    url = {https://doi.org/10.1093/imaiai/iaac012},
    note = {iaac012},
    eprint = {https://academic.oup.com/imaiai/advance-article-pdf/doi/10.1093/imaiai/iaac012/44215716/iaac012.pdf},
}

@article{Soltanolkotabietal2017,
  title={Theoretical Insights Into the Optimization Landscape of Over-Parameterized Shallow Neural Networks},
  author={Mahdi Soltanolkotabi and Adel Javanmard and J. Lee},
  journal={IEEE Transactions on Information Theory},
  year={2017},
  volume={65},
  pages={742-769}
}

@article{Montanarietal2020,
author = {Andrea Montanari and Yiqiao Zhong},
title = {{The interpolation phase transition in neural networks: Memorization and generalization under lazy training}},
volume = {50},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {2816 -- 2847},
keywords = {kernel ridge regression, memorization, neural tangent kernel, overfitting, overparametrization},
year = {2022},
doi = {10.1214/22-AOS2211},
URL = {https://doi.org/10.1214/22-AOS2211}
}

@InProceedings{Nguyenetal2021,
  title = 	 {Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks},
  author =       {Nguyen, Quynh N. and Mondelli, Marco and Montufar, Guido F.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8119--8129},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nguyen21g/nguyen21g.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nguyen21g.html},
  abstract = 	 {A recent line of work has analyzed the theoretical properties of deep neural networks via the Neural Tangent Kernel (NTK). In particular, the smallest eigenvalue of the NTK has been related to the memorization capacity, the global convergence of gradient descent algorithms and the generalization of deep nets. However, existing results either provide bounds in the two-layer setting or assume that the spectrum of the NTK matrices is bounded away from 0 for multi-layer networks. In this paper, we provide tight bounds on the smallest eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of infinite widths and for finite widths. In the finite-width setting, the network architectures we consider are fairly general: we require the existence of a wide layer with roughly order of $N$ neurons, $N$ being the number of data samples; and the scaling of the remaining layer widths is arbitrary (up to logarithmic factors). To obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of hidden feature matrices, and upper bounds on the Lipschitz constant of input-output feature maps.}
}


@article{Wangetal2021,
author = {Zhichao Wang and Yizhe Zhu},
title = {{Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks}},
volume = {34},
journal = {The Annals of Applied Probability},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {1896 -- 1947},
keywords = {neural networks, neural tangent kernel, random feature regression, Random matrix theory},
year = {2024},
doi = {10.1214/23-AAP2010},
URL = {https://doi.org/10.1214/23-AAP2010}
}

@inproceedings{Chizatetal2018b,
author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
title = {On lazy training in differentiable programming},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {264},
numpages = {11}
}

@inproceedings{Yangetal2022,
author = {Yang, Greg and Hu, Edward J. and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
title = {Tensor programs V: tuning large neural networks via zero-shot hyperparameter transfer},
year = {2024},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1306},
numpages = {14},
series = {NIPS '21}
}

@article{Songetal2019,
  title={Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound},
  author={Zhao Song and Xin Yang},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.03593}
}

@article{Wuetal2019,
  title={Global Convergence of Adaptive Gradient Methods for An Over-parameterized Neural Network},
  author={Xiaoxia Wu and Simon Shaolei Du and Rachel A. Ward},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.07111}
}

@inproceedings{Songetal2021,
title={Subquadratic Overparameterization for Shallow Neural Networks},
author={Chaehwan Song and Ali Ramezani-Kebrya and Thomas Pethick and Armin Eftekhari and Volkan Cevher},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=NhbFhfM960}
}

@article{Zouetal2018,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Difan Zou and Yuan Cao and Dongruo Zhou and Quanquan Gu},
  journal={Machine Learning},
  year={2018},
  volume={109},
  pages={467-492}
}

@article{Zouetal2019,
 author = {Zou, Difan and Gu, Quanquan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {An Improved Analysis of Training Over-parameterized Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{Nguyenetal2020,
 author = {Nguyen, Quynh N and Mondelli, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11961--11972},
 publisher = {Curran Associates, Inc.},
 title = {Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Nguyen2021,
  title={On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths},
  author={Quynh N. Nguyen},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231698350}
}

@InProceedings{Bombarietal2022,
title={Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization},
author={Simone Bombari and Mohammad Hossein Amani and Marco Mondelli},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=x8DNliTBSYY}
}

@article{Bartlettetal2021,
  title={Deep learning: a statistical viewpoint},
  author={Peter L. Bartlett and Andrea Montanari and Alexander Rakhlin},
  journal={Acta Numerica},
  year={2021},
  volume={30},
  pages={87 - 201}
}

@book{Villani2008,
  title={Optimal Transport: Old and New},
  author={C. Villani},
  isbn={9783540710509},
  lccn={2008932183},
  series={Grundlehren der mathematischen Wissenschaften},
  year={2008},
  publisher={Springer Berlin Heidelberg}
}

@inproceedings{Aroraetal2019,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Sanjeev Arora and Simon Shaolei Du and Wei Hu and Zhiyuan Li and Ruosong Wang},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@article{Gambaetal2023,
  title={On the Lipschitz Constant of Deep Networks and Double Descent},
  author={Matteo Gamba and Hossein Azizpour and Marten Bjorkman},
  booktitle={British Machine Vision Conference},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:256389664}
}

@article{Khromovetal2023,
title={Some Fundamental Aspects about Lipschitz Continuity of Neural Networks},
author={Grigory Khromov and Sidak Pal Singh},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=5jWsW08zUh}
}

@InProceedings{Woodworthetal2020,
  title = 	 {Kernel and Rich Regimes in Overparametrized Models},
  author =       {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3635--3673},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/woodworth20a/woodworth20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/woodworth20a.html}
}


@InProceedings{Yangetal2021,
  title = 	 {Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks},
  author =       {Yang, Greg and Hu, Edward J.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11727--11737},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21c/yang21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21c.html}
}

@misc{Terjeketal2022,
  doi = {10.48550/ARXIV.2205.13507},
  
  url = {https://arxiv.org/abs/2205.13507},
  
  author = {Terjék, Dávid and González-Sánchez, Diego},
  
  keywords = {Machine Learning (cs.LG), Functional Analysis (math.FA), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics, 68T07 (Primary) 46N10, 90C26 (Secondary)},
  
  title = {A framework for overparameterized learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Hornetal1992,
  title = {Block-matrix generalizations of Schur's basic theorems on Hadamard products},
  journal = {Linear Algebra and its Applications},
  volume = {172},
  pages = {337-346},
  year = {1992},
  issn = {0024-3795},
  doi = {https://doi.org/10.1016/0024-3795(92)90033-7},
  url = {https://www.sciencedirect.com/science/article/pii/0024379592900337},
  author = {Roger A. Horn and Roy Mathias},
  abstract = {In a classic 1911 paper, I. Schur gave several useful bounds for the spectral norm and eigenvalues of the Hadamard (entrywise) product of two matrices. Motivated by applications to the theory of monotone and convex matrix functions, we are led to consider Hadamard products in which both factors are conformally partitioned block matrices and the entries of one factor are constant within each block. Such products are special cases of a block Kronecker (tensor) product, and it is in this context that we present generalizations of Schur's results.}
}

@article{Rudelsonetal2015,
  title={Small Ball Probabilities for Linear Images of High-Dimensional Distributions},
  author={Mark Rudelson and Roman Vershynin},
  journal={International Mathematics Research Notices},
  year={2014},
  volume={2015},
  pages={9594-9617}
}

@article{Barlowetal1969,
  author = {Richard E. Barlow and Albert W. Marshall and Frank Proschan},
  title = {{Some inequalities for starshaped and convex functions.}},
  volume = {29},
  journal = {Pacific Journal of Mathematics},
  number = {1},
  publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
  pages = {19 -- 42},
  year = {1969},
}

@Inbook{Malliavin1995,
  author="Malliavin, Paul",
  title="Gaussian Sobolev Spaces and Stochastic Calculus of Variations",
  bookTitle="Integration and Probability",
  year="1995",
  publisher="Springer New York",
  address="New York, NY",
  pages="229--252",
  isbn="978-1-4612-4202-4",
  doi="10.1007/978-1-4612-4202-4_5",
  url="https://doi.org/10.1007/978-1-4612-4202-4_5"
}

@article{Buraietal2005,
author = {Burai, Pál and Sz{\'a}z, Arpad},
year = {2005},
month = {01},
pages = {77-87},
title = {Relationships between homogeneity, subadditivity and convexity properties},
volume = {16},
journal = {Univ. Beograd. Publ. Elektrotehn. Fak. Ser. Mat.},
doi = {10.2298/PETF0516077B}
}

@article{Reams1999,
  title={Hadamard inverses, square roots and products of almost semidefinite matrices},
  author={Robert Reams},
  journal={Linear Algebra and its Applications},
  year={1999},
  volume={288},
  pages={35-43},
  url={https://api.semanticscholar.org/CorpusID:18558077}
}

@Article{Murthy2015,
author={Murthy, G. S. R.},
title={A Note on Multivariate Folded Normal Distribution},
journal={Sankhya B},
year={2015},
month={May},
day={01},
volume={77},
number={1},
pages={108-113},
abstract={This note points out some flaws in an article by Chakraborty and Chatterjee (Sankhya: The Indian Journal of Statistics, 75-B, 1, 1--15, 2013) in which formulae for the mean vector, dispersion matrix and the moment generating function of multivariate folded normal distribution are derived. It is observed that the formulae and their derivation are incorrect. Correct formula of the mean for general case, and formula for the dispersion for the special case where the mean of the parent distribution is zero are stated.},
issn={0976-8394},
doi={10.1007/s13571-014-0092-9},
url={https://doi.org/10.1007/s13571-014-0092-9}
}

@article{Kanetal2017,
author = {Raymond Kan and Cesare Robotti},
title = {On Moments of Folded and Truncated Multivariate Normal Distributions},
journal = {Journal of Computational and Graphical Statistics},
volume = {26},
number = {4},
pages = {930-934},
year = {2017},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2017.1322092},
URL = {https://doi.org/10.1080/10618600.2017.1322092},
eprint = {https://doi.org/10.1080/10618600.2017.1322092}
}

@article{Minsker2017,
title = {On some extensions of Bernstein’s inequality for self-adjoint operators},
journal = {Statistics \& Probability Letters},
volume = {127},
pages = {111-119},
year = {2017},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2017.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167715217301207},
author = {Stanislav Minsker}
}

@article{Rosascoetal2010,
  title={On Learning with Integral Operators},
  author={Lorenzo Rosasco and Mikhail Belkin and Ernesto de Vito},
  journal={J. Mach. Learn. Res.},
  year={2010},
  volume={11},
  pages={905-934},
  url={https://api.semanticscholar.org/CorpusID:6304931}
}

@inproceedings{Banerjeeetal2023,
  title={Neural Tangent Kernel at Initialization: Linear Width Suffices},
  author={Arindam Banerjee and Pedro Cisneros-Velarde and Libin Zhu and Misha Belkin},
  booktitle={The 39th Conference on Uncertainty in Artificial Intelligence},
  year={2023},
  url={https://openreview.net/forum?id=VJaoe7Rp9tZ}
}

@book{Odonell2014,
  author       = {Ryan O'Donnell},
  title        = {Analysis of Boolean Functions},
  publisher    = {Cambridge University Press},
  year         = {2014},
  url          = {http://www.cambridge.org/de/academic/subjects/computer-science/algorithmics-complexity-computer-algebra-and-computational-g/analysis-boolean-functions},
  isbn         = {978-1-10-703832-5}
}

@article{Danielyetal2016,
 author = {Daniely, Amit and Frostig, Roy and Singer, Yoram},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf},
 volume = {29},
 year = {2016}
}

@book{Hornetal1994,
  title={Topics in Matrix Analysis},
  author={Horn, R.A. and Johnson, C.R.},
  isbn={9780521467131},
  lccn={86023310},
  url={https://books.google.hu/books?id=LeuNXB2bl5EC},
  year={1994},
  publisher={Cambridge University Press}
}

@inproceedings{Pooleetal2016,
  author = {Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Exponential expressivity in deep neural networks through transient chaos},
  url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf},
  volume = {29},
  year = {2016}
}

@inproceedings{Schoenholzetal2017,
title={Deep Information Propagation},
author={Samuel S. Schoenholz and Justin Gilmer and Surya Ganguli and Jascha Sohl-Dickstein},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=H1W1UN9gg}
}

@InProceedings{Hayouetal2019,
  title = 	 {On the Impact of the Activation function on Deep Neural Networks Training},
  author =       {Hayou, Soufiane and Doucet, Arnaud and Rousseau, Judith},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2672--2680},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/hayou19a/hayou19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/hayou19a.html}
}

@misc{Yang2020,
      title={Tensor Programs II: Neural Tangent Kernel for Any Architecture}, 
      author={Greg Yang},
      year={2020},
      eprint={2006.14548},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Xuetal2024,
  author  = {Jiaming Xu and Hanjing Zhu},
  title   = {Overparametrized Multi-layer Neural Networks: Uniform Concentration of Neural Tangent Kernel and Convergence of Stochastic Gradient Descent},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {94},
  pages   = {1--83},
  url     = {http://jmlr.org/papers/v25/23-0740.html}
}

@InProceedings{Seleznovaetal22,
  title = 	 {Neural Tangent Kernel Beyond the Infinite-Width Limit: Effects of Depth and Initialization},
  author =       {Seleznova, Mariia and Kutyniok, Gitta},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {19522--19560},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/seleznova22a/seleznova22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/seleznova22a.html},
  abstract = 	 {Neural Tangent Kernel (NTK) is widely used to analyze overparametrized neural networks due to the famous result by Jacot et al. (2018): in the infinite-width limit, the NTK is deterministic and constant during training. However, this result cannot explain the behavior of deep networks, since it generally does not hold if depth and width tend to infinity simultaneously. In this paper, we study the NTK of fully-connected ReLU networks with depth comparable to width. We prove that the NTK properties depend significantly on the depth-to-width ratio and the distribution of parameters at initialization. In fact, our results indicate the importance of the three phases in the hyperparameter space identified in Poole et al. (2016): ordered, chaotic and the edge of chaos (EOC). We derive exact expressions for the NTK dispersion in the infinite-depth-and-width limit in all three phases and conclude that the NTK variability grows exponentially with depth at the EOC and in the chaotic phase but not in the ordered phase. We also show that the NTK of deep networks may stay constant during training only in the ordered phase and discuss how the structure of the NTK matrix changes during training.}
}

@InProceedings{Hayouetal2022,
  title = 	 {The Curse of Depth in Kernel Regime},
  author =       {Hayou, Soufiane and Doucet, Arnaud and Rousseau, Judith},
  booktitle = 	 {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
  pages = 	 {41--47},
  year = 	 {2022},
  editor = 	 {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
  volume = 	 {163},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v163/hayou22a/hayou22a.pdf},
  url = 	 {https://proceedings.mlr.press/v163/hayou22a.html},
  abstract = 	 {Recent work by Jacot et al. (2018) has shown that training a neural network of any kind with gradient descent is strongly related to kernel gradient descent in function space with respect to the Neural Tangent Kernel (NTK). Empirical results in (Lee et al., 2019) demonstrated high performance of a linearized version of training using the so-called NTK regime. In this paper, we show that the large depth limit of this regime is unexpectedly trivial, and we fully characterize the convergence rate to this trivial regime.}
}

@InProceedings{Xiaoetal2020,
  title = 	 {Disentangling Trainability and Generalization in Deep Neural Networks},
  author =       {Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10462--10472},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/xiao20b/xiao20b.pdf},
  url = 	 {https://proceedings.mlr.press/v119/xiao20b.html},
  abstract = 	 {A longstanding goal in the theory of deep learning is to characterize the conditions under which a given neural network architecture will be trainable, and if so, how well it might generalize to unseen data. In this work, we provide such a characterization in the limit of very wide and very deep networks, for which the analysis simplifies considerably. For wide networks, the trajectory under gradient descent is governed by the Neural Tangent Kernel (NTK), and for deep networks the NTK itself maintains only weak data dependence. By analyzing the spectrum of the NTK, we formulate necessary conditions for trainability and generalization across a range of architectures, including Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs). We identify large regions of hyperparameter space for which networks can memorize the training set but completely fail to generalize. We find that CNNs without global average pooling behave almost identically to FCNs, but that CNNs with pooling have markedly different and often better generalization performance. These theoretical results are corroborated experimentally on CIFAR10 for a variety of network architectures. We include a \href{https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/disentangling_trainability_and_generalization.ipynb}{colab} notebook that reproduces the essential results of the paper.}
}

@InProceedings{Haninetal2020,
  title={Finite Depth and Width Corrections to the Neural Tangent Kernel},
  author={Boris Hanin and Mihai Nica},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SJgndT4KwB}
}

@inproceedings{Yangetal2024,
  title={Tensor Programs {VI}: Feature Learning in Infinite Depth Neural Networks},
  author={Greg Yang and Dingli Yu and Chen Zhu and Soufiane Hayou},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=17pVDnpwwl}
}

@misc{Yang2021,
  title={Tensor Programs III: Neural Matrix Laws}, 
  author={Greg Yang},
  year={2021},
  eprint={2009.10685},
  archivePrefix={arXiv},
  primaryClass={cs.NE}
}

@inproceedings{Suetal2019,
  author = {Su, Lili and Yang, Pengkun},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/253f7b5d921338af34da817c00f42753-Paper.pdf},
  volume = {32},
  year = {2019}
}

@misc{Yangetal2023,
      title={A Spectral Condition for Feature Learning}, 
      author={Greg Yang and James B. Simon and Jeremy Bernstein},
      year={2023},
      eprint={2310.17813},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{Vandervaartetal2023,
  title={Weak Convergence and Empirical Processes: With Applications to Statistics},
  author={van der Vaart, A.W. and Wellner, J.A.},
  isbn={9783031290404},
  series={Springer Series in Statistics},
  url={https://books.google.hu/books?id=vfzKEAAAQBAJ},
  year={2023},
  publisher={Springer International Publishing}
}

@article{Schoenberg1937,
  title={On Certain Metric Spaces Arising From Euclidean Spaces by a Change of Metric and Their Imbedding in Hilbert Space},
  author={I. J. Schoenberg},
  journal={Annals of Mathematics},
  year={1937},
  volume={38},
  pages={787},
  url={https://api.semanticscholar.org/CorpusID:123211412}
}

@Article{Horvath2023,
  author={Horv{\'a}th, L{\'a}szl{\'o}},
  title={Integral inequalities using signed measures corresponding to majorization},
  journal={Revista de la Real Academia de Ciencias Exactas, F{\'i}sicas y Naturales. Serie A. Matem{\'a}ticas},
  year={2023},
  month={Mar},
  day={02},
  volume={117},
  number={2},
  pages={80},
  abstract={In this paper we derive majorization type integral inequalities using measure spaces with signed measures. We obtain necessary and sufficient conditions for the studied integral inequalities to be satisfied. To apply our results, we first generalize Hardy--Littlewood--P{\'o}lya and Fuchs inequalities. Then we deal with the nonnegativity of some integrals with nonnegative convex functions. As a consequence, the known characterization of Steffensen--Popoviciu measures on compact intervals is extended to arbitrary intervals. Finally, we give necessary and sufficient conditions for the satisfaction of the integral Jensen inequality and the integral Lah--Ribari{\v{c}} inequality for signed measures. All the considered problems are also studied for special classes of convex functions. To prove the main assertions some approximation results for nonnegative convex functions are also developed.},
  issn={1579-1505},
  doi={10.1007/s13398-023-01409-7},
  url={https://doi.org/10.1007/s13398-023-01409-7}
}

@book{Tretter2008,
  author = {Tretter, Christiane},
  title = {Spectral Theory of Block Operator Matrices and Applications},
  publisher = {IMPERIAL COLLEGE PRESS},
  year = {2008},
  doi = {10.1142/p493},
  address = {},
  edition   = {},
  URL = {https://www.worldscientific.com/doi/abs/10.1142/p493},
  eprint = {https://www.worldscientific.com/doi/pdf/10.1142/p493}
}

@article{Jakovcevicetal2015,
  title = {Forward stable eigenvalue decomposition of rank-one modifications of diagonal matrices},
  journal = {Linear Algebra and its Applications},
  volume = {487},
  pages = {301-315},
  year = {2015},
  issn = {0024-3795},
  doi = {https://doi.org/10.1016/j.laa.2015.09.025},
  url = {https://www.sciencedirect.com/science/article/pii/S0024379515005406},
  author = {N. {Jakovčević Stor} and I. Slapničar and J.L. Barlow},
  keywords = {Eigenvalue decomposition, Diagonal-plus-rank-one matrix, Real symmetric matrix, Arrowhead matrix, High relative accuracy, Forward stability},
  abstract = {We present a new algorithm for solving an eigenvalue problem for a real symmetric matrix which is a rank-one modification of a diagonal matrix. The algorithm computes each eigenvalue and all components of the corresponding eigenvector with high relative accuracy in O(n) operations. The algorithm is based on a shift-and-invert approach. Only a single element of the inverse of the shifted matrix eventually needs to be computed with double the working precision. Each eigenvalue and the corresponding eigenvector can be computed separately, which makes the algorithm adaptable for parallel computing. Our results extend to the complex Hermitian case. The algorithm is similar to the algorithm for solving the eigenvalue problem for real symmetric arrowhead matrices from N. Jakovčević Stor et al. (2015) [16].}
}

@misc{mlpsateoc1,
  title={{MLP}s at the {EOC}: Spectrum of the {NTK}}, 
  author={D\'avid Terj\'ek and Diego Gonz\'alez-S\'anchez},
  year={2025},
  eprint={????.?????},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{mlpsateoc2,
  title={{MLP}s at the {EOC}: Concentration of the {NTK}}, 
  author={D\'avid Terj\'ek and Diego Gonz\'alez-S\'anchez},
  year={2025},
  eprint={????.?????},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{mlpsateoc3,
  title={MLPs at the EOC: Feature Learning}, 
  author={D\'avid Terj\'ek},
  year={2025},
  eprint={????.?????},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}