\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{tocloft}

% Page setup
\geometry{a4paper, margin=2.5cm}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{NTK Eigenvalue Bounds: Comprehensive Analysis}
\fancyhead[R]{\thepage}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}

% Mathematical commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\evmin}[1]{\lambda_{\min}\left(#1\right)}
\newcommand{\evmax}[1]{\lambda_{\max}\left(#1\right)}
\newcommand{\svmin}[1]{\sigma_{\min}\left(#1\right)}
\newcommand{\tr}{\text{tr}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\u}{\mathbf{u}}
\newcommand{\v}{\mathbf{v}}
\newcommand{\odot}{\odot}
\newcommand{\bigOmg}{\Omega}
\newcommand{\bigTheta}{\Theta}
\newcommand{\Pr}{\text{Pr}}
\newcommand{\KNTK}{K_{\text{NTK}}}
\newcommand{\KLim}{K^{(L)}}
\newcommand{\KEmp}{\overline{K}^{(L)}}
\newcommand{\KCosine}{K_L}
\newcommand{\lambdaMin}{\lambda_{\min}}
\newcommand{\sigmaMin}{\sigma_{\min}}
\newcommand{\Order}{\mathcal{O}}
\newcommand{\TildeOrder}{\tilde{\mathcal{O}}}
\newcommand{\OmegaNotation}{\Omega}
\newcommand{\TildeOmega}{\tilde{\Omega}}
\newcommand{\cN}{\mathcal{N}}

\title{Comprehensive Analysis of Neural Tangent Kernel Eigenvalue Bounds:\\
Mathematical Foundations, Proof Techniques, and Concentration Inequalities}

\author{Synthesis from Banerjee et al., Nguyen et al., and Related Works}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
%This document provides an exhaustive compilation and analysis of mathematical techniques for bounding the smallest eigenvalue $\lambdaMin(\KNTK)$ of the Neural Tangent Kernel. We synthesize results from three complementary perspectives: (1) concentration inequalities and their applications to NTK analysis, (2) timeline-based evolution of proof techniques from classical to modern approaches, and (3) detailed lemma-by-lemma analysis of fundamental results. The document covers the progression from classical Hermite polynomial methods requiring $\TildeOmega(n^2)$ width to modern generalized Hermite techniques achieving $\TildeOmega(n)$ width, along with empirical NTK decomposition methods and spectral analysis via power series. Key mathematical tools include matrix concentration inequalities (Chernoff, Bernstein, Hanson-Wright), matrix analysis techniques (Weyl, Schur, Gershgorin), and specialized tools for neural network analysis (Khatri-Rao products, generalized Hermite polynomials).
\end{abstract}

\tableofcontents
\newpage

\section{Introduction: The Fundamental Problem}

The Neural Tangent Kernel (NTK) has emerged as a central object in the theoretical analysis of deep neural networks. For a neural network $f(\x; \theta)$ with parameters $\theta$, the NTK is defined as:
$$\KNTK(x_i, x_j) = \left\langle \frac{\partial f(\x_i; \theta)}{\partial \theta}, \frac{\partial f(\x_j; \theta)}{\partial \theta} \right\rangle$$

In matrix form, $\KNTK = JJ^T$ where $J$ is the Jacobian matrix of the network outputs with respect to parameters. The smallest eigenvalue $\lambdaMin(\KNTK)$ plays a crucial role in:


The central question addressed in this document is: \textit{Under what conditions can we guarantee $\lambdaMin(\KNTK) \geq c > 0$ with high probability?}


\section{What the SOTA gives us}

\begin{itemize}
    \item \textbf{Banerjee et al. (2023)}: 1 linear width layer suffices for ReLU networks to get confident bounds on $\lambdaMin$
    \item \textbf{Nguyen et al. (2021)}: Very tight linear bound
    \item \textbf{Saratchandran et al. (2024)}: Periodic activations (cosine)
    \item \textbf{Tarjek (2022)}: linear scaling of $\lambdaMin$ for MLP at the EOCs
\end{itemize}


What is behind everything is the kernel integral operator $K$ and its spectrum.

\newpage

\section{Mathematical Foundation: Concentration Inequalities}

\subsection{Scalar Concentration Inequalities}

The foundation of modern NTK analysis rests on powerful concentration inequalities that control the deviation of random variables from their expectations.

\begin{theorem}[Chernoff Bound for Bounded Random Variables]
Let $X_1, \ldots, X_n$ be independent random variables with $X_i \in [0,1]$ and $\E[X_i] = \mu_i$. Let $S = \sum_{i=1}^n X_i$ and $\mu = \E[S] = \sum_{i=1}^n \mu_i$. Then:
$$\Pr(S \geq (1+\delta)\mu) \leq e^{-\frac{\delta^2 \mu}{2+\delta}}$$
$$\Pr(S \leq (1-\delta)\mu) \leq e^{-\frac{\delta^2 \mu}{2}}$$
for $\delta > 0$.
\end{theorem}

\begin{theorem}[Bernstein's Inequality]
Let $X_1, \ldots, X_n$ be independent random variables with $\E[X_i] = 0$, $|X_i| \leq M$, and $\E[X_i^2] \leq \sigma_i^2$. Let $S = \sum_{i=1}^n X_i$ and $\sigma^2 = \sum_{i=1}^n \sigma_i^2$. Then:
$$\Pr(|S| \geq t) \leq 2\exp\left(-\frac{t^2/2}{\sigma^2 + Mt/3}\right)$$
\end{theorem}

\subsection{Quadratic Forms and Hanson-Wright Inequalities}

For neural networks, we frequently encounter quadratic forms in random variables, necessitating more sophisticated concentration tools.

\begin{theorem}[Classical Hanson-Wright Inequality]
Let $Y_1, \ldots, Y_n$ be independent mean-zero $\alpha$-subgaussian random variables, and $\mathbf{A} = (a_{i,j})$ be a symmetric matrix. Then:
$$\Pr\left(\left|\sum_{i,j=1}^n a_{i,j}(Y_i Y_j - \E[Y_i Y_j])\right| \geq t\right) \leq 2\exp\left(-\frac{1}{C}\min\left\{\frac{t^2}{\beta^4\|\mathbf{A}\|_{HS}^2}, \frac{t}{\beta^2\|\mathbf{A}\|_{op}}\right\}\right)$$
\end{theorem}

where $\|\mathbf{A}\|_{HS} = \sqrt{\sum_{i,j} |a_{i,j}|^2}$ is the Hilbert-Schmidt norm and $\|\mathbf{A}\|_{op}$ is the operator norm.

\begin{theorem}[Generalized Hanson-Wright for Random Tensors (Chang, 2022)]
For random tensor vector $\overline{\mathcal{X}} \in \C^{(n \times I_1 \times \cdots \times I_M) \times (I_1 \times \cdots \times I_M)}$ and fixed tensor $\overline{\overline{\mathcal{A}}}$, the polynomial function:
$$f_j(\overline{\mathcal{X}}) = \left(\sum_{i,k=1}^n \mathcal{A}_{i,k} \star_M (\mathcal{X}_i - \E[\mathcal{X}_i]) \star_M (\mathcal{X}_k - \E[\mathcal{X}_k])\right)^j$$
satisfies concentration bounds involving Ky Fan $k$-norms of tensor sums, extending classical matrix concentration to tensor settings.
\end{theorem}

\subsection{Matrix Concentration Inequalities}

Matrix concentration inequalities provide direct control over eigenvalues of random matrix sums, which is essential for NTK analysis.

\begin{theorem}[Matrix Chernoff Bound (Tropp, 2012)]
Let $X_1, \ldots, X_n$ be independent random Hermitian matrices with $X_i \preceq R \cdot I$ and $\E[X_i] \preceq \mu \cdot I$. Let $S = \sum_{i=1}^n X_i$. Then:
$$\Pr(\lambda_{\max}(S) \geq (1+\delta)\mu n) \leq d \left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^{\mu n / R}$$
\end{theorem}

\begin{theorem}[Matrix Bernstein Inequality (Vershynin, 2018)]
Let $X_1, \ldots, X_n$ be independent random matrices with $\E[X_i] = 0$, $\|X_i\| \leq L$, and $\left\|\sum_{i=1}^n \E[X_i X_i^*]\right\| \leq \sigma^2$. Then:
$$\Pr\left(\left\|\sum_{i=1}^n X_i\right\| \geq t\right) \leq 2d \exp\left(-\frac{t^2/2}{\sigma^2 + Lt/3}\right)$$
\end{theorem}

\section{Fundamental Mathematical Tools}

\subsection{Matrix Analysis Techniques}

\begin{lemma}[Weyl's Inequality]
For Hermitian matrices $A, B$:
$$\lambda_i(A) + \lambda_n(B) \leq \lambda_i(A + B) \leq \lambda_i(A) + \lambda_1(B)$$
\end{lemma}

\begin{lemma}[Schur Product Theorem]
For PSD matrices $P, Q$:
$$\evmin{P \circ Q} \geq \evmin{P} \min_i Q_{ii}$$
\end{lemma}

\begin{lemma}[Gershgorin Circle Theorem]
For matrix $A = (a_{ij})$, all eigenvalues lie in the union of discs:
$$\bigcup_{i=1}^n \left\{z \in \C : |z - a_{ii}| \leq \sum_{j \neq i} |a_{ij}|\right\}$$
\end{lemma}

\subsection{Specialized Neural Network Tools}

\begin{definition}[Khatri-Rao Product]
For matrices $A \in \R^{m \times k}$ and $B \in \R^{n \times k}$, the Khatri-Rao product $A \star B \in \R^{mn \times k}$ is the column-wise Kronecker product.
\end{definition}

\begin{definition}[Hadamard Power]
For matrix $A$ and positive integer $r$, the $r$-th Hadamard power $A^{\odot r}$ is the element-wise $r$-th power.
\end{definition}

\newpage

\section{Proof Schemes and High-Level Strategies}

\subsection{General Framework for Eigenvalue Bounds}

All modern approaches to bounding $\lambdaMin(\KNTK)$ follow a common high-level structure, though they differ in technical details and specific tools employed.

\begin{enumerate}
    \item \textbf{Step 1: Problem Decomposition}
    \begin{itemize}
        \item Decompose the NTK into manageable components
        \item Separate empirical from expected quantities
        \item Identify the key matrices/terms to bound
    \end{itemize}
    
    \item \textbf{Step 2: Concentration Analysis}
    \begin{itemize}
        \item Apply concentration inequalities to control random fluctuations
        \item Relate empirical quantities to their expectations
        \item Establish high-probability bounds
    \end{itemize}
    
    \item \textbf{Step 3: Spectral Analysis}
    \begin{itemize}
        \item Analyze the spectrum of expected quantities
        \item Use matrix inequalities to combine bounds
        \item Derive final eigenvalue bounds
    \end{itemize}
\end{enumerate}

\subsection{Proof Scheme 1: Matrix Concentration + Generalized Hermite (Banerjee et al.)}

\textbf{High-Level Strategy}: Use generalized Hermite polynomials to handle inhomogeneous activations, combined with matrix concentration.

\begin{enumerate}
    \item \textbf{Setup and Decomposition}
    \begin{itemize}
        \item \textit{Input}: Neural network with smooth activation $\phi$, width $m_l$ at layer $l$
        \item \textit{Goal}: Bound $\lambdaMin(\KNTK) = \lambdaMin(JJ^T)$
        \item \textit{Key insight}: Focus on activation matrices $A^{(l)} \in \R^{n \times m_l}$
    \end{itemize}
    
    \item \textbf{Apply Matrix Concentration (Lemma 4.1)}
    \begin{itemize}
        \item \textit{Tool}: Matrix Chernoff bound
        \item \textit{Result}: $\evmin{A^{(l)}(A^{(l)})^T} \geq \frac{m_l \lambda_l}{4}$ w.h.p.
        \item \textit{Key quantity}: $\lambda_l = \evmin{\E[\phi(\text{input}) \phi(\text{input})^T]}$
    \end{itemize}
    
    \item \textbf{Generalized Hermite Expansion (Innovation)}
    \begin{itemize}
        \item \textit{Classical limitation}: Standard Hermite only works for homogeneous activations
        \item \textit{Solution}: Use $H_r^{[c^2\sigma^2]}(cx)$ with variable variance parameter
        \item \textit{Result}: Handle activations with different scales across layers
    \end{itemize}
    
    \item \textbf{Hermite Series Analysis (Lemma A.2, A.3)}
    \begin{itemize}
        \item \textit{Apply}: Orthogonality of generalized Hermite polynomials
        \item \textit{Get}: $\E[\phi(CU\tilde{g}) \phi(CU\tilde{g})^T] \succeq \sum_{r=0}^{\infty} \sigma^{6r} c_0^{6r} (M_r(\phi) U^{\star r})(M_r(\phi) U^{\star r})^T$
        \item \textit{Key}: Khatri-Rao powers $U^{\star r}$ capture interaction structure
    \end{itemize}
    
    \item \textbf{Recursive Bound Construction (Theorem 4.2)}
    \begin{itemize}
        \item \textit{Apply}: Induction across layers
        \item \textit{Control}: Hermite coefficients $\mu_r^{[c_{l,i}^2 \sigma^2]}(\phi)$
        \item \textit{Get}: $\lambda_{l+1} \geq \left(\frac{(\mu_{r,0}^{(l)})^2}{6 c_{\phi,\sigma_0}}\right)^l \left(\frac{\sigma_0^2}{2}\right)^{3rl} \lambda_1$
    \end{itemize}
    
    \item \textbf{Width Optimization}
    \begin{itemize}
        \item \textit{Classical requirement}: $m_l \geq \tilde{\Omega}(n^2)$
        \item \textit{Generalized Hermite achievement}: $m_l \geq \tilde{\Omega}(n)$
        \item \textit{Key factor}: Better control of variance parameters
    \end{itemize}
\end{enumerate}

\subsection{Proof Scheme 2: Empirical NTK Decomposition (Nguyen et al.)}

\textbf{High-Level Strategy}: Direct decomposition of empirical NTK $JJ^T$ using feature matrices and chain rule.

\begin{enumerate}
    \item \textbf{NTK Decomposition (Chain Rule)}
    \begin{itemize}
        \item \textit{Apply}: Chain rule for gradients
        \item \textit{Get}: $JJ^T = \sum_{k=0}^{L-1} F_k F_k^T \circ G_{k+1} G_{k+1}^T$
        \item \textit{Components}: $F_k$ (feature matrices), $G_k$ (derivative terms)
    \end{itemize}
    
    \item \textbf{Apply Weyl's Inequality}
    \begin{itemize}
        \item \textit{Tool}: $\lambda_i(A + B) \geq \lambda_i(A) + \lambda_n(B)$
        \item \textit{Result}: $\evmin{JJ^T} \geq \sum_{k=0}^{L-1} \evmin{F_k F_k^T \circ G_{k+1} G_{k+1}^T}$
        \item \textit{Strategy}: Bound each term separately
    \end{itemize}
    
    \item \textbf{Apply Schur Product Theorem}
    \begin{itemize}
        \item \textit{Tool}: For PSD matrices $P, Q$: $\evmin{P \circ Q} \geq \evmin{P} \min_i Q_{ii}$
        \item \textit{Get}: $\evmin{F_k F_k^T \circ G_{k+1} G_{k+1}^T} \geq \evmin{F_k F_k^T} \min_i \|(G_{k+1})_{i:}\|_2^2$
        \item \textit{Reduction}: Two separate problems to solve
    \end{itemize}
    
    \item \textbf{Bound Feature Matrix Eigenvalues (Theorem 5.1)}
    \begin{itemize}
        \item \textit{Apply}: Matrix Chernoff concentration
        \item \textit{Relate}: $\evmin{F_k F_k^T}$ to $\evmin{\E[F_k F_k^T]}$
        \item \textit{Use}: Hermite expansion for expected quantities
        \item \textit{Apply}: Gershgorin circle theorem for data-dependent bounds
    \end{itemize}
    
    \item \textbf{Bound Vector Norms (G-bound Lemma)}
    \begin{itemize}
        \item \textit{Target}: $\min_i \|(G_{k+1})_{i:}\|_2^2$
        \item \textit{Apply}: Hanson-Wright inequality for quadratic forms
        \item \textit{Control}: Products of weight matrices and activation patterns
        \item \textit{Get}: $\|(G_{k+1})_{i:}\|_2^2 \geq c > 0$ w.h.p.
    \end{itemize}
    
    \item \textbf{Combine Bounds}
    \begin{itemize}
        \item \textit{Apply}: Union bound over all layers and data points
        \item \textit{Optimize}: Width requirements for each layer
        \item \textit{Result}: $\evmin{JJ^T} \geq c \cdot \text{(data and architecture dependent)}$
    \end{itemize}
\end{enumerate}

\subsection{Proof Scheme 3: Spectral Analysis via Power Series}

\textbf{High-Level Strategy}: Represent NTK as power series and analyze complete spectrum.

\begin{enumerate}
    \item \textbf{Power Series Representation}
    \begin{itemize}
        \item \textit{Setup}: Unit variance initialization
        \item \textit{Expand}: $K_{\text{NTK}}(\langle \x_i, \x_j \rangle) = \sum_{p=0}^{\infty} a_p \langle \x_i, \x_j \rangle^p$
        \item \textit{Coefficients}: $a_p = \sum_{k=1}^L F(p, k, \bar{\mu})$ (Hermite-based)
    \end{itemize}
    
    \item \textbf{Coefficient Analysis}
    \begin{itemize}
        \item \textit{Study}: Decay rate $|a_p| \leq C \rho^p$ for $\rho < 1$
        \item \textit{Identify}: Dominant terms (low-order $p$)
        \item \textit{Control}: Tail behavior of series
    \end{itemize}
    
    \item \textbf{Spectral Decomposition}
    \begin{itemize}
        \item \textit{Represent}: $K = \sum_{j=0}^{m-1} c_j G^{\odot j}$ (finite truncation)
        \item \textit{Apply}: Effective rank lemma
        \item \textit{Get}: $\text{rank}(K_m) \leq \text{controlled expression}$
    \end{itemize}
    
    \item \textbf{Eigenvalue Separation}
    \begin{itemize}
        \item \textit{Identify}: "Outlier" eigenvalues vs. bulk spectrum
        \item \textit{Show}: Spectral gap between large and small eigenvalues
        \item \textit{Bound}: $\lambda_{\min}$ in terms of series coefficients
    \end{itemize}
    
    \item \textbf{Condition Number Analysis}
    \begin{itemize}
        \item \textit{Compute}: $\frac{\lambda_{\max}}{\lambda_{\min}}$ ratio
        \item \textit{Show}: Only $O(d)$ large eigenvalues when $m \gg n \gg d$
        \item \textit{Result}: Well-conditioned kernel matrix
    \end{itemize}
\end{enumerate}

\subsection{Comparative Analysis of Proof Strategies}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Approach} & \textbf{Key Innovation} & \textbf{Main Tool} & \textbf{Strength} \\
\hline
Generalized Hermite & Variable variance & Matrix Chernoff & Optimal width \\
\hline
NTK Decomposition & Feature matrices & Schur + Weyl & Direct analysis \\
\hline
Power Series & Complete spectrum & Effective rank & Full characterization \\
\hline
\end{tabular}
\end{table}

\subsection{Common Proof Patterns and Techniques}

\textbf{Pattern 1: Concentration-then-Spectral}
\begin{enumerate}
    \item Apply concentration inequality (Chernoff, Bernstein, Hanson-Wright)
    \item Relate random matrix to its expectation
    \item Analyze spectrum of expected matrix
    \item Combine using matrix perturbation theory
\end{enumerate}

\textbf{Pattern 2: Decompose-then-Bound}
\begin{enumerate}
    \item Decompose complex matrix into simpler components
    \item Apply matrix inequalities (Weyl, Schur) to preserve eigenvalue bounds
    \item Bound each component separately
    \item Reconstruct bound for original matrix
\end{enumerate}

\textbf{Pattern 3: Series-then-Truncate}
\begin{enumerate}
    \item Represent kernel as infinite series
    \item Analyze convergence and decay properties
    \item Truncate to finite approximation
    \item Control approximation error and bound spectrum
\end{enumerate}

\subsection{Critical Technical Innovations}

\begin{enumerate}
    \item \textbf{Generalized Hermite Polynomials}
    \begin{itemize}
        \item \textit{Problem}: Classical Hermite fails for inhomogeneous activations
        \item \textit{Solution}: $H_r^{[q]}(x)$ with variable variance parameter $q$
        \item \textit{Impact}: Reduces width requirement from $\tilde{\Omega}(n^2)$ to $\tilde{\Omega}(n)$
    \end{itemize}
    
    \item \textbf{Matrix Decomposition Techniques}
    \begin{itemize}
        \item \textit{Problem}: Direct analysis of $JJ^T$ is complex
        \item \textit{Solution}: Decompose via feature matrices and chain rule
        \item \textit{Impact}: Enables layer-by-layer analysis
    \end{itemize}
    
    \item \textbf{Advanced Matrix Concentration}
    \begin{itemize}
        \item \textit{Problem}: Scalar concentration insufficient for matrices
        \item \textit{Solution}: Matrix Chernoff, Bernstein, Hanson-Wright
        \item \textit{Impact}: Direct control of eigenvalues
    \end{itemize}
    
    \item \textbf{Spectral Analysis Tools}
    \begin{itemize}
        \item \textit{Problem}: Understanding complete eigenvalue distribution
        \item \textit{Solution}: Power series + effective rank analysis
        \item \textit{Impact}: Full characterization of kernel spectrum
    \end{itemize}
\end{enumerate}
















\newpage
\section{Main Theorems and Final Bounds}

\subsection{Limiting NTK Results}

\begin{theorem}[Smallest Eigenvalue of Limiting NTK (Nguyen et al.)]
Let $\{x_i\}_{i=1}^{N}$ be a set of i.i.d. points from $P_X$ satisfying the distributional assumptions. For any even integer $r\ge 2$:
$$L\Order(d) \geq \evmin{K^{(L)}} \geq \mu_r(\sigma)^2\; \bigOmg(d)$$

with probability at least $1 - Ne^{-\bigOmg{d}} - N^2e^{-\bigOmg{dN^{-2/(r-0.5)}}}$
\end{theorem}

where $\mu_r(\sigma)$ is the $r$-th Hermite coefficient of the ReLU function.

\subsection{Empirical NTK Results}

\begin{theorem}[NTK at Initialization (Banerjee et al.)]
For smooth activations with appropriate width conditions:
$$\lambdaMin(\KNTK(\cdot;\theta_0)) \geq c_0 \lambda_1$$

where $\lambda_1 = \lambdaMin(\E[\text{expected kernel}])$ and the required width is $\TildeOmega(n)$.
\end{theorem}

\subsection{Effective Rank and Condition Number}

\begin{theorem}[NTK Effective Rank Bound]
Suppose $\phi(x) = \text{ReLU}(x)$ and $m \geq d$. Then with probability at least $1 - 3\epsilon$:
$$\frac{\text{Tr}(\mathbf{K}_{\text{outer}})}{\lambda_1(\mathbf{K}_{\text{outer}})} \leq 12 \left(\frac{\sqrt{m} + \sqrt{d} + t_1}{\sqrt{m} - \sqrt{d} - t_1}\right)^2 \frac{\text{Tr}(X^T X)}{\lambda_1(X^T X)}$$
\end{theorem}

\textbf{Consequence}: The NTK has only $O(d)$ large eigenvalues when $m \gtrsim n \gg d$.


\newpage



\section{Open Questions and Future Directions}

\subsection{Remaining Challenges}
\begin{enumerate}
    \item \textbf{Very different assumptions}: All of the papers assumes differently (domains, init, architecture, activations), I'll try to find a way to unify them
    \item \textbf{Out of the sphere}: Harmonic analysis (integral operator) over $\L(2,\gamma)$ is frequently discussed as a future research direction (not even explored yet)
    \item \textbf{Optimal Width Scaling}: Only needed 1 linear width, getting linear scaling for $\lambdaMin$
    \item \textbf{Broad Activation Coverage}: From ReLU to inhomogeneous activations with harmonic analysis over $\L(2,\gamma)$
    \item \textbf{Sobolev Framework}: We multiply the NTK by P (spectral bias) and analyse this new spectrum with those techniques
    \item \textbf{Deep Narrow Frameworks}: We use the NTK given by a deep narrow network
\end{enumerate}

\newpage

\section*{References}

\begin{enumerate}
    \item Banerjee, A., Cisneros-Velarde, P., Zhu, L., Belkin, M. (2023). Neural Tangent Kernel at Initialization: Linear Width Suffices. UAI 2023.
    
    \item Nguyen, Q., Mondelli, M., Montúfar, G. (2021). Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks. ICML 2021.
    
    \item Saratchandran, H., Chng, S.-F., Lucey, S. (2024). Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate Networks. arXiv:2402.04783v1.
    
    \item Chang, S.Y. (2022). Generalized Hanson-Wright Inequality for Random Tensors. arXiv:2203.00659.
    
    \item Tropp, J.A. (2012). User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics.
    
    \item Vershynin, R. (2018). High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press.
\end{enumerate}

\end{document}
