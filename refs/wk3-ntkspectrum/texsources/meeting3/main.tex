\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{tocloft}
\usepackage{tikz}
% Page setup
\geometry{a4paper, margin=2.5cm}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{NTK Eigenvalue Bounds: Comprehensive Analysis}
\fancyhead[R]{\thepage}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}

% Mathematical commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\evmin}[1]{\lambda_{\min}\left(#1\right)}
\newcommand{\evmax}[1]{\lambda_{\max}\left(#1\right)}
\newcommand{\svmin}[1]{\sigma_{\min}\left(#1\right)}
\newcommand{\tr}{\text{tr}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\u}{\mathbf{u}}
\newcommand{\v}{\mathbf{v}}
\newcommand{\odot}{\odot}
\newcommand{\bigOmg}{\Omega}
\newcommand{\bigTheta}{\Theta}
\newcommand{\Pr}{\text{Pr}}
\newcommand{\KNTK}{K_{\text{NTK}}}
\newcommand{\KLim}{K^{(L)}}
\newcommand{\KEmp}{\overline{K}^{(L)}}
\newcommand{\KCosine}{K_L}
\newcommand{\lambdaMin}{\lambda_{\min}}
\newcommand{\sigmaMin}{\sigma_{\min}}
\newcommand{\Order}{\mathcal{O}}
\newcommand{\TildeOrder}{\tilde{\mathcal{O}}}
\newcommand{\OmegaNotation}{\Omega}
\newcommand{\TildeOmega}{\tilde{\Omega}}
\newcommand{\cN}{\mathcal{N}}

\title{Comprehensive Analysis of Neural Tangent Kernel Eigenvalue Bounds:\\
Mathematical Foundations, Proof Techniques, and Concentration Inequalities}

\author{Synthesis from Banerjee et al., Nguyen et al., and Related Works}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
%This document provides an exhaustive compilation and analysis of mathematical techniques for bounding the smallest eigenvalue $\lambdaMin(\KNTK)$ of the Neural Tangent Kernel. We synthesize results from three complementary perspectives: (1) concentration inequalities and their applications to NTK analysis, (2) timeline-based evolution of proof techniques from classical to modern approaches, and (3) detailed lemma-by-lemma analysis of fundamental results. The document covers the progression from classical Hermite polynomial methods requiring $\TildeOmega(n^2)$ width to modern generalized Hermite techniques achieving $\TildeOmega(n)$ width, along with empirical NTK decomposition methods and spectral analysis via power series. Key mathematical tools include matrix concentration inequalities (Chernoff, Bernstein, Hanson-Wright), matrix analysis techniques (Weyl, Schur, Gershgorin), and specialized tools for neural network analysis (Khatri-Rao products, generalized Hermite polynomials).
\end{abstract}

\tableofcontents
\newpage

\section{Introduction: The Fundamental Problem}

The Neural Tangent Kernel (NTK) has emerged as a central object in the theoretical analysis of deep neural networks. For a neural network $f(\x; \theta)$ with parameters $\theta$, the NTK is defined as:
$$\KNTK(x_i, x_j) = \left\langle \frac{\partial f(\x_i; \theta)}{\partial \theta}, \frac{\partial f(\x_j; \theta)}{\partial \theta} \right\rangle$$

In matrix form, $\KNTK = JJ^T$ where $J$ is the Jacobian matrix of the network outputs with respect to parameters. The smallest eigenvalue $\lambdaMin(\KNTK)$ plays a crucial role in:


The central question addressed in this document is: \textit{Under what conditions can we guarantee $\lambdaMin(\KNTK) \geq c > 0$ with high probability?}


\section{What the SOTA gives us}

\begin{itemize}
    \item \textbf{Banerjee et al. (2023)}: 1 linear width layer suffices for ReLU networks to get confident bounds on $\lambdaMin$
    \item \textbf{Nguyen et al. (2021)}: Very tight linear bound
    \item \textbf{Saratchandran et al. (2024)}: Periodic activations (cosine)
    \item \textbf{Tarjek (2022)}: linear scaling of $\lambdaMin$ for MLP at the EOCs
\end{itemize}


What is behind everything is the kernel integral operator $K$ and its spectrum.

\newpage

\section{Mathematical Foundation: Concentration Inequalities}

\subsection{Scalar Concentration Inequalities}

The foundation of modern NTK analysis rests on powerful concentration inequalities that control the deviation of random variables from their expectations.

\begin{theorem}[Chernoff Bound for Bounded Random Variables]
Let $X_1, \ldots, X_n$ be independent random variables with $X_i \in [0,1]$ and $\E[X_i] = \mu_i$. Let $S = \sum_{i=1}^n X_i$ and $\mu = \E[S] = \sum_{i=1}^n \mu_i$. Then:
$$\Pr(S \geq (1+\delta)\mu) \leq e^{-\frac{\delta^2 \mu}{2+\delta}}$$
$$\Pr(S \leq (1-\delta)\mu) \leq e^{-\frac{\delta^2 \mu}{2}}$$
for $\delta > 0$.
\end{theorem}

\begin{theorem}[Bernstein's Inequality]
Let $X_1, \ldots, X_n$ be independent random variables with $\E[X_i] = 0$, $|X_i| \leq M$, and $\E[X_i^2] \leq \sigma_i^2$. Let $S = \sum_{i=1}^n X_i$ and $\sigma^2 = \sum_{i=1}^n \sigma_i^2$. Then:
$$\Pr(|S| \geq t) \leq 2\exp\left(-\frac{t^2/2}{\sigma^2 + Mt/3}\right)$$
\end{theorem}

\subsection{Quadratic Forms and Hanson-Wright Inequalities}

For neural networks, we frequently encounter quadratic forms in random variables, necessitating more sophisticated concentration tools.

\begin{theorem}[Classical Hanson-Wright Inequality]
Let $Y_1, \ldots, Y_n$ be independent mean-zero $\alpha$-subgaussian random variables, and $\mathbf{A} = (a_{i,j})$ be a symmetric matrix. Then:
$$\Pr\left(\left|\sum_{i,j=1}^n a_{i,j}(Y_i Y_j - \E[Y_i Y_j])\right| \geq t\right) \leq 2\exp\left(-\frac{1}{C}\min\left\{\frac{t^2}{\beta^4\|\mathbf{A}\|_{HS}^2}, \frac{t}{\beta^2\|\mathbf{A}\|_{op}}\right\}\right)$$
\end{theorem}

where $\|\mathbf{A}\|_{HS} = \sqrt{\sum_{i,j} |a_{i,j}|^2}$ is the Hilbert-Schmidt norm and $\|\mathbf{A}\|_{op}$ is the operator norm.

\begin{theorem}[Generalized Hanson-Wright for Random Tensors (Chang, 2022)]
For random tensor vector $\overline{\mathcal{X}} \in \C^{(n \times I_1 \times \cdots \times I_M) \times (I_1 \times \cdots \times I_M)}$ and fixed tensor $\overline{\overline{\mathcal{A}}}$, the polynomial function:
$$f_j(\overline{\mathcal{X}}) = \left(\sum_{i,k=1}^n \mathcal{A}_{i,k} \star_M (\mathcal{X}_i - \E[\mathcal{X}_i]) \star_M (\mathcal{X}_k - \E[\mathcal{X}_k])\right)^j$$
satisfies concentration bounds involving Ky Fan $k$-norms of tensor sums, extending classical matrix concentration to tensor settings.
\end{theorem}

\subsection{Matrix Concentration Inequalities}

Matrix concentration inequalities provide direct control over eigenvalues of random matrix sums, which is essential for NTK analysis.

\begin{theorem}[Matrix Chernoff Bound (Tropp, 2012)]
Let $X_1, \ldots, X_n$ be independent random Hermitian matrices with $X_i \preceq R \cdot I$ and $\E[X_i] \preceq \mu \cdot I$. Let $S = \sum_{i=1}^n X_i$. Then:
$$\Pr(\lambda_{\max}(S) \geq (1+\delta)\mu n) \leq d \left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^{\mu n / R}$$
\end{theorem}

\begin{theorem}[Matrix Bernstein Inequality (Vershynin, 2018)]
Let $X_1, \ldots, X_n$ be independent random matrices with $\E[X_i] = 0$, $\|X_i\| \leq L$, and $\left\|\sum_{i=1}^n \E[X_i X_i^*]\right\| \leq \sigma^2$. Then:
$$\Pr\left(\left\|\sum_{i=1}^n X_i\right\| \geq t\right) \leq 2d \exp\left(-\frac{t^2/2}{\sigma^2 + Lt/3}\right)$$
\end{theorem}

\section{Fundamental Mathematical Tools}

\subsection{Matrix Analysis Techniques}

\begin{lemma}[Weyl's Inequality]
For Hermitian matrices $A, B$:
$$\lambda_i(A) + \lambda_n(B) \leq \lambda_i(A + B) \leq \lambda_i(A) + \lambda_1(B)$$
\end{lemma}

\begin{lemma}[Schur Product Theorem]
For PSD matrices $P, Q$:
$$\evmin{P \circ Q} \geq \evmin{P} \min_i Q_{ii}$$
\end{lemma}

\begin{lemma}[Gershgorin Circle Theorem]
For matrix $A = (a_{ij})$, all eigenvalues lie in the union of discs:
$$\bigcup_{i=1}^n \left\{z \in \C : |z - a_{ii}| \leq \sum_{j \neq i} |a_{ij}|\right\}$$
\end{lemma}

\subsection{Specialized Neural Network Tools}

\begin{definition}[Khatri-Rao Product]
For matrices $A \in \R^{m \times k}$ and $B \in \R^{n \times k}$, the Khatri-Rao product $A \star B \in \R^{mn \times k}$ is the column-wise Kronecker product.
\end{definition}

\begin{definition}[Hadamard Power]
For matrix $A$ and positive integer $r$, the $r$-th Hadamard power $A^{\odot r}$ is the element-wise $r$-th power.
\end{definition}

\newpage

\section{Proof Schemes and High-Level Strategies}

\subsection{General Framework for Eigenvalue Bounds}

All modern approaches to bounding $\lambdaMin(\KNTK)$ follow a common high-level structure, though they differ in technical details and specific tools employed.

\begin{figure}[h]
\centering
\begin{tikzpicture}
    % Define styles
    \tikzset{
        box/.style={rectangle, draw, thick, minimum width=3cm, minimum height=1.2cm, text centered, fill=blue!15},
        arrow/.style={->, thick, blue!70}
    }

    % Main boxes
    \node[box] (step1) at (0,4) {\textbf{Step 1} \\ Problem Decomposition};
    \node[box] (step2) at (0,2) {\textbf{Step 2} \\ Concentration Analysis};
    \node[box] (step3) at (0,0) {\textbf{Step 3} \\ Spectral Analysis};

    % Arrows
    \draw[arrow] (step1) -- (step2);
    \draw[arrow] (step2) -- (step3);

    % Annotations
    \node[align=left, text width=4cm] at (4.5,4) {
        $\bullet$ Decompose NTK \\
        $\bullet$ Identify key terms \\
        $\bullet$ Separate components
    };
    
    \node[align=left, text width=4cm] at (4.5,2) {
        $\bullet$ Apply matrix bounds \\
        $\bullet$ Control randomness \\
        $\bullet$ High-probability results
    };
    
    \node[align=left, text width=4cm] at (4.5,0) {
        $\bullet$ Matrix inequalities \\
        $\bullet$ Combine bounds \\
        $\bullet$ Final $\lambda_{\min}$ result
    };

\end{tikzpicture}
\caption{General Framework for NTK Eigenvalue Bounds}
\end{figure}


\newpage

\subsection{Proof Scheme 1: Empirical NTK Decomposition (Nguyen et al.)}

\textbf{High-Level Strategy}: Direct decomposition of empirical NTK $\bar{K}^{(L)} = JJ^T$ using feature matrices and chain rule to obtain precise bounds on $\lambdaMin$.

\subsubsection{Step 1: Fundamental Chain Rule Decomposition}

The proof begins by decomposing the empirical NTK $\bar{K}^{(L)} = JJ^T$ where $J$ is the Jacobian matrix \eqref{eq:Jac}. Applying the chain rule and standard algebraic manipulations yields the fundamental decomposition:

\begin{align}
\bar{K}^{(L)} = JJ^T = \sum_{k=0}^{L-1} F_k F_k^T \circ B_{k+1} B_{k+1}^T
\end{align}

where:
\begin{itemize}
    \item $F_k = [f_k(x_1), \ldots, f_k(x_N)]^T \in \R^{N \times n_k}$ are the \textbf{feature matrices} at layer $k$
    \item $B_k \in \R^{N \times n_k}$ are the \textbf{derivative term matrices} with $i$-th row given by:
\end{itemize}

\begin{align}
(B_k)_{i:} = \begin{cases}
\Sigma_k(x_i) \left(\prod_{l=k+1}^{L-1} W_l \Sigma_l(x_i)\right) W_L, & k \in [L-2] \\
\Sigma_{L-1}(x_i) W_L, & k = L-1 \\
\frac{1}{\sqrt{N}} \mathbf{1}_N, & k = L
\end{cases}
\end{align}

where $\Sigma_k(x) = \text{diag}([\sigma'(g_{k,j}(x))]_{j=1}^{n_k})$ is the diagonal matrix of activation derivatives.

\subsubsection{Step 2: Application of Schur Product Theorem and Weyl's Inequality}

For PSD matrices $P, Q \in \R^{n \times n}$, the Schur product theorem :
\begin{align}
\evmin{P \circ Q} \geq \evmin{P} \min_{i \in [n]} Q_{ii}
\end{align}

Applying this to each term, then using Weyl's inequality for the sum:

\begin{align}
\evmin{JJ^T} &\geq \sum_{k=0}^{L-1} \evmin{F_k F_k^T \circ B_{k+1} B_{k+1}^T} \\
&\geq \sum_{k=0}^{L-1} \evmin{F_k F_k^T} \min_{i \in [N]} \|(B_{k+1})_{i:}\|_2^2
\end{align}

This reduction separates the problem into:
\begin{enumerate}
    \item The \textbf{minimum eigenvalues of feature Gram matrices}: $\evmin{F_k F_k^T}$
    \item The \textbf{minimum row norms of derivative matrices}: $\min_{i \in [N]} \|(B_{k+1})_{i:}\|_2^2$
\end{enumerate}

\subsubsection{Step 3: Bounding Feature Matrix Eigenvalues}

To bound $\evmin{F_k F_k^T}$, the strategy uses Chernoff-type matrix concentration. The fundamental bound is given by:
\begin{lemma}[Matrix Concentration for Features]
Let $\lambda = \evmin{\E_{w \sim \mathcal{N}(0, \beta_k^2 \mathbf{I}_{n_{k-1}})}[\sigma(F_{k-1}w)\sigma(F_{k-1}w)^T]}$. If 
\begin{align}
n_k \geq \max\left(N, c Q \max(1, \log(4Q)) \log\frac{N}{\delta}\right)
\end{align}
where $Q = \frac{\beta_k^2 \|F_{k-1}\|_F^2}{\lambda}$, then with probability at least $1-\delta$:
\begin{align}
\svmin{F_k}^2 \geq \frac{n_k \lambda}{4}
\end{align}

This lemma assumes that:
\begin{itemize}
\item The width $n_k$ of layer $k$ is sufficiently large compared to the number of samples $N$
\item The feature matrix $F_{k-1}$ from the previous layer has bounded Frobenius norm
\end{itemize}
These conditions ensure concentration of the minimum singular value of the feature matrix around its expected value.
\end{lemma}

\subsubsection{Step 4: Spectral Analysis via Hermite Expansion}

To bound $\lambda$, we use the Hermite expansion of the ReLU. and exploiting the homogeneity of $\sigma$ (unless, generalized hermite):

\begin{align}
\lambda &= \evmin{\E[\sigma(F_{k-1}w)\sigma(F_{k-1}w)^T]} \\
&= \evmin{D \E[\sigma(\hat{F}_{k-1}w)\sigma(\hat{F}_{k-1}w)^T] D}
\end{align}

where $D = \text{diag}(\|\{F_{k-1}\}_{i:}\|_2)$ and $\hat{F}_{k-1} = D^{-1}F_{k-1}$.

Applying the Hermite expansion $\sigma(x) = \sum_{r=0}^{\infty} \mu_r(\sigma) H_r(x)$:

\begin{align}
\E[\sigma(\hat{F}_{k-1}w)\sigma(\hat{F}_{k-1}w)^T] = \mu_0(\sigma)^2 \mathbf{1}_N \mathbf{1}_N^T + \sum_{s=1}^{\infty} \mu_s(\sigma)^2 (\hat{F}_{k-1}^{*s})(\hat{F}_{k-1}^{*s})^T
\end{align}

where $\hat{F}_{k-1}^{*s}$ represents the $s$-th Khatri-Rao power of $\hat{F}_{k-1}$.

\subsubsection{Step 5: Reduction to Khatri-Rao Powers}

For an integer $r \geq 2$, we can lower bound:
\begin{align}
\lambda \geq \mu_r(\sigma)^2 \evmin{D(\hat{F}_{k-1}^{*r})(\hat{F}_{k-1}^{*r})^T D} = \mu_r(\sigma)^2 \frac{\evmin{(F_{k-1}^{*r})(F_{k-1}^{*r})^T}}{\max_{i \in [N]} \|(F_{k-1})_{i:}\|_2^{2(r-1)}}
\end{align}

\subsubsection{Step 6: Feature Centering and Hadamard Power Analysis}

To analyze $(F_{k-1}^{*r})(F_{k-1}^{*r})^T = (F_{k-1}F_{k-1}^T)^{\circ r}$, we introduce centered features $\tilde{F}_{k-1} = F_{k-1} - \E_X[F_{k-1}]$. 

Let $\mu = \E_x[f_{k-1}(x)] \in \R^{n_{k-1}}$ and $\Lambda = \text{diag}(F_{k-1}\mu - \|\mu\|_2^2 \mathbf{1}_N)$. Then:

\begin{align}
(F_{k-1}^{*r})(F_{k-1}^{*r})^T = (F_{k-1}F_{k-1}^T)^{\circ r} \succeq \left(\tilde{F}_{k-1}\tilde{F}_{k-1}^T - \frac{\Lambda \mathbf{1}_N \mathbf{1}_N^T \Lambda}{\|\mu\|_2^2}\right)^{\circ r}
\end{align}

\subsubsection{Step 7: Application of Gershgorin Circle Theorem}

To bound the minimum eigenvalue of $(\tilde{F}_{k-1}\tilde{F}_{k-1}^T)^{\circ r}$, we apply the Gershgorin circle theorem:

\begin{align}
\evmin{(\tilde{F}_{k-1}^{*r})(\tilde{F}_{k-1}^{*r})^T} &\geq \min_{i \in [N]} \|(\tilde{F}_{k-1})_{i:}\|_2^{2r} - N \max_{i \neq j} |\langle (\tilde{F}_{k-1})_{i:}, (\tilde{F}_{k-1})_{j:} \rangle|^r
\end{align}

Using concentration properties of centered features, we show:
\begin{align}
\|(\tilde{F}_{k-1})_{i:}\|_2^{2r} &= \Theta\left(\left(d \prod_{l=1}^{k-1} n_l \beta_l^2\right)^r\right) \\
N \max_{i \neq j} |\langle (\tilde{F}_{k-1})_{i:}, (\tilde{F}_{k-1})_{j:} \rangle|^r &= o\left(\left(d \prod_{l=1}^{k-1} n_l \beta_l^2\right)^r\right)
\end{align}

with exponentially high probability.

\subsubsection{Step 8: Bounding Derivative Term Norms, very technical}

To bound $\min_{i \in [N]} \|(B_{k+1})_{i:}\|_2^2$, we analyze each case:

For $k \in [L-2]$:
\begin{align}
\|(B_{k+1})_{i:}\|_2^2 &= \|\Sigma_{k+1}(x_i) \left(\prod_{l=k+2}^{L-1} W_l \Sigma_l(x_i)\right) W_L\|_2^2 \\
&= \Theta\left(\beta_L^2 n_{k+1} \prod_{l=k+2}^{L-1} n_l \beta_l^2\right)
\end{align}

This bound uses some very technical and important tools:
\begin{itemize}
    \item \textbf{Operator norm for  concentration of Gaussian matrices}
    \item \textbf{Inductive analysis of random matrix products for every layer}
    \item \textbf{Properties of derivative matrices $\Sigma_l(x)$ for every layer}
\end{itemize}

\subsubsection{Step 9: Final Assembly and Width Optimization}

Combining all bounds via union bound yields:

\begin{align}
\evmin{\bar{K}^{(L)}} &\geq \sum_{k=0}^{L-1} \mu_r(\sigma)^2 \Theta\left(d \prod_{l=1}^k n_l \beta_l^2\right) \Theta\left(\beta_L^2 \prod_{l=k+1}^{L-1} n_l \beta_l^2\right) \\
&= \mu_r(\sigma)^2 \Theta\left(d \beta_L^2 \sum_{k=0}^{L-1} \prod_{l=1}^{L-1} n_l \beta_l^2\right)
\end{align}

The width condition requires at least one layer to satisfy:
\begin{align}
n_k = \tilde{\Omega}(N \log N), \quad \text{with } \xi_k = 1
\end{align}

where $\xi_k$ indicates layer $k$ is "wide".

\subsubsection{Upper Bound via Diagonal Analysis}

For the upper bound, we use:
\begin{align}
\evmin{JJ^T} &\leq (JJ^T)_{11} = \sum_{k=0}^{L-1} \|(F_k)_{1:}\|_2^2 \|(B_{k+1})_{1:}\|_2^2 \\
&= \sum_{k=0}^{L-1} \|f_k(x_1)\|_2^2 \|(B_{k+1})_{1:}\|_2^2 \\
&= \mathcal{O}\left(d \beta_L^2 \sum_{k=0}^{L-1} \prod_{l=1}^{L-1} n_l \beta_l^2\right)
\end{align}

\subsubsection{Final Result}

Assuming $\beta_l = 1$ for simplicity and that some layer $k^*$ satisfies $n_{k^*} = \tilde{\Omega}(N)$, the main theorem gives:

\begin{align}
\mu_r(\sigma)^2 \Omega(d) \leq \evmin{\bar{K}^{(L)}} \leq \mathcal{O}(d L)
\end{align}

with probability at least $1 - \text{poly}(N) \exp(-\Omega(\min_l n_l))$.

This approach significantly reduces width requirements compared to classical methods, requiring only one wide layer rather than all layers.

\subsection{Proof Scheme 2: Kernel Integral Operator via Integral Transform (Karhadkar et al.)}

\textbf{Key Inequalities Chain}:
\begin{align}
    \lambda_{\min}(\mathbf{K}_{\psi}^{\infty}) &= \inf_{\|\z\|=1} \|T_{\psi}\mu_{\z}\|_{L^2(\mathbb{S}^{d-1})}^2 \\
    &\geq \sum_{a=1}^N \kappa_a^2 \left|\sum_{i=1}^n g_a(\x_i) z_i\right|^2 \\
    &\geq \min_a \kappa_a^2 \sigma_{\min}^2(\mathbf{D}) \\
    &\geq \min_a \kappa_a^2 \frac{N}{2} \quad \text{(for } N \geq C(\delta^4/2)^{-(d-2)/2}\text{)} \\
    &= \tilde{\Omega}(d_0^{-3}\delta^2) \quad \text{(shallow nets)} \\
    &= \tilde{\Omega}(d_0^{-3}\delta^4) \quad \text{(deep nets)}
\end{align}

\textbf{High-Level Strategy}: Novel approach using integral transforms and spherical harmonics to analyze the NTK for arbitrary spherical data without distributional assumptions or dimension scaling requirements.

\subsubsection{Step 1: Spherical Data Setup and $\delta$-Separation}

The fundamental innovation is analyzing data $\{\x_i\}_{i=1}^n \subset \mathbb{S}^{d_0-1}$ with the $\delta$-separation property:
\begin{align}
    \min_{i \neq k} \min(\|\x_i - \x_k\|, \|\x_i + \x_k\|) \geq \delta
\end{align}

This measures collinearity without requiring distributional assumptions. Key advantages:
\begin{itemize}
    \item No requirement that $d_0$ scales with $n$ (works for constant $d_0$)
    \item No Lipschitz concentration assumptions on data distribution
    \item Direct geometric control via separation parameter $\delta$
\end{itemize}

\subsubsection{Step 2: Infinite-Width Kernel Representation}

For $\psi \in \{\sqrt{d}\sigma, \dot{\sigma}\}$, the limiting NTK becomes:
\begin{align*}
    \mathbf{K}^{\infty}_{\psi} &= \mathbb{E}_{\mathbf{u} \sim U(\mathbb{S}^{d-1})}[\psi(\mathbf{X}^T\mathbf{u})\psi(\mathbf{u}^T\mathbf{X})]
\end{align*}

This reformulation enables the integral transform approach by treating the kernel as an integral over the sphere.

\subsubsection{Step 3: Integral Transform Construction}

For a signed Radon measure $\mu \in \mathcal{M}(\mathbb{S}^{d-1})$, define the integral transform:
\begin{align*}
    (T_{\psi}\mu)(\u) &= \int_{\mathbb{S}^{d-1}} \psi(\langle \u, \x \rangle) d\mu(\x)
\end{align*}

\textbf{Key Insight}: For data-supported measures $\mu_{\z} = \sum_{i=1}^n z_i \delta_{\x_i}$:
\begin{align}
    \lambda_{\min}(\mathbf{K}_{\psi}^{\infty}) &= \inf_{\|\z\|=1} \|T_{\psi}\mu_{\z}\|_{L^2(\mathbb{S}^{d-1})}^2
\end{align}

This reduces eigenvalue analysis to studying integral transform norms.

\subsubsection{Step 4: Spherical Harmonics Decomposition}

Using the orthogonal decomposition $L^2(\mathbb{S}^{d-1}) = \bigoplus_{r=0}^{\infty} \mathcal{H}_r^d$ where $\mathcal{H}_r^d$ are degree-$r$ harmonic homogeneous polynomials:

\begin{align}
    \|T_{\psi}\mu_{\z}\|^2 &\geq \sum_{a=1}^N \kappa_a^2 \left|\sum_{i=1}^n g_a(\x_i) z_i\right|^2 \\
    &\geq \min_a \kappa_a^2 \sigma_{\min}^2(\mathbf{D})
\end{align}

where:
\begin{itemize}
    \item $g_a$ are spherical harmonic eigenfunctions: $T_{\psi} g_a = \kappa_a g_a$
    \item $\mathbf{D} \in \mathbb{R}^{N \times n}$ with $[\mathbf{D}]_{ai} = g_a(\x_i)$
    \item $\kappa_a$ are eigenvalues of the integral transform operator
\end{itemize}

\subsubsection{Step 5: Addition Formula and Matrix Analysis}

The spherical harmonics addition formula provides:
\begin{align}
    \sum_{s=1}^{\dim(\mathcal{H}_r^d)} Y_{r,s}^d(\x) Y_{r,s}^d(\x') &= \frac{(2r + d - 2)C_r^{(d-2)/2}(\langle \x, \x' \rangle)}{d - 2}
\end{align}

For $\delta$-separated data, this gives the crucial bound:
\begin{align}
    \left|\sum_{r=0}^R \sum_{s=1}^{\dim(\mathcal{H}_{2r+\beta}^d)} Y_{2r+\beta,s}^d(\x_i) Y_{2r+\beta,s}^d(\x_j)\right| &\lesssim \left(\frac{\delta^2}{2}\right)^{-(d-2)/4} \binom{2R + \beta + d - 1}{d - 1}^{1/2}
\end{align}

\subsubsection{Step 6: Singular Value Control via Separation}

The key lemma emerges from controlling the condition number of the spherical harmonic evaluation matrix $\mathbf{D}$ using the $\delta$-separation property to ensure diagonal dominance over off-diagonal cross-terms.

\begin{lemma}[Spherical Harmonic Matrix Conditioning]
Let $\mathbf{D} \in \mathbb{R}^{N \times n}$ with $[\mathbf{D}]_{ai} = g_a(\x_i)$ where $\{g_a\}$ are orthonormal spherical harmonic eigenfunctions and $\{\x_i\}$ are $\delta$-separated on $\mathbb{S}^{d_0-1}$. If $N = \sum_{r=0}^R \dim(\mathcal{H}_{2r+\beta}^d) \geq C(\delta^4/2)^{-(d-2)/2}$ for sufficiently large constant $C$, then $\sigma_{\min}(\mathbf{D}) \geq \sqrt{N/2}$.
\end{lemma}

\textbf{Key Lemma}: If $N = \sum_{r=0}^R \dim(\mathcal{H}_{2r+\beta}^d) \geq C(\delta^4/2)^{-(d-2)/2}$, then:
\begin{align}
    \sigma_{\min}(\mathbf{D}) &\geq \sqrt{\frac{N}{2}}
\end{align}

\subsubsection{Step 7: Integral Transform Eigenvalue Analysis}

Using the Funk-Hecke formula, the integral transform eigenvalues are:
\begin{align}
    \kappa_r &= \int_{-1}^1 \psi(t) C_r^{(d-2)/2}(t) (1-t^2)^{(d-3)/2} dt
\end{align}

For ReLU activations, these integrals have explicit expressions involving Gegenbauer polynomials.

\subsubsection{Step 8: Final Bounds Assembly}

Combining all components yields the main result:

\textbf{For shallow networks}: If $d_1 = \tilde{\Omega}(\|\mathbf{X}\|^2 d_0^3 \delta^{-2})$, then:
\begin{align}
    \lambda_{\min}(\mathbf{K}) &= \tilde{\Omega}(d_0^{-3}\delta^2)
\end{align}

\textbf{For deep networks}: With pyramidal widths $d_l \geq d_{l+1}$ and $d_1 = \tilde{\Omega}(n d_0^3 \delta^{-4})$:
\begin{align}
    \lambda_{\min}(\mathbf{K}) &= \tilde{\Omega}(d_0^{-3}\delta^4)
\end{align}

\textbf{Revolutionary Impact}: This approach completely removes the requirement that input dimension $d_0$ scales with sample size $n$, enabling analysis of fixed-dimensional data with arbitrary distributions on the sphere.

\subsection{Comparative Analysis of Proof Strategies}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Approach} & \textbf{Key Innovation} & \textbf{Main Tool} & \textbf{Strength} \\
\hline
Generalized Hermite & Variable variance & Matrix Chernoff & Optimal width \\
\hline
NTK Decomposition & Feature matrices & Schur + Weyl & Direct analysis \\
\hline
Power Series & Complete spectrum & Effective rank & Full characterization \\
\hline
\end{tabular}
\end{table}


























\subsection{Proof Scheme 3: Spectral Analysis via Power Series}

\textbf{Theorem} (Power Series Decomposition): The Neural Tangent Kernel admits a power series representation:
\begin{align}
K_{\text{NTK}}(\langle \mathbf{x}_i, \mathbf{x}_j \rangle) = \sum_{p=0}^{\infty} a_p \langle \mathbf{x}_i, \mathbf{x}_j \rangle^p
\end{align}
where coefficients $a_p = \sum_{k=1}^L F(p, k, \bar{\mu})$ are derived from Hermite polynomial expansions.

\textbf{Key Steps in Analysis:}

1. \textbf{Coefficient Properties}
   \begin{align}
   |a_p| &\leq C\rho^p, \quad \rho < 1 \\
   F(p,k,\bar{\mu}) &= \mathbb{E}_{z \sim \mathcal{N}(0,\bar{\mu})}[H_p(z)]
   \end{align}

2. \textbf{Finite Truncation}
   \begin{align}
   K_m = \sum_{j=0}^{m-1} c_j G^{\odot j}
   \end{align}
   where $G_{ij} = \langle \mathbf{x}_i, \mathbf{x}_j \rangle$ and $\odot$ denotes Hadamard product.

3. \textbf{Spectral Bounds}
   \begin{align}
   \text{rank}(K_m) &\leq R(m,n,d) \\
   \lambda_{\min}(K) &\geq \sum_{p=0}^m a_p \lambda_{\min}(G^{\odot p}) - \epsilon_m
   \end{align}
   where $\epsilon_m$ is the truncation error.

4. \textbf{Eigenvalue Distribution}
   \begin{itemize}
   \item Large eigenvalues: $\lambda_i = \Theta(n)$ for $i \leq d$
   \item Bulk spectrum: $\lambda_i = O(\sqrt{n})$ for $i > d$
   \item Spectral gap: $\Delta = \lambda_d - \lambda_{d+1} = \Omega(\sqrt{n})$
   \end{itemize}

5. \textbf{Condition Number}
   When $m \gg n \gg d$:
   \begin{align}
   \kappa(K) = \frac{\lambda_{\max}}{\lambda_{\min}} = O\left(\frac{n}{\sqrt{n}}\right) = O(\sqrt{n})
   \end{align}

\textbf{Corollary}: The NTK matrix is well-conditioned with high probability when the network width satisfies:
\begin{align}
d_1 \geq \Omega(n \log n)
\end{align}













\subsection{Overview of Proof Schemes and Their Main Tools}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    scheme/.style={draw, rounded corners, fill=blue!15, minimum width=4.5cm, minimum height=3.5cm, text width=4.3cm, align=center, thick},
    tools/.style={draw, rounded corners, fill=green!10, minimum width=4.5cm, minimum height=2cm, text width=4.3cm, align=center},
    arrow/.style={->, thick, >=latex, blue!70}
]

% Main scheme boxes
\node[scheme] (scheme1) at (0,0) {
    \textbf{\Large Scheme 1} \\
    \textbf{NTK Decomposition} \\
    \small{(Nguyen et al.)} \\[0.3cm]
    \textit{Strategy:} Direct $JJ^T$ \\
    matrix decomposition \\
    via chain rule
};

\node[scheme] (scheme2) at (6,0) {
    \textbf{\Large Scheme 2} \\
    \textbf{Integral Transform} \\
    \small{(Karhadkar et al.)} \\[0.3cm]
    \textit{Strategy:} Spherical \\
    harmonic analysis \\
    via integral operators
};

\node[scheme] (scheme3) at (12,0) {
    \textbf{\Large Scheme 3} \\
    \textbf{Power Series} \\
    \small{(Spectral Analysis)} \\[0.3cm]
    \textit{Strategy:} Complete \\
    spectrum analysis \\
    via Hermite expansion
};

% Tool boxes
\node[tools] (tools1) at (0,-4.5) {
    \textbf{Main Tools:} \\
    • Schur Product Theorem \\
    • Weyl's Inequality \\
    • Matrix Chernoff Bounds \\
    • Feature Matrix Analysis
};

\node[tools] (tools2) at (6,-4.5) {
    \textbf{Main Tools:} \\
    • Spherical Harmonics \\
    • Funk-Hecke Formula \\
    • $\delta$-Separation \\
    • Addition Formula
};

\node[tools] (tools3) at (12,-4.5) {
    \textbf{Main Tools:} \\
    • Hermite Polynomials \\
    • Power Series \\
    • Effective Rank \\
    • Coefficient Analysis
};

% Arrows
\draw[arrow] (scheme1) -- (tools1);
\draw[arrow] (scheme2) -- (tools2);
\draw[arrow] (scheme3) -- (tools3);

% Key advantages boxes
\node[draw, rounded corners, fill=yellow!10, minimum width=4.5cm, minimum height=1.5cm, text width=4.3cm, align=center] (adv1) at (0,-7) {
    \textbf{Key Advantage:} \\
    Only 1 linear layer needed
};

\node[draw, rounded corners, fill=yellow!10, minimum width=4.5cm, minimum height=1.5cm, text width=4.3cm, align=center] (adv2) at (6,-7) {
    \textbf{Key Advantage:} \\
    No dimension scaling required
};

\node[draw, rounded corners, fill=yellow!10, minimum width=4.5cm, minimum height=1.5cm, text width=4.3cm, align=center] (adv3) at (12,-7) {
    \textbf{Key Advantage:} \\
    Complete spectral characterization
};

% Final arrows
\draw[arrow] (tools1) -- (adv1);
\draw[arrow] (tools2) -- (adv2);
\draw[arrow] (tools3) -- (adv3);

\end{tikzpicture}
\caption{High-Level Overview of NTK Eigenvalue Bound Proof Schemes}
\end{figure}

\textbf{Comparative Strengths:}
\begin{itemize}
    \item \textbf{Scheme 1} excels in practical width requirements - revolutionary reduction from $\tilde{\Omega}(n^2)$ to $\tilde{\Omega}(n)$
    \item \textbf{Scheme 2} removes distributional assumptions and dimension scaling - works for constant dimension
    \item \textbf{Scheme 3} provides the most complete theoretical understanding of the entire spectrum
\end{itemize}

\newpage
\section{Main Theorems and Final Bounds}

\subsection{Limiting NTK Results}

\begin{theorem}[Smallest Eigenvalue of Limiting NTK (Nguyen et al.)]
Let $\{x_i\}_{i=1}^{N}$ be a set of i.i.d. points from $P_X$ satisfying the distributional assumptions. For any even integer $r\ge 2$:
$$L\Order(d) \geq \evmin{K^{(L)}} \geq \mu_r(\sigma)^2\; \bigOmg(d)$$

with probability at least $1 - Ne^{-\bigOmg{d}} - N^2e^{-\bigOmg{dN^{-2/(r-0.5)}}}$
\end{theorem}

where $\mu_r(\sigma)$ is the $r$-th Hermite coefficient of the ReLU function.

\subsection{Empirical NTK Results}

\begin{theorem}[NTK at Initialization (Banerjee et al.)]
For smooth activations with appropriate width conditions:
$$\lambdaMin(\KNTK(\cdot;\theta_0)) \geq c_0 \lambda_1$$

where $\lambda_1 = \lambdaMin(\E[\text{expected kernel}])$ and the required width is $\TildeOmega(n)$.
\end{theorem}

\subsection{Effective Rank and Condition Number}

\begin{theorem}[NTK Effective Rank Bound]
Suppose $\phi(x) = \text{ReLU}(x)$ and $m \geq d$. Then with probability at least $1 - 3\epsilon$:
$$\frac{\text{Tr}(\mathbf{K}_{\text{outer}})}{\lambda_1(\mathbf{K}_{\text{outer}})} \leq 12 \left(\frac{\sqrt{m} + \sqrt{d} + t_1}{\sqrt{m} - \sqrt{d} - t_1}\right)^2 \frac{\text{Tr}(X^T X)}{\lambda_1(X^T X)}$$
\end{theorem}

\textbf{Consequence}: The NTK has only $O(d)$ large eigenvalues when $m \gtrsim n \gg d$.


\newpage



\section{Open Questions and Future Directions}

\subsection{Remaining Challenges}
\begin{enumerate}
    \item \textbf{Very different assumptions}: All of the papers assumes differently (domains, init, architecture, activations), I'll try to find a way to unify them
    \item \textbf{Out of the sphere}: Harmonic analysis (integral operator) over $\L(2,\gamma)$ is frequently discussed as a future research direction (not even explored yet)
    \item \textbf{Optimal Width Scaling}: Only needed 1 linear width, getting linear scaling for $\lambdaMin$
    \item \textbf{Broad Activation Coverage}: From ReLU to inhomogeneous activations with harmonic analysis over $\L(2,\gamma)$
    \item \textbf{Sobolev Framework}: We multiply the NTK by P (spectral bias) and analyse this new spectrum with those techniques
    \item \textbf{Deep Narrow Frameworks}: We use the NTK given by a deep narrow network
\end{enumerate}

\subsubsection{Roadmap}
\begin{itemize}
    \item Study behavior of narrow NTK to identify potential simplifications before developing general approach, and unify assumptions
    \item Incorporate Sobolev framework and analyze kernel matrix/operator changes in spherical framework, including eigenvalue modifications (with experiments)
    \item Extend harmonic analysis from sphere to entire space (with experiments)
    \item Apply narrow network NTK to complete framework
\end{itemize}



\newpage

\section*{References}

\begin{enumerate}
    \item Banerjee, A., Cisneros-Velarde, P., Zhu, L., Belkin, M. (2023). Neural Tangent Kernel at Initialization: Linear Width Suffices. UAI 2023.
    
    \item Nguyen, Q., Mondelli, M., Montúfar, G. (2021). Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks. ICML 2021.
    
    \item Saratchandran, H., Chng, S.-F., Lucey, S. (2024). Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate Networks. arXiv:2402.04783v1.
    
    \item Chang, S.Y. (2022). Generalized Hanson-Wright Inequality for Random Tensors. arXiv:2203.00659.
    
    \item Tropp, J.A. (2012). User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics.
    
    \item Vershynin, R. (2018). High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press.
\end{enumerate}

\end{document}
