\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{Aroraetal2019}
Sanjeev Arora, Simon~Shaolei Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Banerjee et~al.(2023)Banerjee, Cisneros-Velarde, Zhu, and
  Belkin]{Banerjeeetal2023}
Arindam Banerjee, Pedro Cisneros-Velarde, Libin Zhu, and Misha Belkin.
\newblock Neural tangent kernel at initialization: Linear width suffices.
\newblock In \emph{The 39th Conference on Uncertainty in Artificial
  Intelligence}, 2023.
\newblock URL \url{https://openreview.net/forum?id=VJaoe7Rp9tZ}.

\bibitem[Bombari et~al.(2022)Bombari, Amani, and Mondelli]{Bombarietal2022}
Simone Bombari, Mohammad~Hossein Amani, and Marco Mondelli.
\newblock Memorization and optimization in deep neural networks with minimum
  over-parameterization.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=x8DNliTBSYY}.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{Chizatetal2019}
L\'{e}na\"{\i}c Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf}.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{Danielyetal2016}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock 29, 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2016/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf}.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{Duetal2019}
Simon~S. Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  1675--1685. PMLR, 09--15 Jun 2019{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v97/du19c.html}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and Singh]{Duetal2018}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Hanin and Nica(2020)]{Haninetal2020}
Boris Hanin and Mihai Nica.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJgndT4KwB}.

\bibitem[Hayou et~al.(2019)Hayou, Doucet, and Rousseau]{Hayouetal2019}
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau.
\newblock On the impact of the activation function on deep neural networks
  training.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  2672--2680. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/hayou19a.html}.

\bibitem[Hayou et~al.(2022)Hayou, Doucet, and Rousseau]{Hayouetal2022}
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau.
\newblock The curse of depth in kernel regime.
\newblock In Melanie~F. Pradier, Aaron Schein, Stephanie Hyland, Francisco
  J.~R. Ruiz, and Jessica~Z. Forde, editors, \emph{Proceedings on "I (Still)
  Can't Believe It's Not Better!" at NeurIPS 2021 Workshops}, volume 163 of
  \emph{Proceedings of Machine Learning Research}, pages 41--47. PMLR, 13 Dec
  2022.
\newblock URL \url{https://proceedings.mlr.press/v163/hayou22a.html}.

\bibitem[Horn and Mathias(1992)]{Hornetal1992}
Roger~A. Horn and Roy Mathias.
\newblock Block-matrix generalizations of schur's basic theorems on hadamard
  products.
\newblock \emph{Linear Algebra and its Applications}, 172:\penalty0 337--346,
  1992.
\newblock ISSN 0024-3795.
\newblock \doi{https://doi.org/10.1016/0024-3795(92)90033-7}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0024379592900337}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{Jacotetal2018}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{Liuetal2022}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0
  85--116, 2022.
\newblock ISSN 1063-5203.
\newblock \doi{https://doi.org/10.1016/j.acha.2021.12.009}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S106352032100110X}.
\newblock Special Issue on Harmonic Analysis and Machine Learning.

\bibitem[Montanari and Zhong(2022)]{Montanarietal2020}
Andrea Montanari and Yiqiao Zhong.
\newblock {The interpolation phase transition in neural networks: Memorization
  and generalization under lazy training}.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (5):\penalty0 2816 --
  2847, 2022.
\newblock \doi{10.1214/22-AOS2211}.
\newblock URL \url{https://doi.org/10.1214/22-AOS2211}.

\bibitem[Nguyen(2021)]{Nguyen2021}
Quynh~N. Nguyen.
\newblock On the proof of global convergence of gradient descent for deep relu
  networks with linear widths.
\newblock 2021.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:231698350}.

\bibitem[Nguyen and Mondelli(2020)]{Nguyenetal2020}
Quynh~N Nguyen and Marco Mondelli.
\newblock Global convergence of deep networks with one wide layer followed by
  pyramidal topology.
\newblock 33:\penalty0 11961--11972, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf}.

\bibitem[Nguyen et~al.(2021)Nguyen, Mondelli, and Montufar]{Nguyenetal2021}
Quynh~N. Nguyen, Marco Mondelli, and Guido~F. Montufar.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel
  for deep relu networks.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 8119--8129. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/nguyen21g.html}.

\bibitem[Oymak and Soltanolkotabi(2019)]{Oymaketal2019a}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  4951--4960. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/oymak19a.html}.

\bibitem[Oymak and Soltanolkotabi(2020)]{Oymaketal2020}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees
  for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 84--105, 2020.
\newblock \doi{10.1109/JSAIT.2020.2991332}.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{Pooleetal2016}
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya
  Ganguli.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf}.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{Schoenholzetal2017}
Samuel~S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=H1W1UN9gg}.

\bibitem[Seleznova and Kutyniok(2022)]{Seleznovaetal22}
Mariia Seleznova and Gitta Kutyniok.
\newblock Neural tangent kernel beyond the infinite-width limit: Effects of
  depth and initialization.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 19522--19560. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/seleznova22a.html}.

\bibitem[Song et~al.(2021)Song, Ramezani-Kebrya, Pethick, Eftekhari, and
  Cevher]{Songetal2021}
Chaehwan Song, Ali Ramezani-Kebrya, Thomas Pethick, Armin Eftekhari, and Volkan
  Cevher.
\newblock Subquadratic overparameterization for shallow neural networks.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=NhbFhfM960}.

\bibitem[Su and Yang(2019)]{Suetal2019}
Lili Su and Pengkun Yang.
\newblock On learning over-parameterized neural networks: A functional
  approximation perspective.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/253f7b5d921338af34da817c00f42753-Paper.pdf}.

\bibitem[Terj\'ek and Gonz\'alez-S\'anchez(2025)]{mlpsateoc1}
D\'avid Terj\'ek and Diego Gonz\'alez-S\'anchez.
\newblock {MLP}s at the {EOC}: Spectrum of the {NTK}, 2025.

\bibitem[Tretter(2008)]{Tretter2008}
Christiane Tretter.
\newblock \emph{Spectral Theory of Block Operator Matrices and Applications}.
\newblock IMPERIAL COLLEGE PRESS, 2008.
\newblock \doi{10.1142/p493}.
\newblock URL \url{https://www.worldscientific.com/doi/abs/10.1142/p493}.

\bibitem[van~der Vaart and Wellner(2023)]{Vandervaartetal2023}
A.W. van~der Vaart and J.A. Wellner.
\newblock \emph{Weak Convergence and Empirical Processes: With Applications to
  Statistics}.
\newblock Springer Series in Statistics. Springer International Publishing,
  2023.
\newblock ISBN 9783031290404.
\newblock URL \url{https://books.google.hu/books?id=vfzKEAAAQBAJ}.

\bibitem[Vershynin(2018)]{Vershynin2018}
Roman Vershynin.
\newblock \emph{High-Dimensional Probability: An Introduction with Applications
  in Data Science}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2018.
\newblock \doi{10.1017/9781108231596}.

\bibitem[Wang and Zhu(2024)]{Wangetal2021}
Zhichao Wang and Yizhe Zhu.
\newblock {Deformed semicircle law and concentration of nonlinear random
  matrices for ultra-wide neural networks}.
\newblock \emph{The Annals of Applied Probability}, 34\penalty0 (2):\penalty0
  1896 -- 1947, 2024.
\newblock \doi{10.1214/23-AAP2010}.
\newblock URL \url{https://doi.org/10.1214/23-AAP2010}.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{Woodworthetal2020}
Blake Woodworth, Suriya Gunasekar, Jason~D. Lee, Edward Moroshko, Pedro
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In Jacob Abernethy and Shivani Agarwal, editors, \emph{Proceedings of
  Thirty Third Conference on Learning Theory}, volume 125 of \emph{Proceedings
  of Machine Learning Research}, pages 3635--3673. PMLR, 09--12 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v125/woodworth20a.html}.

\bibitem[Xiao et~al.(2020)Xiao, Pennington, and Schoenholz]{Xiaoetal2020}
Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz.
\newblock Disentangling trainability and generalization in deep neural
  networks.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 10462--10472. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/xiao20b.html}.

\bibitem[Xu and Zhu(2024)]{Xuetal2024}
Jiaming Xu and Hanjing Zhu.
\newblock Overparametrized multi-layer neural networks: Uniform concentration
  of neural tangent kernel and convergence of stochastic gradient descent.
\newblock \emph{Journal of Machine Learning Research}, 25\penalty0
  (94):\penalty0 1--83, 2024.
\newblock URL \url{http://jmlr.org/papers/v25/23-0740.html}.

\bibitem[Yang(2020)]{Yang2020}
Greg Yang.
\newblock Tensor programs ii: Neural tangent kernel for any architecture, 2020.

\bibitem[Yang(2021)]{Yang2021}
Greg Yang.
\newblock Tensor programs iii: Neural matrix laws, 2021.

\bibitem[Yang and Hu(2021)]{Yangetal2021}
Greg Yang and Edward~J. Hu.
\newblock Tensor programs iv: Feature learning in infinite-width neural
  networks.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 11727--11737. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/yang21c.html}.

\bibitem[Yang et~al.(2023)Yang, Simon, and Bernstein]{Yangetal2023}
Greg Yang, James~B. Simon, and Jeremy Bernstein.
\newblock A spectral condition for feature learning, 2023.

\bibitem[Yang et~al.(2024{\natexlab{a}})Yang, Hu, Babuschkin, Sidor, Liu,
  Farhi, Ryder, Pachocki, Chen, and Gao]{Yangetal2022}
Greg Yang, Edward~J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David
  Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tensor programs v: tuning large neural networks via zero-shot
  hyperparameter transfer.
\newblock In \emph{Proceedings of the 35th International Conference on Neural
  Information Processing Systems}, NIPS '21, Red Hook, NY, USA,
  2024{\natexlab{a}}. Curran Associates Inc.
\newblock ISBN 9781713845393.

\bibitem[Yang et~al.(2024{\natexlab{b}})Yang, Yu, Zhu, and Hayou]{Yangetal2024}
Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou.
\newblock Tensor programs {VI}: Feature learning in infinite depth neural
  networks.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=17pVDnpwwl}.

\bibitem[Zou and Gu(2019)]{Zouetal2019}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock 32, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf}.

\end{thebibliography}
