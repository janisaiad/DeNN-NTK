%Recently, \citet{lai2023_GeneralizationAbility} demonstrated the generalization ability of the two-layer wide neural network on one dimensional data:
%1. the properly early stopped gradient descent will result in a neural network achieving the optimal convergence rate;
%2. the overfitted neural network (trained by gradient descent) can not generalize.
%In this paper, we greatly extended their results to multilayer neural network with data supported in $\R^{d}$.
%These results strongly suggest that the overfitted wide neural network (trained by gradient descent) can not generalize well,
%which urges us to scrutinize the widely reported `benign overfitting phenomenon' in deep neural network literature.
%It would be of great interest to see if these results can be extended to the large dimensional data where $d \propto n^{s}$ for some $s>0$ instead of the fixed $d$ here.

In this paper, we develop a novel approach for determining the eigenvalue decay rate (EDR) of certain kernels using transformation and restriction.
Using this approach, we determine the EDR of the NTKs associated with multilayer fully-connected ReLU neural networks on a general domain.
Combining this result with the uniform approximation of the neural network by the NTK regression,
we determine the generalization performance of the over-parameterized neural network through the kernel regression theory.
The theoretical results show that proper early stopping is essential for the generalization performance of the neural networks,
which urges us to scrutinize the widely reported ``benign overfitting phenomenon'' in deep neural network literature.

For future directions, it is natural to extend our results to the NTKs associated with other neural network architectures,
such as convolutional neural networks and residual neural networks.
Also, it would be of great interest to see if these results can be extended to the large dimensional data where $d \propto n^{s}$ for some $s>0$ instead of the fixed $d$ here.