
\documentclass{article} 
\usepackage{iclr2021_conference,times}

\usepackage{hyperref}
\usepackage{url}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{enumitem}

\input{macros}

\title{Deep Equals Shallow for ReLU Networks \\ in Kernel Regimes}


\author{Alberto Bietti\thanks{Work done while at Inria.} \\
NYU\thanks{Center for Data Science, New York University. New York, USA.} \\
\texttt{alberto.bietti@nyu.edu}
\And
Francis Bach \\
Inria\thanks{Inria - Département d’Informatique de l’École Normale Supérieure.
PSL Research University.
Paris, France.} \\
\texttt{francis.bach@inria.fr} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
\begin{document}


\maketitle

\begin{abstract}
Deep networks are often considered to be more expressive than shallow ones in terms of approximation.
Indeed, certain functions can be approximated by deep networks provably more efficiently than by shallow ones, however, no tractable algorithms are known for learning such deep models.
Separately, a recent line of work has shown that deep networks trained with gradient descent may behave like (tractable) kernel methods in a certain over-parameterized regime, where the kernel is determined by the architecture and initialization, and this paper focuses on approximation for such kernels.
We show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their ``shallow'' two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures.
Our main theoretical result relies on characterizing such eigenvalue decays through differentiability properties of the kernel function, which also easily applies to the study of other kernels defined on the~sphere.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
\input{introduction}

\section{Review of Approximation with Dot-Product Kernels}
\label{sec:background}
\input{background}

\section{Main Result and Applications to Deep Networks}
\label{sec:approx}
\input{approx}


\section{Numerical experiments}
\label{sec:experiments}
\input{experiments}

\section{Discussion}
\input{discussion}


\subsubsection*{Acknowledgments}
The authors would like to thank David Holzmüller for finding an error in an earlier version of the paper, which led us to include the new assumption on differentiation of asymptotic expansions in Theorem~\ref{thm:decay}.
This work was funded in part by the French government under management of Agence Nationale
de la Recherche as part of the ``Investissements d’avenir'' program, reference ANR-19-P3IA-0001
(PRAIRIE 3IA Institute). We also acknowledge support of the European Research Council (grant
SEQUOIA 724063).

\bibliography{full,bibli}
\bibliographystyle{iclr2021_conference}

\appendix
\section{Background on Spherical Harmonics}
\label{sec:appx_background}
\input{appx_background}

\section{Proof of Theorem~\ref{thm:decay}}
\label{sec:appx_thm_proof}
\input{appx_thm_proof}

\section{Other Proofs}
\label{sec:appx_proofs}
\input{appx_proofs}

\end{document}
