\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{tocloft}
\usepackage{tikz}
% Page setup
\geometry{a4paper, margin=2.5cm}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Analyse Spectrale du NTK et Entraînement Sobolev}
\fancyhead[R]{\thepage}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}

% Mathematical commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\evmin}[1]{\lambda_{\min}\left(#1\right)}
\newcommand{\evmax}[1]{\lambda_{\max}\left(#1\right)}
\newcommand{\svmin}[1]{\sigma_{\min}\left(#1\right)}
\newcommand{\tr}{\text{tr}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\uu}{\mathbf{u}} % changed from \u to avoid conflict
\newcommand{\vv}{\mathbf{v}} % changed from \v to avoid conflict
% \newcommand{\odot}{\odot} % already defined by amssymb if loaded, or define explicitly
\newcommand{\bigOmg}{\Omega}
\newcommand{\bigTheta}{\Theta}
\newcommand{\Pr}{\text{Pr}}
\newcommand{\KNTK}{K_{\text{NTK}}}
\newcommand{\KLim}{K^{(L)}}
\newcommand{\KEmp}{\overline{K}^{(L)}}
\newcommand{\KCosine}{K_L}
\newcommand{\lambdaMin}{\lambda_{\min}}
\newcommand{\lambdaMax}{\lambda_{\max}}
\newcommand{\sigmaMin}{\sigma_{\min}}
\newcommand{\Order}{\mathcal{O}}
\newcommand{\TildeOrder}{\tilde{\mathcal{O}}}
\newcommand{\OmegaNotation}{\Omega}
\newcommand{\TildeOmega}{\tilde{\Omega}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\LaplaceBeltrami}{\Delta_{\mathbb{S}^{d_0-1}}}

\title{Analyse de l'Échelle des Valeurs Propres du NTK et \ de sa Modification par Entraînement de type Sobolev}

\author{Synthèse et Analyse}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Ce document examine l'échelle des valeurs propres du Noyau Tangent Neuronal (NTK), en s'appuyant sur des travaux récents, et détaille comment l'entraînement de type Sobolev, via un préconditionnement matriciel, modifie le spectre du NTK. Nous nous concentrons sur les réseaux de neurones profonds avec des activations homogènes (comme ReLU) et analysons l'impact de cette technique sur la décroissance des valeurs propres, en particulier sur la plus petite valeur propre et son vecteur propre associé, dans le contexte des harmoniques sphériques.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

Le Noyau Tangent Neuronal (NTK) est un outil théorique fondamental pour comprendre l'entraînement et la généralisation des réseaux de neurones profonds. Son spectre de valeurs propres, en particulier sa plus petite valeur propre $\lambdaMin(\KNTK)$, gouverne la dynamique d'entraînement et les capacités de généralisation du modèle. Une observation courante pour de nombreux noyaux, y compris le NTK dans certains régimes, est une décroissance rapide de leurs valeurs propres, par exemple $\lambda_k \sim k^{-\alpha}$, où $k$ indexe les valeurs propres (souvent par fréquence croissante) et $\alpha$ dépend de la régularité du noyau et de la dimensionnalité des données. Cette décroissance peut limiter l'apprentissage des fonctions de haute fréquence et affecter le conditionnement du noyau.

Ce rapport vise à :
\begin{enumerate}
    \item Synthétiser les résultats concernant l'échelle des valeurs propres du NTK, en se basant sur des études récentes (par exemple, celles indiquées par les arXiv ID \texttt{2305.02657} et \texttt{2205.14300}).
    \item Expliquer en détail comment une technique d'entraînement inspirée des espaces de Sobolev, utilisant une matrice de préconditionnement $P$, peut contrecarrer cette décroissance des valeurs propres. Nous supposerons que $P$ partage les mêmes vecteurs propres que le NTK $K$ (par exemple, les harmoniques sphériques pour des données sur la sphère $\mathbb{S}^{d_0-1}$), et le noyau modifié devient $K_S = KP$.
\end{enumerate}
L'objectif final est de comprendre comment cette approche modifie le spectre du NTK, en particulier sa plus petite valeur propre et le mode (vecteur propre) qui lui est associé.

\section{Échelle des Valeurs Propres du Noyau Tangent Neuronal}

L'analyse du spectre du NTK, et notamment de sa plus petite valeur propre $\lambdaMin$, est cruciale. Des travaux récents ont établi des conditions sous lesquelles $\lambdaMin(\KNTK)$ est strictement positive, garantissant que le processus d'apprentissage par descente de gradient peut minimiser la fonction de perte.

\subsection{Non-dégénérescence du NTK}
Des études comme celles de Banerjee et al. (2023, arXiv:2305.02657) ont montré que pour des réseaux ReLU, une largeur de couche "linéaire" (par exemple, $\TildeOmega(N)$ où $N$ est le nombre d'échantillons) suffit pour garantir que $\lambdaMin(\KNTK) > c > 0$ avec une forte probabilité. Cela contraste avec des exigences de largeur quadratique plus anciennes.
\begin{theorem}[Banerjee et al., 2023, simplifié]
Pour des réseaux de neurones profonds avec activations ReLU et sous certaines conditions sur les données, si la largeur $m$ des couches cachées satisfait $m = \TildeOmega(N)$, alors $\lambdaMin(\KNTK) \geq c_0 > 0$ avec haute probabilité.
\end{theorem}
De même, Nguyen et al. (2022, arXiv:2205.14300) ont exploré la non-dégénérescence du NTK pour des réseaux profonds avec des activations log-Lipschitz (une classe qui n'inclut pas directement ReLU, mais des versions lissées). Leurs travaux, ainsi que d'autres (par exemple, Nguyen et al., ICML 2021, arXiv:2006.06333), soutiennent l'idée que le NTK est typiquement non dégénéré pour des architectures suffisamment larges.

\subsection{Décroissance Spectrale Générale}
Au-delà de la simple non-dégénérescence de $\lambdaMin$, le comportement de l'ensemble du spectre est important. Pour les opérateurs intégraux associés à des noyaux définis sur des variétés de dimension $d_0$, les valeurs propres $\lambda_l$ (où $l$ peut correspondre au degré d'une base de fonctions propres, comme les harmoniques sphériques) présentent souvent une décroissance polynomiale : $\lambda_l(K) \sim l^{-\alpha}$. L'exposant $\alpha$ dépend de la régularité (nombre de dérivées continues) du noyau et de la dimension $d_0$. Par exemple, pour un noyau $s$ fois différentiable sur une variété de dimension $d_0$, on observe souvent $\alpha \approx 2s/d_0$. Le NTK issu d'activations ReLU possède une régularité limitée (par exemple, $C^1$ mais pas $C^2$ dans de nombreux cas), ce qui peut conduire à une décroissance relativement rapide de ses valeurs propres, potentiellement $\alpha > 1$. L'utilisateur a spécifiquement mentionné une décroissance en $l^{-d_0}$ comme point de référence à contrecarrer, ce qui impliquerait $\alpha = d_0$.

Cette décroissance rapide signifie que les modes de haute fréquence (associés aux grandes valeurs de $l$) ont de petites valeurs propres, ce qui peut rendre leur apprentissage lent ou numériquement instable.

\section{Entraînement de type Sobolev pour la Modification du Spectre NTK}

Pour pallier la décroissance rapide des valeurs propres du NTK, des techniques d'entraînement inspirées des espaces de Sobolev peuvent être employées. L'idée centrale est de préconditionner le noyau $K$ avec un opérateur (ou une matrice) $P$ qui rehausse les composantes de haute fréquence.

\subsection{Le Concept d'Entraînement Sobolev}
L'entraînement de type Sobolev vise à modifier la métrique de l'espace des fonctions dans lequel l'optimisation a lieu. En pratique, cela peut se traduire par la modification de la fonction de perte ou, de manière équivalente dans le régime des noyaux, par la modification du noyau lui-même. L'objectif est de donner plus de poids aux composantes de la fonction qui varient rapidement (hautes fréquences).

\subsection{Préconditionnement avec la Matrice P}
Nous considérons un noyau NTK $K$ (par exemple, pour un MLP avec activation ReLU, ou une autre activation homogène). Soit $P$ une matrice (ou un opérateur) de préconditionnement. Le noyau modifié par l'entraînement Sobolev est $K_S = KP$.

Une hypothèse clé, formulée par l'utilisateur, est que $K$ et $P$ partagent les mêmes vecteurs propres. Pour des données sur la sphère $\mathbb{S}^{d_0-1}$ et des noyaux invariants par rotation, ces vecteurs propres communs sont typiquement les harmoniques sphériques $Y_{lm}(\uu)$.
Soit $v_l$ un vecteur propre commun (correspondant à un harmonique sphérique de degré $l$):
\begin{align*}
    Kv_l &= \lambda_l(K) v_l \\
    Pv_l &= \lambda_l(P) v_l
\end{align*}
Alors, pour le noyau modifié $K_S = KP$:
\begin{align*}
    K_S v_l = K(Pv_l) = K(\lambda_l(P)v_l) = \lambda_l(P) (Kv_l) = \lambda_l(P) \lambda_l(K) v_l
\end{align*}
Ainsi, les valeurs propres de $K_S$ sont simplement le produit des valeurs propres de $K$ et de $P$:
\begin{equation}
    \lambda_l(K_S) = \lambda_l(K) \lambda_l(P)
    \label{eq:eigen_prod}
\end{equation}

\subsection{Contrecarrer la Décroissance des Valeurs Propres avec les Harmoniques Sphériques}
Considérons des données d'entrée sur la sphère unité $\mathbb{S}^{d_0-1}$. Les harmoniques sphériques $Y_{lm}$ forment une base orthonormée de $L^2(\mathbb{S}^{d_0-1})$.
\begin{itemize}
    \item \textbf{Valeurs propres du NTK $K$}: Comme discuté, supposons que les valeurs propres $\lambda_l(K)$ associées aux harmoniques de degré $l$ décroissent polynomialement pour $l$ grand:
    \begin{equation}
        \lambda_l(K) \sim c_K l^{-\alpha} \quad \text{pour } l \ge 1
    \end{equation}
    L'utilisateur a suggéré une décroissance de référence de type $l^{-d_0}$, donc nous pouvons considérer $\alpha \approx d_0$.

    \item \textbf{Opérateur de préconditionnement $P$}: L'opérateur $P$ est choisi pour amplifier les hautes fréquences. Il est typiquement lié à un opérateur pseudo-différentiel de type Sobolev, par exemple $P \propto (c_0 I - \LaplaceBeltrami)^{s/2}$, où $\LaplaceBeltrami$ est l'opérateur de Laplace-Beltrami sur $\mathbb{S}^{d_0-1}$ et $s > 0$. Les harmoniques sphériques $Y_{lm}$ sont des fonctions propres de $\LaplaceBeltrami$:
    \begin{equation*}
        -\LaplaceBeltrami Y_{lm} = l(l+d_0-2) Y_{lm}
    \end{equation*}
    où $l(l+d_0-2)$ est la valeur propre pour le degré $l$.
    Par conséquent, les valeurs propres de $P$ se comportent comme suit pour $l$ grand:
    \begin{equation}
        \lambda_l(P) \sim c_P (l(l+d_0-2))^{s/2} \sim c'_P l^s \quad \text{pour } l \ge 1
    \end{equation}

    \item \textbf{Valeurs propres du noyau modifié $K_S = KP$}: En utilisant l'équation \eqref{eq:eigen_prod}:
    \begin{equation}
        \lambda_l(K_S) = \lambda_l(K) \lambda_l(P) \sim (c_K l^{-\alpha}) (c'_P l^s) = c_{KP} l^{s-\alpha}
        \label{eq:modified_decay}
    \end{equation}
\end{itemize}
Si l'on souhaite contrecarrer une décroissance initiale $\lambda_l(K) \sim l^{-d_0}$ (c'est-à-dire $\alpha = d_0$), on peut choisir $s \approx d_0$. Dans ce cas, $\lambda_l(K_S) \sim c_{KP} l^{d_0-d_0} = c_{KP}$, ce qui signifie que les valeurs propres du noyau modifié deviennent approximativement constantes pour les hautes fréquences, au lieu de décroître. Si $s > d_0$, elles pourraient même augmenter avec $l$.

\subsection{Impact sur la Plus Petite Valeur Propre et son Vecteur Propre}
Le comportement de la "plus petite valeur propre" est modifié de manière significative.
\begin{itemize}
    \item Pour le noyau original $K$, si $\lambda_l(K) \sim l^{-\alpha}$ avec $\alpha > 0$, les valeurs propres deviennent très petites pour les $l$ élevés (hautes fréquences). La "plus petite valeur propre" est souvent comprise comme la limite lorsque $l \to \infty$ (qui est zéro), ou la valeur propre pour le plus grand $l$ considéré dans une approximation finie.

    \item Pour le noyau modifié $K_S = KP$, les valeurs propres sont $\lambda_l(K_S) \sim l^{s-\alpha}$.
    \begin{enumerate}
        \item Si $s > \alpha$ (par exemple, $s=d_0$ et $\alpha < d_0$, ou $s > d_0$ pour $\alpha=d_0$), alors $s-\alpha > 0$. Les valeurs propres $\lambda_l(K_S)$ \textit{augmentent} avec $l$. Dans ce cas, la plus petite valeur propre de $K_S$ (pour $l \ge 1$) correspondrait au degré $l$ le plus bas, par exemple $l=1$. Le vecteur propre associé serait l'harmonique sphérique de plus bas degré (non constant).
        \item Si $s = \alpha$ (par exemple, $s=d_0$ et $\alpha=d_0$), alors $s-\alpha = 0$. Les valeurs propres $\lambda_l(K_S)$ sont approximativement constantes pour $l$ grand. Il n'y a plus de "petite" valeur propre unique due à la décroissance.
        \item Si $s < \alpha$ (le préconditionnement est "trop faible"), alors $s-\alpha < 0$. Les valeurs propres $\lambda_l(K_S)$ décroissent encore avec $l$, mais plus lentement que $\lambda_l(K)$. La plus petite valeur propre correspondrait toujours aux $l$ les plus élevés.
    \end{enumerate}
\end{itemize}
Dans tous les cas où $s>0$, les valeurs propres $\lambda_l(K)$ qui étaient initialement petites (pour $l$ grand) sont multipliées par $\lambda_l(P) \sim l^s$, qui est grand pour $l$ grand. Ainsi, $\lambda_l(K_S)$ est significativement plus grand que $\lambda_l(K)$ pour les hautes fréquences. Cela améliore le conditionnement global du noyau et facilite l'apprentissage des composantes de haute fréquence de la fonction cible.

Le vecteur propre associé à $\lambda_l(K_S)$ reste l'harmonique sphérique $Y_{lm}$ (ou la combinaison linéaire correspondante si $v_l$ est un espace propre). La question de savoir "pour quel vecteur elle est la plus petite" dépend donc du nouveau profil $l^{s-\alpha}$ et de la plage de $l$ considérée. Si $s \ge \alpha$, les valeurs propres ne tendent plus vers zéro pour les hautes fréquences, ce qui est l'effet désiré.

\section{Cas Spécifique du NTK des MLP avec Activation ReLU}
Le NTK pour un MLP avec activation ReLU (qui est homogène) est connu pour avoir une régularité limitée. Sur $\mathbb{S}^{d_0-1}$, sa fonction noyau $K(\x, \x')$ dépend de $\langle \x, \x' \rangle$. La décomposition en harmoniques sphériques (ou polynômes de Gegenbauer) révèle une décroissance polynomiale des $\lambda_l(K)$. L'application du préconditionnement de Sobolev $P$ avec un $s$ approprié permet de rendre ce spectre beaucoup plus "plat", améliorant l'apprentissage. Par exemple, des travaux comme ceux de Druker et al. (2023, arXiv:2305.19380) explorent empiriquement et théoriquement la puissance du préconditionnement de type Sobolev pour les noyaux, y compris le NTK.

\section{Conclusion}
L'entraînement de type Sobolev, par le biais d'un préconditionnement $P$ qui partage les vecteurs propres du NTK $K$, offre un mécanisme puissant pour remodeler le spectre du noyau $K_S = KP$. En choisissant judicieusement $P$ (par exemple, basé sur des puissances de l'opérateur de Laplace-Beltrami, caractérisé par le paramètre $s$), il est possible de contrecarrer la décroissance naturelle des valeurs propres $\lambda_l(K) \sim l^{-\alpha}$ du NTK, en particulier pour les activations ReLU.

Si les valeurs propres de $P$ croissent comme $\lambda_l(P) \sim l^s$, les valeurs propres modifiées $\lambda_l(K_S) \sim l^{s-\alpha}$ peuvent être rendues constantes (si $s=\alpha$) ou même croissantes (si $s>\\alpha$) avec la fréquence $l$. Cela a pour effet d'augmenter significativement les valeurs propres associées aux hautes fréquences, améliorant le conditionnement du problème d'apprentissage et permettant une meilleure capture des détails fins de la fonction cible. La nature du "plus petit" mode propre est ainsi transformée : au lieu d'être systématiquement un mode de haute fréquence, il peut devenir un mode de basse fréquence ou le spectre peut devenir relativement plat, éliminant le problème des valeurs propres évanescentes aux hautes fréquences.

\section*{Références}

\begin{enumerate}
    \item Banerjee, A., Cisneros-Velarde, P., Zhu, L., Belkin, M. (2023). Neural Tangent Kernel at Initialization: Linear Width Suffices. \textit{arXiv:2305.02657}.
    \item Nguyen, Q., Pham, T., Nguyen, T., Montúfar, G. (2022). The Neural Tangent Kernel of Deep Networks with Log-Lipschitz Activations is Non-Degenerate. \textit{arXiv:2205.14300}.
    \item Nguyen, Q., Mondelli, M., Montúfar, G. (2021). Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks. \textit{ICML 2021 (arXiv:2006.06333)}.
    \item Druker, R., Falik, M., Razin, N., Shamir, O., Wagner, T. (2023). On the Power of Preconditioning in Kernel Ridge Regression: A Theoretical and Empirical Study of Sobolev Training for Neural Tangent Kernels. \textit{arXiv:2305.19380}.
    % Ajouter d'autres références si nécessaire, par exemple sur la décomposition en harmoniques sphériques des noyaux ou la régularité du NTK ReLU.
\end{enumerate}

\end{document}
