
\section{Introduction}

Deep neural networks have achieved remarkable success across a diverse range of applications in the past decade. 
However, it is well-known that small, adversarially chosen perturbations of the input can drastically alter the output of the neural network.
These perturbations, which often can be chosen to be invisible to the human eye,
are known in the literature as \textit{adversarial examples}.
One of the first papers to empirically examine this phenomenon was \cite{szegedy2013intriguing}.
This observation has lead to an extensive body of research,
focusing both on devising increasingly sophisticated adversarial attacks
(see, for example,  \cite{papernot2016limitations,moosavi2016deepfool,kurakin2018adversarial,athalye2018obfuscated,carlini2018audio})
and on constructing robust defenses against such attacks
(see, for example, \cite{madry2018towards,schott2018towards,pang2019improving,dong2020adversarial,bui2021unified}).
Arguably, this has resulted in an "arms race" between adversarial attacks and defenses. 
The theoretical understanding of adversarial examples, however, remains limited.

A natural measure for the worst-case robustness against adversarial examples is given
by the Lipschitz constant of a neural network function $\Phi: \mathbb{R}^d \rightarrow \mathbb{R}$,
which is defined via
\begin{equation*}
  \lip (\Phi )
  :=
  \underset{x,y \in \mathbb{R}^d, \, x \neq y}{\sup}  \frac{\vert \Phi (x)- \Phi (y) \vert}{\Vert x - y \Vert_2},
\end{equation*}
where by $\Vert \cdot \Vert_2$ we denote the $\ell_2$-norm of a vector.
Indeed, the Lipschitz constant measures how much a small change of the input can perturb the output. 
A small Lipschitz constant may imply that no adversarial examples exist,
whereas a large Lipschitz constant is indicative of the existence of adversarial examples.
For these reasons, several works have proposed algorithms to determine
upper bounds for the Lipschitz constant of a trained deep neural network---see,
e.g., \cite{fazlyab2019efficient,ebihara2023local,shi2022efficiently}---in order to quantify the worst-case robustness against adversarial examples.



\emph{Random neural networks}, i.e., neural networks whose weights and biases are chosen at random,
are often used as a first step in theoretically understanding empirically observed phenomena in deep learning;
see for instance \cite{dirksen2022separation,bartlett2021adversarial,daniely2020most}.
In this spirit, in this paper, we establish upper and lower bounds for the Lipschitz constant of random ReLU neural networks.
Our main contributions can be summarized as follows:
\begin{itemize}
  \item In the case of shallow neural networks with ReLU activation function
        and random weights following a variant of the He initialization \cite{he2015delving},
        we prove high-probability upper and lower bounds for the Lipschitz constant.
        These bounds match up to an absolute numerical constant.

  \item In the case of deep neural networks with ReLU activation function,
        we again derive high-probability upper and lower bounds for the Lipschitz constant.
        For this, we assume that our networks are wide enough, i.e.,
        they satisfy $N\gtrsim L^2 d$, where $N$ is the width of the neural network,
        $d$ is the dimension of the input, and $L$ is the depth of the neural network.
        If the depth $L$ is assumed to be a constant, our upper and lower bounds match up to a factor that is logarithmic in the width.
\item Regarding the biases, we allow arbitrary symmetric distributions, whereas earlier works on the Lipschitz constant of neural networks \cite{nguyen2021tight,buchanan2021deep}
require all biases to be initialized to zero, as in the classical He initialization.
\end{itemize}




The paper is structured as follows:
\Cref{sec:Background} provides technical background regarding random ReLU networks.
In Section \ref{sec:mainresults}, we present our main theoretical results
and in Section \ref{sec:preliminaries} we review some technical tools needed for our proofs.
The proofs regarding the upper bound of the Lipschitz constant are given in \Cref{sec:upper}. 
The corresponding lower bound is derived in \Cref{sec:low_bound}.


\subsection{Related Work}\label{subsec:relatedwork}

In \cite{bubeck2021law}, it has been conjectured that any shallow neural network
(with one hidden layer)  which interpolates the training data must have large Lipschitz constant,
unless the neural network is extremely overparameterized.
In \cite{bubeck2021universal}, this conjecture was proven and generalized to a much broader class
of models beyond (shallow) neural networks. 
However, while this result states that neural networks interpolating the data
can only be adversarially robust if they are extremely overparameterized,
it does not imply that overparameterized networks are inherently adversarially robust;
this is because only a \emph{lower} bound for the Lipschitz constant (which tends to zero as the number of network parameters goes to infinity) is presented.
Moreover, when the data is no longer \emph{perfectly} interpolated,
the lower bound for the Lipschitz constant in \cite{bubeck2021universal} no longer applies.
This leaves open the possibility of training adversarially robust neural networks
without huge overparameterization under such conditions.
For these reasons, a more detailed analysis of the Lipschitz constant of neural networks is essential.

\emph{Upper} bounds for the Lipschitz constants of random neural networks were derived in \cite{nguyen2021tight,buchanan2021deep}
in the context of the lazy training regime \cite{chizat2019lazy}. 
In contrast to our work, these two results do not contain any lower bounds. 
We refer to \Cref{remark:compare} for a detailed comparison of the results in \cite{nguyen2021tight,buchanan2021deep} with our bounds.

A \emph{lower} bound on the gradient of random ReLU networks at a fixed point was derived in \cite{bartlett2021adversarial}
in the context of proving the existence of an adversarial example at a fixed point. This result immediately implies a lower bound on the Lipschitz constant as well,
since random ReLU networks are almost surely differentiable at a fixed point (cf. \Cref{app:diff}).
We refer to \Cref{remark:compare_lower} for a comparison to the lower bound that we derive.

A line of research related to studying the Lipschitz constant of neural networks is concerned with proving the existence
of adversarial perturbations in \emph{random} neural networks.
The first result was obtained in \cite{shamir2019simple}, where it was shown that
by only perturbing a few entries of the input vector one can construct an adversarial example.
Realizing that these perturbations may be very large,
this result was further improved in \cite{daniely2020most},
where it was shown that adversarial examples can be constructed with much smaller perturbations.
However, this result applies only to random ReLU networks for which the width of each layer
is small relative to the width of the previous layer and for which the penultimate layer still needs to be large enough (depending on the input dimension).
In the case of shallow networks, \cite{bubeck2021single} improved the latter result for networks
with ReLU activation or smooth activation whose width depends subexponentially on the input dimension.
The follow-up work \cite{bartlett2021adversarial} extended this result to the multi-layer setting.
Recently, this has been further improved in \cite{montanari2023adversarial}
which presented a result that requires no restriction on the width of the neural network
and which applies to general non-constant locally Lipschitz continuous activations.
All of the above results, however, are of a somewhat different nature compared to the results of the paper at hand.
The above results show, for an arbitrarily \emph{fixed} input, the existence,
with high probability, of a small adversarial perturbation. 
In contrast, our paper considers the adversarial robustness over all possible inputs and,
thereby, characterizes worst-case adversarial robustness.

Finally, let us note that going beyond random neural networks,
in \cite{wang2022adversarial} the existence of adversarial examples was recently shown
for trained neural networks in the lazy training regime \cite{chizat2019lazy}.
Moreover, in \cite{vardi2022gradient,frei2023double} it was shown that in neural networks
with one hidden layer, the bias of gradient flow towards margin maximization induces
the existence of adversarial examples.
In \cite{bombari2023beyond}, adversarial robustness of (trained) random features
and neural tangent kernel regression models was studied.
Here, adversarial robustness was measured with respect to a sensitivity measure,
which is the $\ell_2$-norm of the gradient at a data point scaled by the $\ell_2$-norm of this data point.
 This sensitivity measure was analyzed for one fixed vector from the input data distribution,
 i.e., the \emph{average-case} adversarial robustness was analyzed.







\subsection{Notation}

For any mathematical statement $A$, we set $\mathbbm{1}_A =1$ if $A$ is true and $\mathbbm{1}_A = 0$ otherwise.
The notation $\mathbbm{1}_A$ will also be used for the indicator function of a set $A$. 
We write $a_+ \defeq \max \{0,a\}$ for $a \in \RR$.

For two vectors $x,y \in \RR^k$, we write $\langle x,y \rangle \defeq \sum_{i=1}^k x_iy_i$
for the inner product of $x$ and $y$.
Moreover, we denote by $\Vert x \Vert_2 \defeq \sqrt{\langle x, x \rangle}$ the \emph{Euclidean norm} of $x$. 
Given a metric space $T$ with metric $\varrho$, for $x \in T$ and $r>0$ we write
\begin{equation*}
  B^\varrho_{T} (x,r)
  \defeq \left\{ y \in T : \ \varrho(x,y) < r\right\},
  \quad
  \overline{B}^\varrho_{T} (x,r)
  \defeq \left\{ y \in T : \ \varrho(x,y) \leq r\right\}.
\end{equation*}
When considering the Euclidean norm on $\RR^k$, we denote the open and closed ball by $B_k(x,r)$
and $\overline{B}_k(x,r)$, respectively.
Furthermore, we denote $\mathbb{S}^{k-1} \defeq \left\{ x \in \RR^k: \ \Vert x \Vert_2 = 1\right\}$.
For a subset $T \subseteq \RR^k$, we denote the diameter of $T$ as
$\diam(T) \defeq \underset{x,y \in T}{\sup} \Vert x - y \Vert_2 \in [0, \infty]$. 

For a matrix $A \in \RR^{k_1 \times k_2}$, we define 
\begin{equation*}
  \Vert A \Vert_2 \defeq \underset{ x \in \B_{k_2}(0,1)}{\sup} \Vert Ax \Vert_2.
\end{equation*}
 By $A_{i,-}$ and $A_{-,j}$ we denote the $i$-th row and $j$-th column of $A$, respectively.
 We write $v \odot w \in \RR^k$ for the \emph{Hadamard product} of two vectors $v, w \in \RR^k$,
 which is defined as $(v \odot w)_j \defeq v_j \cdot w_j $ for $j \in \{1,...,k\}$.

For any set $A$ and an element $a \in A$, we denote by $\delta_a$ the \emph{Dirac-measure} at $a$.
For $\sigma > 0$, we write $\mathcal{N}(0, \sigma^2)$ for the standard normal distribution
with expectation $0$ and variance $\sigma^2$. 
\paul{We write $\mathcal{N}(0, I_k)$ for the distribution of a $k$-dimensional random vector with independent $\mathcal{N}(0,1)$-entries.}
Moreover, we write $\EE[X]$ for the expectation of a random variable $X$.
By $\PP^X$ we denote the probability measure induced by some random variable $X$,
i.e., $\PP^X(A) = \PP(X \in A) = \PP (X^{-1}(A))$. Furthermore, we denote $A^c$ for the complement of any event $A$.

