\newcommand{\gell}{\mathcal{G}^{(\ell)}}
\newcommand{\hell}{\mathcal{H}^{(\ell)}}
\newcommand{\gelll}{\mathcal{G}^{(\ell+1)}}
\newcommand{\G}{\mathcal{G}'}

\section{Proof of the lower bound} \label{sec:low_bound}
In this section, we establish \emph{lower} bounds on the Lipschitz constant of randomly initialized ReLU networks. 
The strategy for deriving these lower bounds for shallow networks differs significantly from the approach for deep networks. 
The lower bound in the case of shallow networks (see \Cref{sec:low_bound_shallow}) follows from the fact that the 
Lipschitz constant of a shallow ReLU network can be lower bounded by the Lipschitz constant of the corresponding \emph{linear} network (see \Cref{prop:shallow_low_linear}), combined with
 concentration properties of Gaussian matrices and vectors. 
In the case of deep networks (see \Cref{subsec:deep_lower}), we follow the approach that was already described in \eqref{eq:upbound}: 
Our strategy is to fix a point $x^{(0)} \in \RR^d \setminus \{0\}$ and derive lower bounds for the expression
\begin{equation*}
\Vert W^{(L)} D^{(L-1)}(x_0)W^{(L-1)} \cdots D^{(0)}(x_0) W^{(0)} \Vert_2.
\end{equation*}


\subsection{The shallow case}\label{sec:low_bound_shallow}
In this subsection, we deal with the case of shallow networks, i.e., $L=1$. 
We make heavy use of \Cref{prop:shallow_low_linear}, which states that we only have to consider the corresponding \emph{linear} network in that case, 
i.e., the network that arises from a ReLU network by omitting the ReLU activation. 
This reduces the problem to bounding the norm of the product of a Gaussian matrix with a Gaussian vector (from below). 
\begin{theorem}\label{thm:shallow_low_bound_2}
There exists a constant $c>0$ with the following property: If $\Phi:\RR^d \to \RR$ is a random shallow ReLU network with width $N$ satisfying \Cref{assum:1}, then for every $t,u \geq 0$ it holds
\begin{equation*}
\lip(\Phi) \geq \frac{1}{\sqrt{2}} \cdot \left(1 - \frac{u}{\sqrt{N}}\right)_+  \cdot (\sqrt{d} - t)_+
\end{equation*}
with probability at least $(1-2\exp(-ct^2))_+(1-2\exp(-cu^2))_+$. Recall that we write $a_+ = \max\{0,a\}$ for any number $a \in \RR$.
\end{theorem}
\begin{proof}
From \Cref{prop:shallow_low_linear} we infer that it holds
\begin{equation*}
\lip(\Phi) \geq \frac{1}{2}\lip(\widetilde{\Phi}) = \frac{1}{2} \cdot \Vert W^{(1)}\cdot W^{(0)} \Vert_2.
\end{equation*}
Therefore, in the following we only consider the expression $\Vert W^{(1)}\cdot W^{(0)} \Vert_2$.

To this end, we introduce the notations $U^{(0)} \defeq (W^{(0)})^T \in \RR^{d \times N}$ and $U^{(1)} \defeq (W^{(1)})^T \in \RR^{N}$. 
Moreover, we fix $t,u \geq 0$. Consider the set
\begin{equation*}
A_1 \defeq \{W^{(1)}: \ \Vert W^{(1)} \Vert_2 = \Vert U^{(1)} \Vert_2 \geq (\sqrt{N} - u)_+\}.	
\end{equation*} 
Since the entries of $W^{(1)}$ are $\mathcal{N}(0,1)$-distributed, and since the norm of a Gaussian random vector concentrates around the square root of its size (see, e.g., \cite[Theorem~3.1.1~\&~Equation~(2.14)]{vershynin_high-dimensional_2018}, we infer
\begin{equation*}
\PP^{W^{(1)}}(A_1) \geq (1 - 2\exp(-cu^2))_+
\end{equation*}
with a suitably chosen constant $c>0$. We now \emph{fix} the vector $W^{(1)}$ and consider the set
\begin{equation*}
A_2 (W^{(1)}) \defeq \left\{W^{(0)}: \ \Vert U^{(0)} U^{(1)}\Vert_2 \geq \frac{\sqrt{2} \Vert U^{(1)} \Vert_2}{\sqrt{N}} \cdot (\sqrt{d}-t)_+\right\}.
\end{equation*}
Firstly, assume that $W^{(1)} \neq 0$. Note then that $\frac{\sqrt{N}}{\sqrt{2} \Vert U^{(1)} \Vert_2} \cdot U^{(0)} U^{(1)} \sim \mathcal{N}(0, I_d)$, as follows from the independence of the rows of $U^{(0)}$ and \cite[Exercise~3.3.3~(a)]{vershynin_high-dimensional_2018}. 
Therefore, using again \cite[Theorem~3.1.1~\&~Equation~(2.14)]{vershynin_high-dimensional_2018} we get
\begin{align*}
&\norel \quad\frac{\sqrt{N}}{\sqrt{2} \Vert U^{(1)} \Vert_2} \cdot \Vert U^{(0)} U^{(1)} \Vert_2 \geq (\sqrt{d} - t)_+ \\
&\Leftrightarrow \quad  \Vert U^{(0)} U^{(1)} \Vert_2 \geq \frac{\sqrt{2} \Vert U^{(1)} \Vert_2}{\sqrt{N}} \cdot (\sqrt{d}-t)_+
\end{align*}
with probability at least $1- 2\exp(-ct^2)$ and the last inequality remains true in the case $U^{(1)}  = 0$. 
Hence, we see
\begin{equation*}
\PP^{W^{(0)}} (A_2(W^{(1)})) \geq (1 - 2\exp(-ct^2))_+.
\end{equation*}
For any tuple $(W^{(0)}, W^{(1)})$ with $W^{(1)} \in A_1$ and $W^{(0)} \in A_2(W^{(1)})$ we get
\begin{align*}
\Vert U^{(0)} U^{(1)}\Vert_2 \geq \frac{\sqrt{2} \Vert U^{(1)} \Vert_2}{\sqrt{N}} \cdot (\sqrt{d}-t)_+ \geq \frac{\sqrt{2} (\sqrt{N}-u)_+}{\sqrt{N}} \cdot (\sqrt{d}-t)_+ = \sqrt{2} \cdot \left(1 - \frac{u}{\sqrt{N}}\right)_+ \cdot (\sqrt{d} - t)_+.
\end{align*}
Therefore, \Cref{prop:highprob} yields the claim.
\end{proof}
Again, we plug in special values for $u$ and $t$ to derive the main result.

\renewcommand*{\proofname}{Proof of \Cref{thm:main_shallow_lower}}
\begin{proof}
Let $c_2>0$ be the constant appearing in \Cref{thm:shallow_low_bound_2}. We then pick $u = \frac{\sqrt{N}}{2}$ and $t = \frac{\sqrt{d}}{2}$ and directly get
\begin{equation*}
\lip(\Phi) \geq \frac{1}{\sqrt{2}} \cdot \frac{1}{2} \cdot \frac{1}{2} \cdot \sqrt{d} = \frac{1}{4\sqrt{2}} \cdot \sqrt{d}
\end{equation*}
with probability at least $(1-2\exp(-c_2 N/4))_+(1-2\exp(-c_2 d/4))_+$. 
Hence, the first claim follows picking $c \defeq c_2/4$.

For the expectation bound, we assume $d,N > \frac{\ln(2)}{c}$ and use Markov's inequality to get
\begin{align*}
\EE[\lip(\Phi)] &\geq \PP\left(\lip(\Phi) \geq \frac{1}{4\sqrt{2}} \cdot \sqrt{d}\right) \cdot \frac{1}{4\sqrt{2}} \cdot \sqrt{d} \\
&\geq (1- 2 \exp(-cN))(1-2\exp(-cd)) \cdot \frac{1}{4\sqrt{2}} \cdot \sqrt{d} \\
&\geq \underbrace{\big(1- 2 \exp(-c\cdot\left(\lfloor \ln(2) / c\rfloor + 1\right))\big)\big(1-2\exp(-c\cdot\left(\lfloor \ln(2) / c\rfloor + 1\right))\big) \cdot \frac{1}{4\sqrt{2}}}_{=: c_1} \cdot \sqrt{d} \\
&= c_1 \cdot \sqrt{d}. \qedhere
\end{align*}
\end{proof}
\renewcommand*{\proofname}{Proof}


\subsection{The deep case} \label{subsec:deep_lower}
In this subsection, we deal with the case of deep networks, i.e., $L \geq 2$. 
We note that the condition $L \geq 2$ is not needed in order for our proofs to work and in particular that our proofs also work in the case of shallow networks. 
However, for what follows we will need an additional assumption on the distribution of the biases (see \Cref{assum:2}) and the additional condition $N \gtrsim dL^2$ to prove our final lower bound on the Lipschitz constant,
whereas these assumptions are \emph{not} needed in the case of shallow networks. 
That is why we presented a different proof for shallow networks in \Cref{sec:low_bound_shallow}.

The basic observation is that, if we fix a point $x_0 \in \RR^d \setminus \{0\}$, the weights $W^{(0)}, ..., W^{(\ell - 1)}$ and biases $b^{(0)}, ..., b^{(\ell -1 )}$ and assume that the output of the $\ell$-th layer is non-zero (i.e., $x^{(\ell)} \neq 0$ with $x^{(\ell)}$ as in \Cref{eq:d-matrices}), 
then the matrix $\sqrt{N} D^{(\ell)}(x_0) W^{(\ell)}$ has \emph{isotropic, independent and sub-gaussian rows} (with respect to the randomness in $W^{(\ell)}$ and $b^{(\ell)}$), 
which is shown in \Cref{thm: isotropic_rows,thm: dev_conditions}. 
Here, a random vector $X \in \RR^k$ is called isotropic iff
\begin{equation*}
\EE[X X^T] = I_{k \times k}
\end{equation*}
with $I_{k \times k}$ denoting the $k$-dimensional identity matrix. 

Afterwards, using the \emph{matrix deviation inequality} (see \cite[Theorem~3]{Liaw2017}) 
we show that the product $D^{(L-1)}(x_0)W^{(L-1)} \cdots D^{(0)}(x_0) W^{(0)}$ is almost isometric 
with high probability which then implies the claim. 

We start by showing that for some fixed $x_0 \in \RR^d \setminus \{0\}$ the matrices 
\begin{equation*}
\sqrt{N} D^{(\ell)}(x_0) W^{(\ell)}
\end{equation*}
have isotropic, independent and sub-gaussian rows 
when conditioning on the previous weights and biases $W^{(0)}, ..., W^{(\ell - 1)}, b^{(0)}, ..., b^{(\ell - 1)}$ and assuming $x^{(\ell)} \neq 0$. 
Since $W^{(0)} \in \RR^{N \times d}$ whereas in contrast $W^{(\ell)} \in \RR^{N \times N}$ for $1 \leq \ell \leq L-1$,
we are going to prove the result generally for matrices $W \in \RR^{N \times k}$. 
Let us therefore first introduce the basic assumptions of what follows.
\begin{assumption} \label{assum_1}
Let $W \in \RR^{N \times k}$ be a random matrix and $b \in \RR^N$ a random vector with 
\begin{equation*}
    W_{i,j} \sim \mathcal{N}\left(0, 2/N\right), \quad b_i \sim \mathcal{D}_i, \quad \text{for } 1 \leq i \leq N \quad \text{and} \quad 1 \leq j \leq k,
\end{equation*}
where each $\mathcal{D}_i$ is a symmetric probability distribution on $\RR$. Furthermore, we assume that all the entries of $W$ and $b$ are jointly independent. 
\end{assumption}
First, we show that the rows of the matrix are indeed isotropic, which is done in the following lemma. 
\label{isotropic}
\begin{lemma} \label{thm: isotropic_rows}
Let \Cref{assum_1} be satisfied and fix any vector $x \in \RR^k \setminus \{0\}$. Then each row of
\begin{equation*}
    \sqrt{N} \cdot \diag(Wx + b) \cdot W \in \RR^{N \times k}
\end{equation*}
is an isotropic random vector. Here, $\Delta$ is defined as in \Cref{subsec:gradient}.
\end{lemma}
\begin{proof}
We first consider the case $x = (\alpha,0,...,0)^T \in \RR^k$ with $\alpha \in \RR \setminus \{0\}$. Let $i \in \{1,...,N\}$ and define $V \defeq \sqrt{N} \cdot \diag(Wx + b) \cdot W$. It is well-known that $V_{i,-}$ is an isotropic random vector if and only if
\begin{equation*}
    \EE \langle \left(V_{i,-}\right)^T, y \rangle^2 = \Vert y \Vert_2^2
\end{equation*}
for every $y \in \RR^k$; see \cite[Lemma 3.2.3]{vershynin_high-dimensional_2018}. Therefore, take any arbitrary vector $y \in \RR^k$. A direct calculation yields
\begin{align}
    \EE \left[\langle \left(V_{i,-}\right)^T,y\rangle^2\right] &= \EE  \left[\left(\sum_{\ell=1}^k V_{i,\ell} \hspace{0.05cm} y_\ell\right)^2 \right] = \EE \left[\sum_{j, \ell = 1}^k V_{i,\ell} \hspace{0.05cm} y_\ell V_{i, j}\hspace{0.05cm} y_j \right] = \sum_{j, \ell = 1}^k y_j y_\ell \EE \left[V_{i,j} V_{i,\ell}\right] \nonumber\\
    &= N \cdot \sum_{j, \ell = 1}^k y_j y_\ell \EE \left(\mathbbm{1}_{(Wx + b)_i > 0} \cdot W_{i,j} \cdot W_{i, \ell}\right) \nonumber\\
    \label{eq: first_computation}
    &= N \cdot \sum_{j, \ell = 1}^k y_j y_\ell \EE \left(\mathbbm{1}_{\alpha W_{i,1} + b_i > 0} \cdot W_{i,j} \cdot W_{i, \ell}\right).
\end{align}
If $j \neq \ell$ and w.l.o.g. $j \neq 1$ (since $j \neq 1$ or $\ell \neq 1$, because otherwise $j = 1 = \ell$), it follows by independence that
\begin{equation} \label{eq: second_computation}
    \EE \left(\mathbbm{1}_{\alpha W_{i,1} + b_i> 0} \cdot W_{i,j} \cdot W_{i, \ell}\right) = \underbrace{\EE \left(W_{i,j}\right)}_{=0} \cdot \EE \left(\mathbbm{1}_{\alpha W_{i,1} +b_i> 0} \cdot W_{i, \ell}\right) = 0.
\end{equation}
If $j = \ell \neq 1$, we see
\begin{equation} \label{eq: third_computation}
    \EE \left(\mathbbm{1}_{\alpha W_{i,1} + b_i> 0} \cdot W_{i,j} \cdot W_{i, \ell}\right) = \EE \left(\mathbbm{1}_{\alpha W_{i,1} + b_i> 0}\right) \cdot \EE \left(W_{i,j}^2\right) = \frac{1}{2} \cdot \frac{2}{N} = \frac{1}{N},
\end{equation}
using that $\alpha W_{i,1} + b_i$ has a symmetric and continuous probability distribution.
Here, the continuity follows from $\alpha \neq 0$ and the fact that the random variables $W_{i,1}$ and $b_i$ are independent with $W_{i,1}$ having an absolutely continuous distribution (see, e.g., \cite[Proposition~9.1.6]{dudley2002real}).
If $j = \ell = 1$, we have
\begin{equation*} 
    \EE \left(\mathbbm{1}_{\alpha W_{i,1} + b_i> 0} \cdot W_{i,j} \cdot W_{i, \ell}\right) = \EE \left(\mathbbm{1}_{\alpha W_{i,1} + b_i > 0} \cdot W_{i,1}^2\right).
\end{equation*}
For simplicity, we write $X = W_{i,1}$ and $Y = b_i$ and note since $(X,Y) \sim (-X, -Y)$ (since $X$ and $Y$ are independent and both symmetrically distributed) that
\begin{equation*}
    \EE \left(\mathbbm{1}_{\alpha X+Y > 0} \cdot X^2\right) = \EE \left(\mathbbm{1}_{-\alpha X-Y > 0} \cdot (-X)^2\right) = \EE \left(\mathbbm{1}_{\alpha X+Y < 0} \cdot X^2\right).
\end{equation*}
This yields
\begin{align}
    \EE \left(\mathbbm{1}_{\alpha X+Y > 0} \cdot X^2\right) &= \frac{1}{2} \left(\EE \left(\mathbbm{1}_{\alpha X+Y > 0} \cdot X^2\right) + \EE \left(\mathbbm{1}_{\alpha X+Y < 0} \cdot X^2\right)\right) \nonumber\\
    \label{eq: fourth_computation}
    &= \frac{1}{2}\EE \left(\left(\mathbbm{1}_{\alpha X+Y > 0}+\mathbbm{1}_{\alpha X+Y < 0}\right) \cdot X^2\right) = \frac{1}{2}\EE (X^2) = 1/N.
\end{align}
Here, we used that $\EE (\mathbbm{1}_{\alpha X + Y = 0} \cdot X^2) = 0$, since $\alpha X$ has an absolutely continuous distribution (note $\alpha \neq 0$) and $X$ and $Y$ are independent so that $\PP (\alpha X + Y = 0) =0$. 
Inserting \eqref{eq: second_computation}, \eqref{eq: third_computation} and \eqref{eq: fourth_computation} into \eqref{eq: first_computation} yields 
\begin{equation*}
    \EE \langle \left(V_{i,-}\right)^T, y\rangle^2 = \left\Vert y \right\Vert_2^2,
\end{equation*}
hence the rows of $V$ are isotropic in the case $x = (\alpha,0,...,0)^T \in \RR^k$ with $\alpha \in \RR \setminus \{0\}$.

Now consider an arbitrary vector $x \in \RR^k \setminus \{0\}$. 
Taking an orthogonal deterministic matrix $U \in \RR^{k \times k}$ with $x =  U\alpha e_1$ for some $\alpha \in \RR \setminus \{0\}$ yields
\begin{equation*}
    \diag(Wx + b) \cdot W = \diag\left(WU\alpha e_1 + b\right) \cdot WU \cdot U^{-1}.
\end{equation*}
For every $j \in \{1,...,N\}$ we see
\begin{align*}
\left((WU)^T\right)_{-,j} = \left(U^T W^T\right)_{-,j} = U^T \left(W^T \right)_{-,j } \sim \mathcal{N}\left(0, \frac{2}{N}I_k \right),
\end{align*}
where in the last step we used the rotation invariance of the Gaussian distribution \cite[Proposition 3.3.2]{vershynin_high-dimensional_2018}. This yields
\begin{equation*}
(WU)_{j,-} \sim \mathcal{N}\left(0, \frac{2}{N}I_k\right)
\end{equation*}
for every $j \in \{1,...,N\}$. Since $(WU)_{j,-}$ only depends on $W_{j,-}$, this implies
\begin{equation*}
(WU)_{i,j} \iid \mathcal{N}(0, 2/N) \quad \text{for all } i \in \{1,...,N\}, j \in \{1,...,k\}.
\end{equation*}
Hence, the matrix $V \defeq \diag\left((WU) \alpha e_1 + b \right) \cdot (WU)$ has isotropic rows by what has just been shown (the case $x = (\alpha, 0, ..., 0)^T$).
But then $V \cdot U^{-1}$ has isotropic rows too. Indeed, let $i \in \{1,...,N\}$. Then we have $\left(\left(V \cdot U^{-1}\right)_{i,-}\right)^T = U \cdot \left(V_{i,-}\right)^T$ where $\left(V_{i,-}\right)^T$ is an isotropic random vector. Writing $Z \defeq \left(V_{i,-}\right)^T$, we see for any vector $ y \in \RR^k$ that
\begin{equation*}
    \EE \langle UZ , y \rangle^2= \EE \langle Z, U^T y \rangle^2 = \left\Vert U^T y \right\Vert_2^2 = \left\Vert y \right\Vert_2 ^2,
\end{equation*}
so $UZ$ is isotropic as was to be shown.
\end{proof}
Our next lemma collects all of the properties of the matrices $\sqrt{N}D^{(\ell)}(x_0) W^{(\ell)}$ that we will need. 
\begin{lemma} \label{thm: dev_conditions}
    Let \Cref{assum_1} be satisfied and fix $x \in \RR^k \setminus \{0\}$. Let $\Delta(v), \ v\in \RR^N$, be as defined in \Cref{subsec:gradient}. Then the rows of $\sqrt{N} \diag(Wx + b) \cdot W$ are jointly independent, isotropic random vectors, and sub-gaussian with
    \begin{equation*}
        \left\Vert \left(\sqrt{N} \cdot \diag(Wx + b) \cdot W\right)_{i,-}\right\Vert_{\psi_2} \leq C \quad \text{for all } i \in \{1,...,N\},
    \end{equation*}
    where $C>0$ is an absolute constant. 
\end{lemma}
\begin{proof}
    Note that
    \begin{equation*}
        \left(\sqrt{N} \diag(Wx + b) \cdot W\right)_{i,-} = \sqrt{N} \cdot \mathbbm{1}_{(Wx+b)_i > 0} \cdot W_{i,-},
    \end{equation*}
    which only depends on $b_i$ and the $i$-th row of $W$, which implies that the rows of this matrix are jointly independent. 
    For every vector $y \in \RR^k$ and every $i \in \{1,...,N\}$ we see
    \begin{align*}
    	 \abs{\left(\sqrt{N} \diag(Wx + b) \cdot W\right)_{i,-} \cdot y } &= \abs{\sum_{j=1}^k \left(\diag(Wx + b) \cdot \sqrt{N}W\right)_{i,j}y_j} = \abs{\sum_{j=1}^k \mathbbm{1}_{(Wx + b)_i > 0} \cdot \sqrt{N}W_{i,j}y_j} \\
    	 &= \mathbbm{1}_{(Wx + b)_i > 0} \cdot \abs{\sum_{j = 1}^k \sqrt{N}W_{i,j}y_j} \leq \abs{\sum_{j = 1}^k \sqrt{N}W_{i,j}y_j} .
    \end{align*}
    Note that the $W_{i,j}y_j$ ($j \in \{1,...,k\}$) are independent with $\sqrt{N}W_{i,j}y_j \sim \mathcal{N}(0, 2 y_j^2)$ and hence
    \begin{equation*}
    	\sum_{j = 1}^k \sqrt{N}W_{i,j}y_j \sim \mathcal{N}(0, 2 \cdot \Vert y \Vert_2^2).
    \end{equation*}
    Hence, by definition (see \cite[Section~2.5.2~and~Definition~3.4.1]{vershynin_high-dimensional_2018}) it follows that the random variable $\left(\sqrt{N} \diag(Wx + b) \cdot W\right)_{i,-} \cdot y$ is sub-gaussian and since $y \in \RR^k$ has been chosen arbitrarily we deduce that $\left(\sqrt{N} \diag(Wx + b) \cdot W\right)_{i,-}$ is sub-gaussian with
    \begin{equation*}
    \left\Vert \left(\sqrt{N} \diag(Wx + b) \cdot W\right)_{i,-}\right\Vert_{\psi_2} \leq \underset{y \in \mathbb{S}^{k-1} }{\sup} \left\Vert \sum_{j = 1}^k \sqrt{N}W_{i,j}y_j\right\Vert_{\psi_2} \leq \sqrt{2} \cdot C_1 =: C,
    \end{equation*}
    where $C_1$ is an absolute constant according to \cite[Example 2.5.8 (i)]{vershynin_high-dimensional_2018}.
    
    The isotropy has already been shown in \Cref{thm: isotropic_rows}.
\end{proof}

We now turn to the proof of the lower bound in the case of deep networks. 
For what follows, we assume that the considered $\relu$ network satisfies \Cref{assum:2}. 
We take a \emph{fixed} vector 
\begin{equation*}
x_0\defeq x^{(0)} \in \RR^d \setminus \{0\}
\end{equation*}
 and define the matrices $D^{(0)}(x_0),..., D^{(L-1)}(x_0)$ 
as introduced in \Cref{subsec:gradient}. 
Since $x_0$ is a fixed vector, we omit the argument and just write $D^{(\ell)}$ instead of $D^{(\ell)}(x_0)$. 

First, we prove that the product matrix $D^{(L-1)} W^{(L-1)} \cdots D^{(0)} W^{(0)}$ is almost isometric with high probability. 
This will be based on the fact that the rows of $\sqrt{N}D^{(\ell)}W^{(\ell)}$ are independent, isotropic random vectors (see \Cref{thm: dev_conditions}). 
However, in order to guarantee these properties, we have to make sure that the output of the previous layer $x^{(\ell)}$ is \emph{not} zero. 
Hence, in the following proposition we carefully keep track of this event as well. 
Moreover, since the ultimate goal is to apply \Cref{eq:upbound}, we have to ensure that the network $\Phi$ is differentiable at $x_0$ with
\begin{equation*}
\left(\nabla \Phi(x_0)\right)^T = W^{(L)}\cdot  D^{(L-1)} \cdot W^{(L-1)}\cdots D^{(0)} \cdot W^{(0)},
\end{equation*}
which is satisfied if all pre-activations throughout the network are non-zero. This is why we also consider this event in the following proposition.
\begin{proposition}\label{prop:gell}
Let $W^{(0)},..., W^{(L)}$ and $b^{(0)},..., b^{(L)}$ as in \Cref{assum:2}, and let $x^{(0)} \in \RR^d$ be fixed and let $D^{(0)},..., D^{(L-1)}$ and $x^{(1)},..., x^{(L)}$ as in \Cref{subsec:gradient}. For every $C>0, u \geq 0$ and $\ell \in \{1,...,L\}$, we write $\gell = \gell(u, C)$ for the event defined via the following three properties:
\begin{enumerate}
\item $(W^{(\ell')}x^{(\ell')} + b^{(\ell')})_i \neq 0$ \quad  for all $\ell' \in \{0,..., \ell-1\}$ and $i \in \{1,...,N\}$,
\item $x^{(\ell')} \neq 0$\quad for all $\ell' \in \{0,..., \ell\}$,
\item $\displaystyle \left(\left(1 - \frac{C \!\cdot\! (\sqrt{d} + u)}{\sqrt{N}}\right)_+\right) ^{\ell}\Vert y \Vert_2 \leq \Vert D^{(\ell-1)}W^{(\ell-1)}\cdots D^{(0)}W^{(0)}y \Vert_2 \leq \left( 1+ \frac{C \!\cdot \!(\sqrt{d} + u)}{\sqrt{N}}\right)^\ell \Vert y \Vert_2 $ holds uniformly over all $y \in \RR^d$.
\end{enumerate}
Recall that $a_+ = \max\{a,0\}$ for any $a \in \RR$.
Then, there exists an absolute constant $C>0$ with 
\begin{equation*}
\PP (\mathcal{G}^{(\ell)} ) = \PP (\mathcal{G}^{(\ell)}(u,C) )\geq \left(\left(1 - \frac{1}{2^N} - \exp(-u^2)\right)_+\right)^\ell
\end{equation*} 
for every $u \geq 0$.
\end{proposition}
\begin{proof}
The proof is via induction over $\ell$, where the constant $C$ is determined later. We note that for fixed $\ell$, the defining conditions of $\gell$ only depend on the weights $W^{(0)},..., W^{(\ell - 1)}$ and the biases $b^{(0)},..., b^{(\ell - 1)}$. 

We start with the case $\ell = 1$. We denote
\begin{align*}
A(1) &\defeq \left\{(W^{(0)},..., W^{(\ell - 1)}, b^{(0)}, ..., b^{(\ell - 1)}): \ \text{(1) is satisfied}\right\} \\
B(1) &\defeq \left\{(W^{(0)},..., W^{(\ell - 1)}, b^{(0)}, ..., b^{(\ell - 1)}): \ \text{(2) is satisfied}\right\} \\
C(1) &\defeq \left\{(W^{(0)},..., W^{(\ell - 1)}, b^{(0)}, ..., b^{(\ell - 1)}): \ \text{(3) is satisfied}\right\}.
\end{align*}
For every $i \in \{1,...,N\}$, we have, using $\ast$ to denote the convolution of two measures,
\begin{equation*}
\left(W^{(0)} x^{(0)} + b^{(0)} \right)_i \sim \mathcal{N}(0, 2/N \cdot \Vert x^{(0)} \Vert_2^2) \ast \mathcal{D}^{(0)}_i,
\end{equation*}
where the latter is an absolutely continuous and symmetric probability distribution. Note that we assume $x^{(0)} \neq 0$. Hence, we first conclude from the joint independence of the above random variables that
\begin{equation*}
\PP^{(W^{(0)}, b^{(0)})} \left(A(1)\right) = 1.
\end{equation*}
Moreover, we have
\begin{equation*}
(W^{(0)}, b^{(0)}) \notin B(1) \quad \Longleftrightarrow \quad \left(W^{(0)} x^{(0)} + b^{(0)} \right)_i \leq 0 \quad \text{for all } i \in \{1,...,N\}.
\end{equation*}
From the joint independence, the symmetry and the fact that the random variables $\left(W^{(0)} x^{(0)} + b^{(0)} \right)_i$ follow an absolutely continuous distribution for every $i$, we infer
\begin{equation*}
\PP^{(W^{(0)},b^{(0)})} \left(B(1)^c\right) = \frac{1}{2^N}.
\end{equation*}
Moreover, note that $\sqrt{N}D^{(0)}W^{(0)} = \sqrt{N}\diag(W^{(0)}x^{(0)}+ b^{(0)}) W^{(0)}$ is a matrix with independent isotropic sub-gaussian rows according to \Cref{thm: dev_conditions}. Therefore, the high probability version of the matrix deviation inequality (see \cite[Theorem 3]{Liaw2017}) yields
\begin{equation*}
\underset{y \in \overline{B}_d(0,1)}{\sup} \left\vert \Vert \sqrt{N}D^{(0)}W^{(0)}y\Vert_2 - \sqrt{N} \Vert y \Vert_2 \right\vert \leq C (\sqrt{d} + u)
\end{equation*}
with probability at least $ 1-\exp(- u ^2)$, where we employed \Cref{prop:gauss_width}. $C> 0$ is an absolute constant according to \Cref{thm: dev_conditions}. This yields
\begin{equation*}
\left(1 - \frac{C(\sqrt{d} + u)}{\sqrt{N}}\right)_+\Vert y \Vert_2 \leq \Vert D^{(0)}W^{(0)}y \Vert_2 \leq \left( 1+ \frac{C(\sqrt{d} + u)}{\sqrt{N}}\right) \Vert y \Vert_2 \quad \text{ for all } y \in \RR^d
\end{equation*}
with probability at least $1 - \exp(-u^2)$. This gives us $\PP(C(1)^c) \leq \exp(-u^2)$. This gives us in total
\begin{equation*}
\PP^{(W^{(0)},b^{(0)})}\left(\mathcal{G}^{(1)}\right) \geq 1 - \PP\left(A(1)^c\right)-\PP\left(B(1)^c\right)-\PP\left(C(1)^c\right) \geq 1 - 0 - \frac{1}{2^N} - \exp(-u^2).
\end{equation*}

Fix $1 \leq \ell < L$, set $V' \defeq  D^{(\ell-1)} W^{(\ell-1)} \cdots D^{(0)} W^{(0)}$ and write $\overset{\rightarrow}{W} \defeq (W^{(0)}, ..., W^{(\ell - 1)})$ for the tuple of the first $\ell$ weight matrices and $\overset{\rightarrow}{b} \defeq (b^{(0)}, ..., b^{(\ell - 1)})$ for the tuple of the first $\ell$ bias vectors and assume by induction that
\begin{equation} \label{eq: ind_hyp}
\PP\left((\arrow{W}, \arrow{b}) \in \gell\right) \geq  \left(\left(1 - \frac{1}{2^N} - \exp(-u^2)\right)_+\right)^\ell.
\end{equation} 
 Furthermore, we write
\begin{align*}
&A(\arrow{W}, \arrow{b}) \defeq \left\{ (W^{(\ell)}, b^{(\ell)}) : \ \left(W^{(\ell)}x^{(\ell)} + b^{(\ell)}\right)_i \neq 0 \text{ for all } i \in \{1,...,N\}\right\} \\
 &B(\arrow{W}, \arrow{b}) \defeq \left\{(W^{(\ell)}, b^{(\ell)}): \ x^{(\ell + 1)} = \relu(W^{(\ell)}x^{(\ell)} + b^{(\ell)}) \neq 0\right\} \\
 & C (\overset{\rightarrow}{W}, \overset{\rightarrow}{b}) \defeq \\
 &\left\{ (W^{(\ell )}, b^{(\ell)}):  \! \left(1\! - \!\frac{C(\sqrt{d}\! + \! u)}{\sqrt{N}}\right)_+ \! \Vert V'y \Vert_2 \!\leq \!\Vert D^{(\ell)}W^{(\ell)}V'y\Vert_2 \!\leq\! \left( 1\!+ \!\frac{C(\sqrt{d} \!+ \!u)}{\sqrt{N}}\right)\Vert V'y \Vert_2 \  \text{for all } \! y \! \in \! \RR^d\right\} 
\end{align*}
for each $\overset{\rightarrow}{W}, \overset{\rightarrow}{b}$, where we note that $V'$ is a function of only $\arrow{W}$ and $\arrow{b}$ (and $x^{(0)}$, which is fixed).
Then we see that
\begin{equation*} 
(\arrow{W}, \arrow{b}) \in \gell, \ (W^{(\ell)}, b^{(\ell)}) \in A(\arrow{W}, \arrow{b}) \cap B(\arrow{W}, \arrow{b}) \cap C(\arrow{W}, \arrow{b}) \quad \Longrightarrow \quad (\arrow{W}, W^{(\ell)}, \arrow{b}, b^{(\ell)}) \in \gelll.
\end{equation*}
In view of \Cref{prop:highprob}, we thus seek to bound 
\begin{equation*}
\PP^{(W^{(\ell)}, b^{(\ell)})} \left(A (\overset{\rightarrow}{W}, \overset{\rightarrow}{b}) \cap B(\arrow{W}, \arrow{b}) \cap C(\arrow{W}, \arrow{b})\right)
\end{equation*}
 from below, where $ (\overset{\rightarrow}{W}, \overset{\rightarrow}{b}) \in \gell$ is fixed. 
 
 To this end, we consider the sets $A(\arrow{W}, \arrow{b}), B(\arrow{W}, \arrow{b}), C(\arrow{W}, \arrow{b})$ individually. Note that the vector $x^{(\ell)}$ is fixed and from $(\arrow{W}, \arrow{b}) \in \gell$ we infer $x^{(\ell)} \neq 0$. Hence,
 \begin{equation*}
 \left(W^{(\ell)} x^{(\ell)} + b^{(\ell)} \right)_i \sim \mathcal{N}(0, 2/N \cdot \Vert x^{(\ell)} \Vert_2^2) * \mathcal{D}^{(\ell)}_i
 \end{equation*}
 for every $i \in \{1,...,N\}$, where the latter is again a symmetric and (absolutely) continuous probability distribution. Similar to the case $\ell = 1$, this gives us 
 \begin{equation*}
 \PP^{(W^{(\ell)}, b^{(\ell)})} \left(A(\arrow{W}, \arrow{b})\right) = 1.
 \end{equation*}
 Moreover, as in the case $\ell = 1$, we get 
\begin{equation*}
(W^{(\ell)}, b^{(\ell)}) \notin B(\arrow{W}, \arrow{b}) \quad \Longleftrightarrow \quad \left(W^{(\ell)} x^{(\ell)} + b^{(\ell)} \right)_i \leq 0 \quad \text{for all } i \in \{1,...,N\}.
\end{equation*}
 Again, from the joint independence, the symmetry and the absolute continuity of the distribution of the random variables 
 \begin{equation*}
 \left(W^{(\ell)} x^{(\ell)} + b^{(\ell)} \right)_i , 
 \end{equation*}
 we get
 \begin{equation*}
 \PP^{(W^{(\ell)}, b^{(\ell)})} \left(B(\arrow{W}, \arrow{b})^c\right) = \frac{1}{2^N}.
 \end{equation*}
 Lastly, according to \Cref{thm: dev_conditions}, the matrix
\begin{equation*}
\sqrt{N} D^{(\ell)}W^{(\ell)} = \sqrt{N} \diag(W^{(\ell)}x^{(\ell)} + b^{(\ell)})W^{(\ell)}
\end{equation*}
has independent, sub-gaussian, isotropic rows. Hence, we may again apply the high-probability version of the matrix deviation inequality (\cite[Theorem 3]{Liaw2017}) and obtain
\begin{equation*}
\underset{y \in \IM(V') \cap \overline{B}_N(0,1)}{\sup} \left\vert \sqrt{N} \Vert D^{(\ell)}W^{(\ell)}y\Vert_2 - \sqrt{N} \Vert y \Vert_2\right\vert \leq C (\sqrt{d} + u) 
\end{equation*}
with probability at least $ 1 - \exp(-u^2)$, where we again used \Cref{prop:gauss_width}, noting that the subspace $\IM(V')$ is at most $d$-dimensional since
\begin{equation*}
V' =  D^{(\ell-1)} W^{(\ell-1)} \cdot \cdot \cdot D^{(0)} W^{(0)} \in \RR^{N \times d}.
\end{equation*}
But this directly implies
\begin{equation*}
\left(1- \frac{C(\sqrt{d} + u)}{\sqrt{N}}\right)_+ \Vert y \Vert_2 \leq \Vert D^{(\ell)}W^{(\ell)}y \Vert_2 \leq \left(1+ \frac{C(\sqrt{d} + u)}{\sqrt{N}}\right) \Vert y \Vert_2 \quad \forall y \in \IM(V')
\end{equation*}
with probability at least $1 - \exp(-u^2)$, which means
\begin{equation*}
\PP^{(W^{(\ell)}, b^{(\ell)})} (C (\overset{\rightarrow}{W}, \overset{\rightarrow}{b})) \geq 1-  \exp(-u^2).
\end{equation*}
Hence, we get
\begin{align*}
&\norel\PP^{(W^{(\ell)}, b^{(\ell)})} \left(A (\overset{\rightarrow}{W}, \overset{\rightarrow}{b}) \cap B(\arrow{W}, \arrow{b}) \cap C(\arrow{W}, \arrow{b})\right) \\
&\geq 1 -  \PP^{(W^{(\ell)}, b^{(\ell)})}\left(A (\overset{\rightarrow}{W}, \overset{\rightarrow}{b})^c\right) - \PP^{(W^{(\ell)}, b^{(\ell)})}\left(B (\overset{\rightarrow}{W}, \overset{\rightarrow}{b})^c\right) - \PP^{(W^{(\ell)}, b^{(\ell)})}\left(C (\overset{\rightarrow}{W}, \overset{\rightarrow}{b})^c\right) \\
&\geq 1 - 0 - \frac{1}{2^N} - \exp(-u^2) = 1 - \frac{1}{2^N} -\exp(-u^2)
\end{align*}
for every fixed $(\arrow{W}, \arrow{b}) \in \gell$.
Using \Cref{prop:highprob}, we obtain
\begin{align*}
\PP \left(\gelll \right) \geq \left(1 - \frac{1}{2^N} -\exp(-u^2)\right)_+ \cdot \PP^{(\arrow{W}, \arrow{b})}(\gell) \geq \left(\left(1 - \frac{1}{2^N} -\exp(-u^2)\right)_+\right)^{\ell + 1},
\end{align*}
as was to be shown. Here, we applied the $+$-operator since probabilities are non-negative. 
\end{proof}
To finalize our result, we introduce randomness in $W^{(L)}$ as well and use an argument based on the singular value decomposition of the product $D^{(L-1)}W^{(L-1)} \cdots D^{(0)}W^{(0)}$.
\begin{proposition} \label{prop:grad_lower}
Let $\Phi$ and $W^{(0)},..., W^{(L)}$ as well as $b^{(0)},..., b^{(L)}$ as in \Cref{assum:2} and let $x_0 \defeq x^{(0)} \in \RR^d$ be fixed. Let $D^{(\ell)} \defeq D^{(\ell)}(x_0)$ for $0 \leq \ell \leq L-1$ with $D^{(\ell)}(x_0)$ as in \Cref{subsec:gradient}. We let $k \defeq \min  \{N,d\}, \  u \geq 0, C> 0$ and $t \geq 0$ and define the event $\mathcal{A} = \mathcal{A}(u,t,C)$ via the properties 
\begin{enumerate}
\item{\begin{align*}
\left(\left(1 - \frac{C\cdot (\sqrt{d} + u)}{\sqrt{N}}\right)_+\right)^{L} \cdot (\sqrt{k} - t) &\leq\Vert W^{(L)} D^{(L-1)} W^{(L-1)} \cdot \cdot \cdot D^{(0)} W^{(0)} \Vert_2 \\
&\leq \left(1 + \frac{C\cdot(\sqrt{d} + u)}{\sqrt{N}}\right)^{L} \cdot (\sqrt{k} + t),
\end{align*} }
\item{$\Phi$ is differentiable at $x_0$ with 
\begin{equation*}
\left(\Phi(x_0)\right)^T =W^{(L)} \cdot D^{(L-1)} \cdot W^{(L-1)}\cdots D^{(0)} \cdot W^{(0)}.
\end{equation*}}
\end{enumerate}
Then there exist absolute constants $C, c_1 > 0$ such that 
\begin{equation*}
\PP\left(\mathcal{A}\right) = \PP\left(\mathcal{A}(u,t,C)\right) \geq \left(\left(1 - \frac{1}{2^N}-\exp(-u^2)\right)_+\right)^L \cdot (1- 2 \exp (-c_1 t^2))_+
\end{equation*}
for every $u,t \geq 0$.
\end{proposition}
\begin{proof}
We again denote $V' \defeq D^{(L-1)}W^{(L-1)}\cdot \cdot \cdot D^{(0)}W^{(0)}$. We decompose $V' = U \Sigma Q^T$ with orthogonal matrices $U \in \RR^{N \times N}$ and $Q \in \RR^{d \times d}$ and a matrix $\Sigma \in \RR^{N \times d}$ of the form 
\begin{equation*}
\Sigma = \left(\begin{matrix} \sigma_1 & & \\
				& \ddots & \\
				& & \sigma_d \\
				\hline 
				0 & \hdots & 0 \\
				\vdots & \ddots & \vdots \\
				0 & \hdots & 0  \end{matrix} \right)  \quad \text{if $N \geq d$} \qquad \text{or} \qquad \Sigma = \left(\begin{array}{ccc|ccc} \sigma_1 & &  & 0 & \hdots & 0\\
				& \ddots &  & \vdots & \ddots & \vdots \\
				& & \sigma_N & 0 & \hdots & 0
				 \end{array} \right) \quad \text{if $N \leq d$}
\end{equation*}
with $\sigma_1 \geq ... \geq \sigma_k \geq 0$ (singular value decomposition). Hence, recalling that $W^{(L)} \in \RR^{1 \times N}$, we get
\begin{align}
	\Vert W^{(L)} V'\Vert_2  &= \Vert  W^{(L)} U \Sigma Q^T \Vert_2 = \Vert W^{(L)} U \Sigma \Vert _2 = \left\Vert \left( W^{(L)}U \Sigma\right)^T \right\Vert_2 \nonumber\\ 
	&= \sqrt{ \sum_{i= 1}^k \left(\sigma_i \cdot \left(W^{(L)}U\right)_i\right)^2}
	\label{eq: firstbound} \ 
	 \begin{cases} \leq \sigma_1 \cdot \Vert W' \Vert_2, \\  \geq \sigma_k \cdot \Vert W' \Vert_2 \end{cases}
\end{align}
with $W' \in \RR^{1 \times k}$ denoting the vector of the first $k$ entries of $W^{(L)}U$. 

Let $C>0$ be the absolute constant from \Cref{prop:gell}. We denote $\overset{\rightarrow}{W} \defeq (W^{(0)}, ..., W^{(L-1)})$ and $\overset{\rightarrow}{b} \defeq (b ^{(0)}, ..., b^{(L-1)})$ and fix $u,t \geq0$. Furthermore, let
\begin{align*}
\mathcal{B}(\overset{\rightarrow}{W}, \overset{\rightarrow}{b})\defeq \left\{ W^{(L)}: \ \sqrt{d} + t \geq \Vert W'(\arrow{W}, \arrow{b}, W^{(L)}) \Vert_2 \geq \sqrt{d} - t\right\},
\end{align*}
where we wrote $W'(\arrow{W}, \arrow{b}, W^{(L)})$ to emphasize the dependence of $W'$ on $\arrow{W}, \arrow{b}$ and $W^{(L)}$.

Note that for $(\arrow{W}, \arrow{b}) \in \mathcal{G}^{(L)}$ (where $\mathcal{G}^{(L)}$ is as in \Cref{prop:gell}), property $(2)$ follows directly and independent of the choice of $W^{(L)}$ and $b^{(L)}$. This is due to defining property (1) in \Cref{prop:gell} and the fact that the $\relu$ is differentiable on $\RR \setminus \{0\}$. Hence, from \eqref{eq: firstbound} we infer
\begin{equation} \label{eq: secondbound}
(\arrow{W}, \arrow{b}) \in \mathcal{G}^{(L)}, \ W^{(L)} \in \mathcal{B}(\arrow{W}, \arrow{b}) \quad \Longrightarrow \quad (\arrow{W}, \arrow{b}, W^{(L)}) \in \mathcal{A},
\end{equation}
where we also applied \cite[Equation (4.5)]{vershynin_high-dimensional_2018}.
 From \Cref{prop:gell} we deduce 
\begin{equation} \label{eq: probbound}
\PP^{(\arrow{W}, \arrow{b})} (\mathcal{G}^{(L)}) \geq \left(\left(1- \frac{1}{2^N}-\exp(-u^2)\right)_+\right)^L.
\end{equation}
Furthermore, for fixed $\arrow{W}$ and $\arrow{b}$, the rotation invariance of the Gaussian distribution \cite[Proposition 3.3.2]{vershynin_high-dimensional_2018} implies
\begin{equation*}
W^{(L)} U \sim W^{(L)}.
\end{equation*}
Therefore, $W'$ is a $k$-dimensional random vector with $(W')^T \sim \mathcal{N}(0,I_k)$. Thus, \cite[Theorem 3.1.1]{vershynin_high-dimensional_2018} yields
\begin{equation*}
\left\Vert \Vert W'\Vert_2 - \sqrt{k}\right\Vert_{\psi_2} \leq C_2 \quad \text{(conditioned on $\arrow{W},\arrow{b}$)}
\end{equation*}
with an absolute constant $C_2 > 0$. 
From \cite[Equation (2.14)]{vershynin_high-dimensional_2018} we get
\begin{equation*}
\PP^{W^{(L)}} \left(\left\vert \Vert W' \Vert_2 - \sqrt{k}\right\vert \geq t\right) \leq 2 \exp (-c_1t^2) \quad \text{for fixed } \arrow{W}, \arrow{b}
\end{equation*}
with an absolute constant $c_1 > 0$,
and hence
\begin{equation*} 
\PP^{W^{(L)}} \left(\sqrt{k} + t \geq \Vert W'\Vert_2 \geq (\sqrt{k} - t)_+ \right) \geq 1 - 2 \exp(-c_1 t^2) \quad \text{for fixed } \arrow{W}, \arrow{b}.
\end{equation*}

From \Cref{prop:highprob} and \eqref{eq: secondbound}, we see 
\begin{align*}
\PP^{(\arrow{W}, \arrow{b}, W^{(L)})}(\mathcal{A}) &\geq (1 - 2 \exp(-c_1 t^2))_+ \cdot \PP^{(\arrow{W}, \arrow{b})} (\mathcal{G}^{(L)}) \\
\overset{\eqref{eq: probbound}}&{\geq} (1 - 2 \exp(-c_1 t^2))_+ \cdot \left(\left(1 -  \frac{1}{2^N}-\exp(-u^2)\right)_+\right)^L,
\end{align*}
as was to be shown.
\end{proof}
We remark that the previous result, in addition to providing a lower bound on the Lipschitz constant of random neural networks, is of independent interest on its own,
since it provides a lower and \emph{upper} bound on the gradient of a random ReLU network at a fixed point $x_0\neq 0$. 
It is an interesting question whether this \emph{pointwise} estimate can be used to get a uniform estimate that holds for every point $x^{(0)}$,
thus yielding an \emph{upper} bound on the Lipschitz constant as well.

Moreover, we note that one can even show that a random $\relu$ network $\Phi$ is \emph{almost surely} differentiable with 
\begin{equation*}
\nabla \Phi (x_0)^T = W^{(L)} \cdot D^{(L-1)} \cdots D^{(0)} \cdot W^{(0)}
\end{equation*} 
at any fixed point $x_0 \neq 0$ (and not only with high probability as stated in \Cref{prop:grad_lower}). 
A proof of this fact (which we expect to be of independent interest) is contained in \Cref{app:diff}.

\Cref{prop:grad_lower} and \Cref{eq:upbound} directly give us the following lower bound on the Lipschitz constant of random ReLU networks.
\begin{theorem}\label{thm:low_bound_ut}
There exist absolute constants $C, c_1 > 0$ with the following property: If $\Phi:\RR^d \to \RR$ is a random ReLU network of width $N$ and with $L$ hidden layers according to the random initialization as described in \Cref{assum:2}, then for any $u,t \geq 0$, writing $k \defeq \min\{d,N\}$, it holds
\begin{equation*}
\lip(\Phi) \geq \left(\left(1 - \frac{C\cdot(\sqrt{d} + u)}{\sqrt{N}}\right)_+\right)^{L} \cdot (\sqrt{k} - t) 
\end{equation*}
with probability at least $ \left(\left(1 - \frac{1}{2^N}-\exp(-u^2)\right)_+\right)^L \cdot (1- 2 \exp (-c_1 t^2))_+$.
\end{theorem}

 By plugging in special values for $t$ and $u$ and assuming $N \gtrsim dL^2$, we can now prove the main result.

\renewcommand*{\proofname}{Proof of \Cref{thm:main_3}}
\begin{proof}
Let $\widetilde{C}$ and $\widetilde{c_1}$ be the relabeled constants from \Cref{thm:low_bound_ut}. We can clearly assume $\widetilde{C} \geq 1$. We define the new constants $C \defeq (4\widetilde{C} )^2$ and $c_1 \defeq \widetilde{c_1}/4$. We assume $N \geq C \cdot d \cdot L^2$ and let $u = \frac{\sqrt{N}}{4\widetilde{C}L}$ and $t = \sqrt{d}/2.$ Note that $N \geq CdL^2$ is equivalent to
\begin{equation*}
\sqrt{d} \leq \frac{\sqrt{N}}{4\widetilde{C}L}.
\end{equation*}
We get
\begin{equation*}
\widetilde{C} \cdot \left(\sqrt{d} + u\right) =\widetilde{C} \cdot \left(\sqrt{d} + \frac{\sqrt{N}}{4\widetilde{C}L}\right) \leq \widetilde{C} \cdot \left(\frac{\sqrt{N}}{4\widetilde{C}L} + \frac{\sqrt{N}}{4\widetilde{C}L}\right) = \frac{\sqrt{N}}{2L} \leq \sqrt{N}
\end{equation*}
and hence
\begin{equation*}
1 - \frac{\widetilde{C}\cdot (\sqrt{d} + u)}{\sqrt{N}} \geq 0.
\end{equation*}
Moreover, $N \geq C \cdot d \cdot L^2 \geq d$ and thus $k = \min \{N,d\} = d$. 
Therefore, \Cref{thm:low_bound_ut} yields
\begin{equation*}
\lip(\Phi)\geq \left(1 - \frac{\widetilde{C}(\sqrt{d}+u) }{\sqrt{N}}\right)^{L} \cdot \frac{1}{2} \cdot \sqrt{d}
\end{equation*}
with probability at least 
\begin{align*}
&\norel\left(\left(1 -  \frac{1}{2^N}-\exp(-u^2)\right)_+\right)^L \cdot (1- 2 \exp (-\widetilde{c_1} d / 4))_+\\
&= \left(1 - \frac{1}{2^N}-\exp(-N/(CL^2))\right)^L \cdot (1- 2 \exp (-c_1 d ))_+.
\end{align*} 
Here, we implicitly used that 
\begin{equation*}
1 - \frac{1}{2^N}-\exp(-N/(CL^2)) \geq 1 - \frac{1}{2^N} - \exp(-d) \geq 0,
\end{equation*}
where we employed $N \geq CdL^2$.
Moreover, note that 
\begin{equation*}
\frac{\widetilde{C}\cdot (\sqrt{d} + u)}{\sqrt{N}} = \widetilde{C} \cdot \left(\frac{\sqrt{d}}{\sqrt{N}} + \frac{1}{4\widetilde{C}L}\right) \leq \widetilde{C} \cdot \left(\frac{1}{4\widetilde{C}L}+\frac{1}{4\widetilde{C}L}\right) = \frac{1}{2L},
\end{equation*}
which yields
\begin{equation*}
 \left(1 - \frac{\widetilde{C}\cdot (\sqrt{d}+u) }{\sqrt{N}}\right)^{L} \geq \left( 1 - \frac{1}{2L}\right)^L \geq 1/2 
\end{equation*}
for every $L \in \NN$, which follows from Bernoulli's inequality. This gives us (1).

For the expectation bound, note that by Markov's inequality we have
\begin{equation*}
\EE[\lip(\Phi)] \geq \PP(\lip(\Phi) \geq \sqrt{d}/4) \cdot \frac{\sqrt{d}}{4} \geq \left(\left(1-\frac{1}{2^N}-\exp(-u^2)\right)^L\cdot (1-2\exp(-c_1d))\right)\cdot \frac{\sqrt{d}}{4}.
\end{equation*}
First note that there is an absolute constant $c_3>0$ with 
\begin{equation*}
1-2\exp(-c_1d) \geq c_3
\end{equation*}
for every $d \geq \left\lfloor \frac{\ln(2)}{c_1}\right\rfloor + 1$. Therefore, it remains to find a uniform bound for 
\begin{equation*}
\left(1-\frac{1}{2^N}-\exp(-u)\right)^L.
\end{equation*}
We apply Bernoulli's inequality and $L \leq N$ to obtain
\begin{align*}
\left(1-\frac{1}{2^N}-\exp(-u^2)\right)^L &\geq 1 - \frac{L}{2^N} - L\exp(-u^2)\geq 1 - \frac{N}{2^N} - L\exp \left(-N/(CL^2)\right) \\
& \geq\frac{1}{2} - L\exp(-N/(CL^2)).
\end{align*}
If we now assume $N \geq CL^2 \ln(4L)$, we get
\begin{equation*}
\exp(-N/(CL^2)) \leq \exp(-CL^2\ln(4L)/(CL^2)) = \exp(- \ln(4L)) = \frac{1}{4L}.
\end{equation*}
Hence, the claim follows letting $c_2 \defeq \frac{c_3}{16}$.
\end{proof}
\renewcommand*{\proofname}{Proof}
