
\section{Proof of the upper bound} \label{sec:upper}

Referring to \eqref{eq:lowbound}, the goal of this section is to establish upper bounds for
\begin{equation*}
\underset{x \in \RR^d}{\sup}\Vert W^{(L)} \cdot D^{(L-1)}(x) \cdot W^{(L-1)} \cdots D^{(0)}(x)\cdot W^{(0)}\Vert_2
\end{equation*}
that hold with high probability and in expectation, where the randomness is over the random matrices $W^{(0)} , ..., W^{(L)}$ and the random bias vectors $b^{(0)}, ..., b^{(L-1)}$. For this to make sense one first needs to know that 
\begin{equation} \label{eq:suppp}
\underset{x \in \RR^d}{\sup}\Vert W^{(L)} \cdot D^{(L-1)}(x) \cdot W^{(L-1)}\cdots D^{(0)}(x)\cdot W^{(0)}\Vert_2
\end{equation} 
is indeed measurable; we refer to \Cref{app:measurable} for a proof of this fact. 

Throughout the entire section, we assume that \Cref{assum:1} is satisfied. 
Since the random matrices $W^{(\ell)}$ and the random biases $b^{(\ell)}$ are jointly independent it is possible to calculate the expectation iteratively by first assuming that the matrices
 $W^{(0)}, ... ,W^{(L-1)}$ and the biases $b^{(0)}, ..., b^{(L-1)}$ are fixed and deriving an upper bound when only $W^{(L)}$ is assumed to be random. Then as the final step we are also going to allow randomness in $W^{(0)}, ..., W^{(L-1)}$ and $b^{(0)}, ..., b^{(L-1)}$ to get the desired result. In other words, we are conditioning on $W^{(0)}, ..., W^{(L-1)}, b^{(0)}, ..., b^{(L-1)}$.

The central tool for deriving these bounds is \emph{Dudley's inequality} which can be found for example in \cite[Theorems~5.25~and~5.29]{van2014probability}. We refer to \Cref{app:dudley} for details on Dudley's inequality. The key idea of this section is contained in the following proposition. 
\begin{proposition}\label{prop:key}
Let the matrices $W^{(0)}, ..., W^{(L-1)}$ and the biases $b^{(0)},..., b^{(L-1)}$ be fixed and set $\Lambda \defeq \Vert W^{(L-1)} \Vert_2 \cdots \Vert W^{(0)} \Vert_2$. For $x \in \RR^d$ and $z \in \overline{B}_d(0,1)$ we define
\begin{equation*}
Y_{z,x} \defeq D^{(L-1)}(x) W^{(L-1)}\cdots D^{(0)}(x) \cdot W^{(0)}z \in \RR^N
\end{equation*}
and further
\begin{equation*}
\mathcal{L} = \mathcal{L}(d,N,L,W^{(0)}, ..., W^{(L-1)}, b^{(0)},..., b^{(L-1)})\defeq \left\{ Y_{z,x}: \ x \in \RR^d, \ z \in \overline{B}_d(0,1)\right\} \subseteq \RR^N. 
\end{equation*}
Then there exists an absolute constant $C>0$ such that the following holds: Given any $u \geq 0$, we have
\begin{equation*}
\underset{x \in \RR^d}{\sup} \Vert W^{(L)} \cdot D^{(L-1)}(x)\cdot W^{(L-1)} \cdots D^{(0)}(x) \cdot W^{(0)}\Vert_2 \leq  C \cdot \left( \int_0^{\Lambda} \sqrt{\ln \left(\mathcal{N}(\mathcal{L}, \Vert \cdot \Vert_2, \eps)\right)} \ \dd \eps + u \Lambda \right)
\end{equation*}
with probability at least $(1 - 2\exp(-u^2))$ (with respect to the choice of $W^{(L)}$). Moreover, 
\begin{equation*}
\underset{W^{(L)}}{\EE} \left[ \underset{x \in \RR^d}{\sup} \Vert W^{(L)} \cdot D^{(L-1)}(x) \cdot W^{(L-1)}\cdots D^{(0)}(x)\cdot W^{(0)}\Vert_2\right] \leq C \cdot \int_0^{\Lambda}  \sqrt{\ln \left(\mathcal{N}(\mathcal{L}, \Vert \cdot \Vert_2, \eps)\right)} \ \dd \eps.
\end{equation*}
\end{proposition}
\begin{proof}
For $y \in \mathcal{L}$ it holds that there are $x \in \RR^d$ and $z \in \overline{B}_d(0,1)$ satisfying 
\begin{equation*}
y=Y_{z,x} = D^{(L-1)}(x) W^{(L-1)}\cdots D^{(0)}(x) W^{(0)}z. 
\end{equation*}
We compute
\begin{align*}
\Vert y \Vert_2 &\leq \underbrace{\Vert D^{(L-1)}(x)\Vert_2}_{\leq 1} \cdot \Vert W^{(L-1)}\Vert_2 \cdots \underbrace{\Vert D^{(0)}(x) \Vert_2}_{\leq 1} \cdot \Vert W^{(0)} \Vert_2 \cdot \underbrace{\Vert z \Vert_2}_{\leq 1} \\
&\leq \Vert W^{(L-1)} \Vert_2 \cdots \Vert W^{(0)} \Vert_2 = \Lambda.
\end{align*}
Hence, since $0 = D^{(L-1)}(0) W^{(L-1)}\cdots D^{(0)}(0)W^{(0)}0 \in \mathcal{L}$ it follows 
\begin{equation}\label{eq:cov=1}
\mathcal{N}(\mathcal{L}, \Vert \cdot \Vert_2, \eps) = 1 \quad \text{for } \eps \geq  \Lambda.
\end{equation}
To get the final result we rewrite
\begin{align*}
\underset{x \in \RR^d}{\sup} \Vert W^{(L)}D^{(L-1)}(x)\cdot W^{(L-1)}\cdots D^{(0)}(x) W^{(0)} \Vert_2  &=  \underset{x \in \RR^d, z \in \overline{B}_d(0,1)}{\sup} \left\langle \left(W^{(L)}\right)^T, Y_{z,x}\right\rangle  \\
&=   \underset{Y \in \mathcal{L}}{\sup} \left\langle \left(W^{(L)}\right)^T, Y \right\rangle.
\end{align*}
From the observation $\mathcal{L} \subseteq \overline{B}_N(0, \Lambda)$ we infer $\diam(\mathcal{L}) \leq 2\Lambda$. \Cref{prop:dudley} and \eqref{eq:cov=1} then yield the claim by noticing once again that $0 \in \mathcal{L}$.
\end{proof}
Given the above proposition, the problem of bounding the Lipschitz constant of random ReLU networks has been transferred to bounding the covering numbers of the set $\mathcal{L}$. Finding upper bounds for these covering numbers is the essential task in the following two subsections. In fact, in the following we show that
\begin{equation*}
\mathcal{N}(\mathcal{L}, \Vert \cdot \Vert_2, \eps) \leq \left(\frac{9 \Vert W^{(L-1)} \Vert_2 \cdots \Vert W^{(0)} \Vert_2}{\eps} \right)^{C(d+1)} \cdot \left(\frac{\ee N}{d+1}\right)^{L(d+1)},
\end{equation*}
at least if $N > d+2$. Here, $C>0$ is an absolute constant. However, in the case of shallow networks, i.e., $L=1$, it is possible to show that the above inequality holds without the additional factor $\left(\frac{\ee N}{d+1}\right)^{L(d+1)}$, without the assumption $N > d+2$ and $d$ can be replaced by $\min\{N,d\}$, which in the end leads to a sharper bound on the Lipschitz constant. Therefore, the cases of shallow and deep networks are treated separately in \Cref{sec:shallow,sec:deep}, respectively. The final bounds on the covering numbers can be found in \Cref{lem:cov_num_bound,lem:cov_bound}.


\subsection{The shallow case}
\input{shallow_2.tex}
\subsection{The deep case}
\input{deep.tex}
