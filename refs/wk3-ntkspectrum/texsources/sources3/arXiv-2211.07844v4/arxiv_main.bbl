\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and
  Song]{allenzhu2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
   242--252. PMLR, 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/allen-zhu19a.html}.

\bibitem[Anthony \& Bartlett(2002)Anthony and
  Bartlett]{DBLP:books/daglib/0025992}
Martin Anthony and Peter~L. Bartlett.
\newblock \emph{Neural Network Learning - Theoretical Foundations}.
\newblock Cambridge University Press, 2002.
\newblock URL
  \url{http://www.cambridge.org/gb/knowledge/isbn/item1154061/?site\_locale=en\_GB}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and
  Wang]{fine_grain_arora}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
   322--332. PMLR, 2019{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v97/arora19a.html}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora_exact_comp}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf}.

\bibitem[Azevedo \& Menegatto(2015)Azevedo and
  Menegatto]{azevedo2015eigenvalues}
Douglas Azevedo and Valdir~A Menegatto.
\newblock Eigenvalues of dot-product kernels on the sphere.
\newblock \emph{Proceeding Series of the Brazilian Society of Computational and
  Applied Mathematics}, 3\penalty0 (1), 2015.

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{journals/jmlr/BartlettM02}
Peter~L. Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{J. Mach. Learn. Res.}, 3:\penalty0 463--482, 2002.
\newblock URL
  \url{http://dblp.uni-trier.de/db/journals/jmlr/jmlr3.html#BartlettM02}.

\bibitem[Basri et~al.(2019)Basri, Jacobs, Kasten, and
  Kritchman]{uniform_sphere_data}
Ronen Basri, David~W. Jacobs, Yoni Kasten, and Shira Kritchman.
\newblock The convergence rate of neural networks for learned functions of
  different frequencies.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett (eds.), \emph{Advances
  in Neural Information Processing Systems 32}, pp.\  4763--4772, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/hash/5ac8bb8a7d745102a978c5f8ccdb61b8-Abstract.html}.

\bibitem[Basri et~al.(2020)Basri, Galun, Geifman, Jacobs, Kasten, and
  Kritchman]{10.5555/3524938.3525002}
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira
  Kritchman.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119 of \emph{Proceedings of Machine Learning Research},
  pp.\  685--694. PMLR, 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/basri20a.html}.

\bibitem[Bietti \& Bach(2021)Bietti and Bach]{bietti2021deep}
Alberto Bietti and Francis Bach.
\newblock Deep equals shallow for {ReLU} networks in kernel regimes.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=aDjoksTpXOP}.

\bibitem[Bietti \& Mairal(2019)Bietti and Mairal]{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/c4ef9c39b300931b69a36fb3dbb8d60e-Paper.pdf}.

\bibitem[Bowman \& Mont{\'{u}}far(2022)Bowman and
  Mont{\'{u}}far]{bowman2022implicit}
Benjamin Bowman and Guido Mont{\'{u}}far.
\newblock Implicit bias of {MSE} gradient optimization in underparameterized
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=VLgmhQDVBV}.

\bibitem[Bowman \& Montufar(2022)Bowman and Montufar]{bowman2022spectral}
Benjamin Bowman and Guido Montufar.
\newblock Spectral bias outside the training set for deep networks in the
  kernel regime.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=a01PL2gb7W5}.

\bibitem[Caponnetto \& De~Vito(2007)Caponnetto and
  De~Vito]{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0
  (3):\penalty0 331--368, 2007.

\bibitem[Chen \& Xu(2021)Chen and Xu]{chen2021deep}
Lin Chen and Sheng Xu.
\newblock Deep neural tangent kernel and laplace kernel have the same {RKHS}.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=vK9WrZ0QYQ}.

\bibitem[Cui et~al.(2021)Cui, Loureiro, Krzakala, and
  Zdeborov{\'a}]{cui2021generalization}
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Generalization error rates in kernel regression: The crossover from
  the noiseless to noisy regime.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Da_EHrAcfwd}.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{dual_view}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~29. Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf}.

\bibitem[Davis(2021)]{davis_hermite}
Tom Davis.
\newblock A general expression for {H}ermite expansions with applications.
\newblock 2021.
\newblock \doi{10.13140/RG.2.2.30843.44325}.
\newblock URL
  \url{https://www.researchgate.net/profile/Tom-Davis-2/publication/352374514_A_GENERAL_EXPRESSION_FOR_HERMITE_EXPANSIONS_WITH_APPLICATIONS/links/60c873c5a6fdcc8267cf74d4/A-GENERAL-EXPRESSION-FOR-HERMITE-EXPANSIONS-WITH-APPLICATIONS.pdf}.

\bibitem[{de G. Matthews} et~al.(2018){de G. Matthews}, Hron, Rowland, Turner,
  and Ghahramani]{matthews2018gaussian}
Alexander~G. {de G. Matthews}, Jiri Hron, Mark Rowland, Richard~E. Turner, and
  Zoubin Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=H1-nGgWC-}.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
   1675--1685. PMLR, 2019{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v97/du19c.html}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2018gradient}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Engel et~al.(2022)Engel, Wang, Sarwate, Choudhury, and
  Chiang]{torchNTK}
Andrew Engel, Zhichao Wang, Anand Sarwate, Sutanay Choudhury, and Tony Chiang.
\newblock {TorchNTK}: A library for calculation of neural tangent kernels of
  {PyTorch} models.
\newblock 2022.

\bibitem[Fan \& Wang(2020)Fan and Wang]{NEURIPS2020_572201a4}
Zhou Fan and Zhichao Wang.
\newblock Spectra of the conjugate kernel and neural tangent kernel for
  linear-width neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  7710--7721. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/572201a4497b0b9f02d4f279b09ec30d-Paper.pdf}.

\bibitem[Folland(1999)]{folland1999}
G.~B. Folland.
\newblock \emph{Real analysis: Modern techniques and their applications}.
\newblock Wiley, New York, 1999.

\bibitem[Geifman et~al.(2020)Geifman, Yadav, Kasten, Galun, Jacobs, and
  Ronen]{geifman2020similarity}
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri
  Ronen.
\newblock On the similarity between the {L}aplace and neural tangent kernels.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  1451--1461. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf}.

\bibitem[Geifman et~al.(2022)Geifman, Galun, Jacobs, and Basri]{geifman2022on}
Amnon Geifman, Meirav Galun, David Jacobs, and Ronen Basri.
\newblock On the spectral bias of convolutional neural tangent and gaussian
  process kernels.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=gthKzdymDu2}.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{pmlr-v9-glorot10a}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, volume~9 of \emph{Proceedings of
  Machine Learning Research}, pp.\  249--256. PMLR, 2010.
\newblock URL \url{https://proceedings.mlr.press/v9/glorot10a.html}.

\bibitem[Han et~al.(2022)Han, Zandieh, Lee, Novak, Xiao, and
  Karbasi]{han2022fast}
Insu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao, and Amin
  Karbasi.
\newblock Fast neural kernel embeddings for general activations.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=yLilJ1vZgMe}.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{7410480}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{2015 IEEE International Conference on Computer Vision
  (ICCV)}, pp.\  1026--1034, 2015.

\bibitem[Huang et~al.(2022)Huang, Hogg, and
  Villar]{DBLP:journals/simods/HuangHV22}
Ningyuan~Teresa Huang, David~W. Hogg, and Soledad Villar.
\newblock Dimensionality reduction, regularization, and generalization in
  overparameterized regressions.
\newblock \emph{{SIAM} J. Math. Data Sci.}, 4\penalty0 (1):\penalty0 126--152,
  2022.
\newblock URL \url{https://doi.org/10.1137/20m1387821}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot_ntk}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}.

\bibitem[Jin et~al.(2022)Jin, Banerjee, and Mont\'ufar]{jin2022learning}
Hui Jin, Pradeep~Kr. Banerjee, and Guido Mont\'ufar.
\newblock Learning curves for gaussian process regression with power-law priors
  and targets.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=KeI9E-gsoB}.

\bibitem[Karakida et~al.(2020)Karakida, Akaho, and ichi Amari]{Karakida_2020}
Ryo Karakida, Shotaro Akaho, and Shun ichi Amari.
\newblock Universal statistics of {F}isher information in deep neural networks:
  mean field approach.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2020\penalty0 (12):\penalty0 124005, 2020.
\newblock URL \url{https://doi.org/10.1088/1742-5468/abc62e}.

\bibitem[Kazarinoff(1956)]{kazarinoff_1956}
Donat~K. Kazarinoff.
\newblock On {W}allis' formula.
\newblock \emph{Edinburgh Mathematical Notes}, 40:\penalty0 19–21, 1956.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{LeCuBottOrrMull9812}
Yann~A. LeCun, L{\'e}on Bottou, Genevieve~B. Orr, and Klaus-Robert M{\"u}ller.
\newblock \emph{Efficient BackProp}, pp.\  9--48.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.
\newblock URL \url{https://doi.org/10.1007/978-3-642-35289-8_3}.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl{-}Dickstein]{LeeBNSPS18}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S. Schoenholz, Jeffrey
  Pennington, and Jascha Sohl{-}Dickstein.
\newblock Deep neural networks as {G}aussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1EA-M-0Z}.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{Lee2019WideNN-SHORT}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf}.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{10.5555/3495724.3496995}
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao,
  Roman Novak, and Jascha Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  15156--15172. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/ad086f59924fffe0773f8d0ca22ea712-Paper.pdf}.

\bibitem[Li et~al.(2022)Li, Andreeto, Ranzato, and
  Perona]{li_andreeto_ranzato_perona_2022}
Li, Andreeto, Ranzato, and Perona.
\newblock Caltech 101, Apr 2022.

\bibitem[Li et~al.(2020)Li, Soltanolkotabi, and Oymak]{pmlr-v108-li20j}
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In \emph{Proceedings of the Twenty Third International Conference on
  Artificial Intelligence and Statistics}, volume 108 of \emph{Proceedings of
  Machine Learning Research}, pp.\  4313--4324. PMLR, 2020.
\newblock URL \url{https://proceedings.mlr.press/v108/li20j.html}.

\bibitem[Louart et~al.(2018)Louart, Liao, and Couillet]{10.2307/26542333}
Cosme Louart, Zhenyu Liao, and Romain Couillet.
\newblock A random matrix approach to neural networks.
\newblock \emph{The Annals of Applied Probability}, 28\penalty0 (2):\penalty0
  1190--1248, 2018.
\newblock URL \url{https://www.jstor.org/stable/26542333}.

\bibitem[Mishkin \& Matas(2016)Mishkin and
  Matas]{DBLP:journals/corr/MishkinM15}
Dmytro Mishkin and Jiri Matas.
\newblock All you need is a good init.
\newblock In Yoshua Bengio and Yann LeCun (eds.), \emph{4th International
  Conference on Learning Representations, Conference Track Proceedings}, 2016.
\newblock URL \url{http://arxiv.org/abs/1511.06422}.

\bibitem[Murray et~al.(2022)Murray, Abrol, and Tanner]{MURRAY2022117}
M.~Murray, V.~Abrol, and J.~Tanner.
\newblock Activation function design for deep networks: linearity and effective
  initialisation.
\newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0
  117--154, 2022.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S1063520321001111}.
\newblock Special Issue on Harmonic Analysis and Machine Learning.

\bibitem[Neal(1996)]{neal1996}
Radford~M. Neal.
\newblock \emph{Bayesian Learning for Neural Networks}.
\newblock Springer-Verlag, Berlin, Heidelberg, 1996.

\bibitem[Nguyen(2021)]{nguyenrelu}
Quynh Nguyen.
\newblock On the proof of global convergence of gradient descent for deep relu
  networks with linear widths.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of \emph{Proceedings of Machine Learning Research},
  pp.\  8056--8062. PMLR, 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/nguyen21a.html}.

\bibitem[Nguyen \& Mondelli(2020)Nguyen and Mondelli]{marco}
Quynh Nguyen and Marco Mondelli.
\newblock Global convergence of deep networks with one wide layer followed by
  pyramidal topology.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  11961--11972. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf}.

\bibitem[Nguyen et~al.(2021)Nguyen, Mondelli, and
  Mont\'ufar]{nguyen_tight_bounds}
Quynh Nguyen, Marco Mondelli, and Guido Mont\'ufar.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel
  for deep {ReLU} networks.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of \emph{Proceedings of Machine Learning Research},
  pp.\  8119--8129. PMLR, 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/nguyen21g.html}.

\bibitem[Novak et~al.(2019)Novak, Xiao, Bahri, Lee, Yang, Hron, Abolafia,
  Pennington, and Sohl{-}Dickstein]{bayesianconv}
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Jiri Hron,
  Daniel~A. Abolafia, Jeffrey Pennington, and Jascha Sohl{-}Dickstein.
\newblock Bayesian deep convolutional networks with many channels are
  {G}aussian processes.
\newblock In \emph{7th International Conference on Learning Representations}.
  OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1g30j0qF7}.

\bibitem[Novak et~al.(2022)Novak, Sohl-Dickstein, and
  Schoenholz]{pmlr-v162-novak22a}
Roman Novak, Jascha Sohl-Dickstein, and Samuel~S Schoenholz.
\newblock Fast finite width neural tangent kernel.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato (eds.), \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pp.\  17018--17044. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/novak22a.html}.

\bibitem[O'Donnell(2014)]{donnellbook}
Ryan O'Donnell.
\newblock \emph{Analysis of {B}oolean functions}.
\newblock Cambridge University Press, 2014.

\bibitem[Oymak \& Soltanolkotabi(2020)Oymak and Soltanolkotabi]{solt_mod_over}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees
  for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1), 2020.
\newblock URL \url{https://par.nsf.gov/biblio/10200049}.

\bibitem[Oymak et~al.(2019)Oymak, Fabian, Li, and
  Soltanolkotabi]{DBLP:journals/corr/abs-1906-05392}
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi.
\newblock Generalization guarantees for neural networks via harnessing the
  low-rank structure of the {J}acobian.
\newblock \emph{CoRR}, abs/1906.05392, 2019.
\newblock URL \url{http://arxiv.org/abs/1906.05392}.

\bibitem[Panigrahi et~al.(2020)Panigrahi, Shetty, and
  Goyal]{Panigrahi2020Effect}
Abhishek Panigrahi, Abhishek Shetty, and Navin Goyal.
\newblock Effect of activation functions on the training of overparametrized
  neural nets.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgfdeBYvH}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8024--8035. Curran Associates, Inc., 2019.

\bibitem[Pennington \& Worah(2017)Pennington and Worah]{pennington_nonlinear}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf}.

\bibitem[Pennington \& Worah(2018)Pennington and Worah]{pennington_shallow}
Jeffrey Pennington and Pratik Worah.
\newblock The spectrum of the {F}isher information matrix of a
  single-hidden-layer neural network.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf}.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{Poole2016}
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya
  Ganguli.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~29. Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf}.

\bibitem[Scetbon \& Harchaoui(2021)Scetbon and Harchaoui]{scetbon2021spectral}
Meyer Scetbon and Zaid Harchaoui.
\newblock A spectral analysis of dot-product kernels.
\newblock In \emph{International conference on artificial intelligence and
  statistics}, pp.\  3394--3402. PMLR, 2021.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{samuel2017}
Samuel~S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.
\newblock URL \url{https://openreview.net/pdf?id=H1W1UN9gg}.

\bibitem[Schur(1911)]{Schur1911}
J.~Schur.
\newblock Bemerkungen zur {T}heorie der beschränkten {B}ilinearformen mit
  unendlich vielen {V}eränderlichen.
\newblock \emph{Journal für die reine und angewandte Mathematik},
  140:\penalty0 1--28, 1911.
\newblock URL \url{http://eudml.org/doc/149352}.

\bibitem[Simon et~al.(2022)Simon, Anand, and Deweese]{pmlr-v162-simon22a}
James~Benjamin Simon, Sajant Anand, and Mike Deweese.
\newblock Reverse engineering the neural tangent kernel.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato (eds.), \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pp.\  20215--20231. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/simon22a.html}.

\bibitem[Sontag \& Sussmann(1989)Sontag and
  Sussmann]{Sontag89backpropagationcan}
Eduardo~D. Sontag and Héctor~J. Sussmann.
\newblock Backpropagation can give rise to spurious local minima even for
  networks without hidden layers.
\newblock \emph{Complex Systems}, 3:\penalty0 91--106, 1989.

\bibitem[Velikanov \& Yarotsky(2021)Velikanov and
  Yarotsky]{NEURIPS2021_14faf969}
Maksim Velikanov and Dmitry Yarotsky.
\newblock Explicit loss asymptotics in the gradient descent training of neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  2570--2582. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/14faf969228fc18fcd4fcf59437b0c97-Paper.pdf}.

\bibitem[Vershynin(2012)]{vershynin2011introduction}
Roman Vershynin.
\newblock \emph{Introduction to the non-asymptotic analysis of random
  matrices}, pp.\  210–268.
\newblock Cambridge University Press, 2012.

\bibitem[Weyl(1912)]{Weyl}
Hermann Weyl.
\newblock Das asymptotische {V}erteilungsgesetz der {E}igenwerte linearer
  partieller {D}ifferentialgleichungen (mit einer {A}nwendung auf die {T}heorie
  der {H}ohlraumstrahlung).
\newblock \emph{Mathematische Annalen}, 71\penalty0 (4):\penalty0 441--479,
  1912.
\newblock URL \url{https://doi.org/10.1007/BF01456804}.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{pmlr-v125-woodworth20a}
Blake Woodworth, Suriya Gunasekar, Jason~D. Lee, Edward Moroshko, Pedro
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Proceedings of Thirty Third Conference on Learning Theory},
  volume 125 of \emph{Proceedings of Machine Learning Research}, pp.\
  3635--3673. PMLR, 2020.
\newblock URL \url{https://proceedings.mlr.press/v125/woodworth20a.html}.

\bibitem[Xie et~al.(2017)Xie, Liang, and Song]{pmlr-v54-xie17a}
Bo~Xie, Yingyu Liang, and Le~Song.
\newblock {Diverse Neural Network Learns True Target Functions}.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics}, volume~54 of \emph{Proceedings of
  Machine Learning Research}, pp.\  1216--1224. PMLR, 2017.
\newblock URL \url{https://proceedings.mlr.press/v54/xie17a.html}.

\bibitem[Yang \& Salman(2019)Yang and
  Salman]{https://doi.org/10.48550/arxiv.1907.10599}
Greg Yang and Hadi Salman.
\newblock A fine-grained spectral perspective on neural networks, 2019.
\newblock URL \url{https://arxiv.org/abs/1907.10599}.

\bibitem[Zou \& Gu(2019)Zou and Gu]{zou2019improved}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf}.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Gradient descent optimizes over-parameterized deep {ReLU} networks.
\newblock \emph{Machine learning}, 109\penalty0 (3):\penalty0 467--492, 2020.

\end{thebibliography}
