
\subsection{Gaussian kernel}\label{appendix:background_GK} 

Observe by construction that the flattened collection of preactivations at the first layer $(g^{(1)}(\vx_i))_{i=1}^n$ form a centered Gaussian process, with the covariance between the $\alpha$th and $\beta$th neuron being described by 
\[
\Sigma_{\alpha_\beta}^{(1)}(\vx_i, \vx_j) \defeq \expec[g^{(1)}_{\alpha}(\vx_i) g^{(1)}_{\beta}(\vx_j)] = \delta_{\alpha = \beta} \left( \gamma_w^2 \vx_i^T \vx_j + \gamma_b^2 \right).
\]
Under the Assumption \ref{assumptions:kernel_regime}, the preactivations at each layer $l \in [L+1]$ converge also in distribution to centered Gaussian processes \citep{neal1996, LeeBNSPS18}. We remark that the sequential width limit condition of Assumption \ref{assumptions:kernel_regime} is not necessary for this behavior, for example the same result can be derived in the setting where the widths of the network are sent to infinity simultaneously under certain conditions on the activation function \citep{matthews2018gaussian}. However, as our interests lie in analyzing the limit rather than the conditions for convergence to said limit, for simplicity we consider only the sequential width limit. As per \citet[Eq. 4]{LeeBNSPS18}, the covariance between the preactivations of the $\alpha$th and $\beta$th  neurons at layer $l\geq 2$ for any input pair $\vx, \vy \in \reals$ are described by the following kernel,
\[
\begin{aligned}
\Sigma_{\alpha_\beta}^{(l)}(\vx, \vy) & \defeq \expec[g^{(l)}_{\alpha}(\vx) g^{(l)}_{\beta}(\vy)] \\
&=\delta_{\alpha = \beta} \left(\sigma_w^2 \expec_{g^{(l-1)} \sim \cG\cP (0, \Sigma^{l-1})}[\phi(g^{(l-1)}_{\alpha}(\vx)) \phi( g^{(l-1)}_{\beta}(\vy))] + \sigma_b^2\right).
\end{aligned}
\]
We refer to this kernel as the Gaussian kernel. As each neuron is identically distributed and the covariance between pairs of neurons is 0 unless $\alpha = \beta$, moving forward we drop the subscript and discuss only the covariance between the preactivations of an arbitrary neuron given two inputs. As per the discussion by \citet[Section 2.3]{LeeBNSPS18}, the expectations involved in the computation of these Gaussian kernels can be computed with respect to a bivariate Gaussian distribution, whose covariance matrix has three distinct entries: the variance of a preactivation of $\vx$ at the previous layer, $\Sigma^{(l-1)}(\vx, \vx)$, the variance of a preactivation of $\vy$ at the previous layer, $\Sigma^{(l)}(\vy, \vy)$, and the covariance between preactivations of $\vx$ and $\vy$, $\Sigma^{(l-1)}(\vx, \vy)$. Therefore the Gaussian kernel, or covariance function, and its derivative, which we will require later for our analysis of the NTK, can be computed via the the following recurrence relations, see for instance \citep{LeeBNSPS18, jacot_ntk, arora_exact_comp, nguyen_tight_bounds}, 
\begin{equation}\label{equation:GP_kernel}
    \begin{aligned}
    &\Sigma^{(1)}(\vx, \vy) = \gamma_w^2 \vx^T \vx + \gamma_b^2,\\
    & \mA^{(l)}(\vx, \vy) = \begin{bmatrix}
    \Sigma^{(l-1)}(\vx, \vx) & \Sigma^{(l-1)}(\vx, \vy)\\
    \Sigma^{(l-1)}(\vy, \vx) & \Sigma^{(l-1)}(\vx, \vx)
    \end{bmatrix}\\
    &\Sigma^{(l)}(\vx, \vy) = \sigma_w^2 \expec_{(B_1, B_2) \sim \cN(0, \mA^{(l)}(\vx, \vy))}[\phi(B_1)\phi(B_2)] + \sigma_b^2,\\
    &\dot{\Sigma}^{(l)}(\vx, \vy) = \sigma_w^2\expec_{(B_1, B_2) \sim \cN(0, \mA^{(l)}(\vx, \vy))}\left[\phi'(B_1) \phi'(B_2)\right].
    \end{aligned}
\end{equation}

\subsection{Neural Tangent Kernel (NTK)}\label{appendix:background_NTK}
As discussed in the Section \ref{sec:intro}, under Assumption \ref{assumptions:kernel_regime} $\tilde{\Theta}^{(l)}$ converges in probability to a deterministic limit, which we denote $\Theta^{(l)}$. This deterministic limit kernel can be expressed in terms of the Gaussian kernels and their derivatives from Section \ref{appendix:background_GK} via the following recurrence relationships \cite[Theorem 1]{jacot_ntk}, 
\begin{equation}\label{eq:ntk_kernel_def}
    \begin{aligned}
    \Theta^{(1)}(\vx, \vy) &= \Sigma^{(1)}(\vx,\vy),\\
    \Theta^{(l)}(\vx, \vy) &= \Theta^{(l-1)}(\vx,\vy) \dot{\Sigma}^{(l)}(\vx,\vy) + \Sigma^{(l)}(\vx,\vy)\\
    & = \Sigma^{(l)}(\vx,\vy) + \sum_{h=1}^{l-1}  \Sigma^{(h)}(\vx, \vy)\left( \prod_{h' = h+1}^{l} \dot{\Sigma}^{(h')}(\vx, \vy)\right) \; \forall l \in [2,L+1].
    \end{aligned}
\end{equation}

A useful expression for the NTK matrix, which is a straightforward extension and generalization of \citet[Lemma 3.1]{nguyen_tight_bounds}, is provided in Lemma~\ref{lemma:ntk_exp1} below. 

\begin{lemma}\label{lemma:ntk_exp1}
(Based on \citealt[Lemma 3.1]{nguyen_tight_bounds}) Under Assumption \ref{assumptions:kernel_regime}, a sequence of positive semidefinite matrices $(\mG_{l})_{l=1}^{L+1}$ in $\reals^{n \times n}$, and the related sequence $(\dot{\mG}_{l})_{l=2}^{L+1}$ also in $\reals^{n \times n}$, can be constructed via the following recurrence relationships,
\begin{equation}\label{eq:recurrence_G_matrices}
\begin{aligned}
\mG_{1} &= \gamma_w^2\mX \mX^T+\gamma_b^2 \textbf{1}_{n \times n},\\
\mG_{2}  &= \sigma_w^2 \expec_{\vw \sim \cN(\textbf{0}, \textbf{I}_d)}[\phi(\mX \vw) \phi(\mX \vw)^T] + \sigma_b^2 \textbf{1}_{n \times n},\\
\dot{\mG}_{2} &= \sigma_w^2 \expec_{\vw \sim \cN(\textbf{0}, \textbf{I}_n)}[\phi'(\mX \vw) \phi'(\mX \vw)^T],\\
\mG_{l}  &= \sigma_w^2 \expec_{\vw \sim \cN(\textbf{0}, \textbf{I}_n)}[\phi(\sqrt{\mG_{l-1}} \vw) \phi(\sqrt{\mG_{l-1}} \vw)^T] + \sigma_b^2 \textbf{1}_{n \times n}, \; l \in [3,L+1],\\
\dot{\mG}_{l} &= \sigma_w^2 \expec_{\vw \sim \cN(\textbf{0}, \textbf{I}_n)}[\phi'(\sqrt{\mG_{l-1}} \vw) \phi'(\sqrt{\mG_{l-1}} \vw)^T], \; l \in [3,L+1].
\end{aligned}
\end{equation}
The sequence of NTK matrices $(\mK_{l})_{l=1}^{L+1}$ can in turn be written using the following recurrence relationship,
\begin{equation}\label{eq:reccurence_NTK_matrices}
\begin{aligned}
n\mK_{1} &= \mG_{1},\\
n\mK_{l} &=\mG_{l} + n\mK_{l-1}\odot \dot{\mG}_{l} \\
&= \mG_{l} + \sum_{i=1}^{l-1} \left( \mG_{i} \odot\left( \odot_{j = i+1}^{l} \dot{\mG}_{j}\right)\right).
\end{aligned}
\end{equation}
\end{lemma}
\begin{proof}
    For the sequence $(\mG_l)_{l=1}^{L+1}$ it suffices to prove for any $i,j\in[n]$ and $l \in [L+1]$ that
    \[
    [\mG_{l}]_{i,j} = \Sigma^{(l)}(\vx_i, \vx_j)
    \]
    and $\mG_{l}$ is positive semi-definite.  We proceed by induction, considering the base case $l=1$ and comparing \eqref{eq:recurrence_G_matrices} with \eqref{equation:GP_kernel} then it is evident that
    \[
    [\mG_{1}]_{i,j} = \Sigma^{(1)}(\vx_i, \vx_j).
    \]
    In addition, $\mG_{1}$ is also clearly positive semi-definite as for any $\vu \in \reals^n$
    \[
    \vu^T\mG_{1}\vu = \gamma_w^2\norm{\mX^T\vu}^2 + \gamma_b^2\norm{\textbf{1}_n^T \vu}^2 \geq 0.
    \]
    We now assume the induction hypothesis is true for $\mG_{l-1}$. We will need to distinguish slightly between two cases, $l=2$ and $l\in[3,L+1]$. The proof of the induction step in either case is identical. To this end, and for notational ease, let $\mV = \mX$, $\vw \sim \cN(0, \textbf{I}_d)$ when $l=2$, and $\mV = \sqrt{\mG_{l-1}}$, $\vw \sim \cN(0, \textbf{I}_n)$ for $l\in[3,L+1]$. In either case we let $\vv_i$ denote the $i$th row of $\mV$. For any $i,j \in [n]$
    \[
    [\mG_{l}]_{ij} = \sigma_w^2 \expec_{\vw}[\phi(\vv_i^T \vw)\phi(\vv_j^T\vw)] + \sigma_b^2.
    \]
    Now let $B_1 = \vv_i^T \vw$, $B_2 =\vv_j^T \vw$ and observe for any $\alpha_1, \alpha_2 \in \reals$ that $\alpha_1B_1 + \alpha_2B_2 = \sum_{k}^n(\alpha_1v_{ik}+ \alpha_2v_{jk})w_{k} \sim \cN(0, \norm{\alpha_1 \vv_i + \alpha_2 \vv_j}^2)$. Therefore the joint distribution of $(B_1, B_2)$ is a mean 0 bivariate normal distribution. Denoting the covariance matrix of this distribution as $\tilde{\mA} \in \reals^{2 \times 2}$, then $[\mG_{l}]_{ij}$ can be expressed as
    \[
    [\mG_{l}]_{ij} = \sigma_w^2 \expec_{(B_1, B_2)\sim \tilde{\mA}}[\phi(B_1) \phi(B_2)] + \sigma_b^2.
    \]
    To prove $[\mG_{l}]_{i,j} = \Sigma^{(l)}$ it therefore suffices to show that $\tilde{\mA} = \mA^{(l)}$ as per \eqref{equation:GP_kernel}. This follows by the induction hypothesis as
    \[
    \begin{aligned}
    \expec[B_1^2] &= \vv_i^T \vv_i = [\mG_{l-1}]_{ii} = \Sigma^{(l-1)}(\vx_i, \vx_i),\\
    \expec[B_2^2] &= \vv_j^T \vv_j = [\mG_{l-1}]_{jj} = \Sigma^{(l-1)}(\vx_j, \vx_j),\\
    \expec[B_1B_2] &= \vv_i^T \vv_j = [\mG_{l-1}]_{ij} = \Sigma^{(l-1)}(\vx_i, \vx_j).
    \end{aligned}
    \]
    Finally, $\mG_{l}$ is positive semi-definite as long as $\expec_{\vw}[\phi(\mV \vw) \phi(\mV \vw)^T]$ is positive semi-definite. Let $M(\vw) = \phi(\mV \vw) \in \reals^{n\times n}$ and observe for any $\vw$ that $M(\vw)M(\vw)^T$ is positive semi-definite. Therefore $\expec_{\vw}[M(\vw)M(\vw)^T]$ must also be positive semi-definite. Thus the inductive step is complete and we may conclude for $l \in [L+1]$ that
    \begin{equation} \label{eq:G_sigma_relationship}
        [\mG_{l}]_{i,j} = \Sigma^{(l)}(\vx_i, \vx_j).
    \end{equation}
    For the proof of the expression for the sequence $(\dot{\mG}_l)_{l=2}^{L+1}$ it suffices to prove for any $i,j\in[n]$ and $l \in [L+1]$ that
    \[
    [\dot{\mG}_{l}]_{i,j} = \dot{\Sigma}^{(l)}(\vx_i, \vx_j).
    \]
    By comparing \eqref{eq:recurrence_G_matrices} with \eqref{equation:GP_kernel} this follows immediately from \eqref{eq:G_sigma_relationship}. Therefore with \eqref{eq:recurrence_G_matrices} proven \eqref{eq:reccurence_NTK_matrices} follows from \eqref{eq:ntk_kernel_def}. 
\end{proof}


\subsection{Unit variance initialization}\label{appendix:unit_var_init}
The initialization scheme for a neural network, particularly a deep neural network, needs to be designed with some care in order to avoid either vanishing or exploding gradients during training \cite{pmlr-v9-glorot10a, 7410480, DBLP:journals/corr/MishkinM15,LeCuBottOrrMull9812}. Some of the most popular initialization strategies used in practice today, in particular \cite{LeCuBottOrrMull9812} and \cite{pmlr-v9-glorot10a} initialization, first model the preactivations of the network as Gaussian random variables and then select the network hyperparameters in order that the variance of these idealized preactivations is fixed at one. Under Assumption~\ref{assumptions:kernel_regime} this idealized model on the preactivations is actually realized and if we additionally assume the conditions of Assumption~\ref{assumption:init_var_1} hold then likewise the variance of the preactivations at every layer will be fixed at one. To this end, and as in \cite{Poole2016, MURRAY2022117}, consider the function $V\colon \reals_{\geq 0} \rightarrow \reals_{\geq 0}$ defined as 
\begin{equation}\label{eq:var_func}
    V(q) = \sigma_w^2 \expec_{Z\sim \cN(0,1)}\left[\phi\left(\sqrt{q}Z \right)^2 \right] +  \sigma_b^2. 
\end{equation}
Noting that $V$ is another expression for $\Sigma^{(l)}(\vx, \vx)$, derived via a change of variables as per \cite{Poole2016}, the sequence of variances $(\Sigma^{(l)}(\vx,\vx))_{l=2}^L$ can therefore be generated as follows,
\begin{equation} \label{eq:var_generator}
    \Sigma^{(l)}(\vx,\vx) = V(\Sigma^{(l-1)}(\vx,\vx)).
\end{equation}

The linear correlation $\rho^{(l)}: \reals^d \times \reals^d \rightarrow [-1,1]$ between the preactivations of two inputs $\vx, \vy \in \reals^d$ we define as
\begin{equation}
   \begin{aligned}
        \rho^{(l)}(\vx, \vy) = \frac{\Sigma^{(l)}(\vx, \vy)}{\sqrt{\Sigma^{(l)}(\vx, \vx)\Sigma^{(l)}(\vy, \vy)}} . 
\end{aligned} 
\end{equation}
Assuming $\Sigma^{(l)}(\vx, \vx) = \Sigma^{(l)}(\vy, \vy) = 1$ for all $l \in [L+1]$, then $\rho^{(l)}(\vx, \vy) =  \Sigma^{(l)}(\vx, \vy)$.  Again as in \cite{MURRAY2022117} and analogous to \eqref{eq:var_func}, with $Z_1,Z_2 \sim \cN(0,1)$ independent, $U_1 \defeq Z_1$, $U_2(\rho) \defeq (\rho Z_1 + \sqrt{1- \rho^2}Z_2)$ \footnote{We remark that $U_1, U_2$ are dependent and identically distributed as $U_1, U_2  \sim \cN(0, 1)$.} 
we define the correlation function $R: [-1,1] \rightarrow [-1,1]$ as
\begin{equation} \label{eq:corr_map}
\begin{aligned}
    R(\rho) = \sigma_w^2 \expec[\phi(U_1) \phi(U_2(\rho))] + \sigma_b^2.
\end{aligned}
\end{equation}
Noting under these assumptions that $R$ is equivalent to $\Sigma^{(l)}(\vx, \vy)$, the sequence of correlations $(\rho^{(l)}(\vx,\vy))_{l=2}^L$ can thus be generated as 
\[
\rho^{(l)}(\vx, \vy) = R(\rho^{(l-1)}(\vx, \vy)).
\]
As observed in \cite{Poole2016,samuel2017}, $R(1) = V(1) = 1$, hence $\rho=1$ is a fixed point of $R$. We remark that as all preactivations are distributed as $\cN(0,1)$, then a correlation of one between preactivations implies they are equal. The stability of the fixed point $\rho =1$ is of particular significance in the context of initializing deep neural networks successfully. Under mild conditions on the activation function one can compute the derivative of $R$, see e.g., \cite{Poole2016, samuel2017, MURRAY2022117}, as follows,
\begin{equation} \label{eq:corr_map_diff}
\begin{aligned}
    R'(\rho) = \sigma_w^2 \expec[\phi'(U_1) \phi'(U_2(\rho))].
\end{aligned}
\end{equation}
Observe that the expression for $\dot{\Sigma}^{(l)}$ and $R'$ are equivalent via a change of variables \citep{Poole2016}, and therefore the sequence of correlation derivatives may be computed as
\[
\dot{\Sigma}^{(l)}(\vx, \vy) = R'(\rho^{(l)}(\vx, \vy)).
\]


With the relevant background material now in place we are in a position to prove Lemma~\ref{lemma:unit_var}. 
    
\begin{lemma}\label{lemma:unit_var}
Under Assumptions \ref{assumptions:kernel_regime} and \ref{assumption:init_var_1} and defining $\chi = \sigma_w^2 \expec_{Z \sim \cN(0,1)}[\phi'(Z)^2] \in \reals_{>0}$, then for all $i,j \in [n]$, $l \in [L+1]$
    \begin{itemize}
        \item $[\mG_{n,l}]_{ij} \in [-1,1]$ and $[\mG_{n,l}]_{ii}=1$,
        \item $[\dot{\mG}_{n,l}]_{ij} \in [-\chi,\chi]$ and $[\dot{\mG}_{n,l}]_{ii}=\chi$.
    \end{itemize}
    Furthermore, the NTK is a dot product kernel, meaning $\Theta(\vx_i, \vx_j)$ can be written as a function of the inner product between the two inputs, $\Theta(\vx_i^T\vx_j)$.
\end{lemma}
\begin{proof}
    Recall from Lemma \ref{lemma:ntk_exp1} and its proof that for any $l \in [L+1]$, $i,j \in [n]$ $[\mG_{n,l}]_{ij} = \Sigma^{(l)}(\vx_i, \vx_j)$ and $[\dot{\mG}_{n,l}]_{ij} = \dot{\Sigma}^{(l)}(\vx_i, \vx_j)$. We first prove by induction $\Sigma^{(l)}(\vx_i, \vx_i) = 1$ for all $l \in [L+1]$. The base case $l=1$ follows as
    \[
    \Sigma^{(1)}(\vx,\vx) = \gamma_w^2 \vx^T \vx + \gamma_b^2 = \gamma_w^2 + \gamma_b^2 =1.
    \]
    Assume the induction hypothesis is true for layer $l-1$. With $Z \sim \cN(0,1)$, then from \eqref{eq:var_func} and \eqref{eq:var_generator}
    \[
    \begin{aligned}
    \Sigma^{(l)}(\vx, \vx) &= V(\Sigma^{(l-1)}(\vx,\vx))\\
    & = \sigma_w^2 \expec\left[\phi^2\left(\sqrt{\Sigma^{(l-1)}(\vx, \vx)}Z \right) \right] +  \sigma_b^2\\
    & = \sigma_w^2 \expec\left[\phi^2\left(Z \right) \right] +  \sigma_b^2\\
    &=1,
    \end{aligned}
    \]
    thus the inductive step is complete. As an immediate consequence it follows that $[\mG_{l}]_{ii}=1$.  Also, for any $i,j \in [n]$ and $l \in [L+1]$,
    \[
    \begin{aligned}
    \Sigma^{(l)}(\vx_i,\vx_j) &=\rho^{(l)}(\vx_i,\vx_j) = R(\rho^{(l-1)}(\vx_i,\vx_j))
    = R(...R(R(\vx_i^T\vx_j))).
    \end{aligned}
    \]
    Thus we can consider $\Sigma^{(l)}$ as a univariate function of the input correlation $\Sigma: [-1,1] \rightarrow [-1,1]$ and also conclude that $[\mG_{l}]_{ij} \in [-1,1]$. Furthermore,
    \[
    \begin{aligned}
    \dot{\Sigma}^{(l)}(\vx_i,\vx_j) = R'(\rho^{(l)}(\vx_i, \vx_j)) = R'(R(...R(R(\vx_i^T\vx_j)))), 
    \end{aligned}
    \]
    which likewise implies $\dot{\Sigma}$ is a dot product kernel. Recall now the random variables introduced to define $R$: $Z_1, Z_2 \sim \cN(0,1)$ are independent and $U_1 = Z_1$, $U_2 = (\rho Z_1 + \sqrt{1- \rho^2}Z_2)$. Observe $U_1, U_2$ are dependent but identically distributed as $U_1, U_2  \sim \cN(0, 1)$. For any $\rho \in [-1,1]$ then applying the Cauchy-Schwarz inequality gives 
    \[
	\begin{aligned}
	|R'(\rho)|^2 = \sigma_w^4 \left| \expec[\phi'(U_1) \phi'(U_2)] \right|^2
	\leq \sigma_w^4 \expec[\phi'(U_1)^2] \expec[\phi'(U_2)^2]
	 = \sigma_w^4 \expec[\phi'(U_1)^2]^2 = |R'(1)|^2.
	\end{aligned}
	\]
	As a result, under the assumptions of the lemma $\dot{\Sigma}^{(l)}:[-1,1] \rightarrow [-\chi, \chi]$ and $\dot{\Sigma}^{(l)}(\vx_i, \vx_i) = \chi$. From this it immediately follows that $[\dot{\mG}_{l}]_{ij} \in [-\chi,\chi]$ and $[\dot{\mG}_{l}]_{ii}=\chi$ as claimed. Finally, as $\Sigma: [-1,1] \rightarrow [-1,1]$ and $\dot{\Sigma}: [-1,1] \rightarrow [-\chi,\chi]$ are dot product kernels, then from \eqref{eq:ntk_kernel_def} the NTK must also be a dot product kernel and furthermore a univariate function of the pairwise correlation of its input arguments.
\end{proof}

The following corollary, which follows immediately from Lemma \ref{lemma:unit_var} and \eqref{eq:reccurence_NTK_matrices}, characterizes the trace of the NTK matrix in terms of the trace of the input gram.


\begin{corollary} \label{cor:trace_with_depth}
    Under the same conditions as Lemma \ref{lemma:unit_var}, suppose $\phi$ and $\sigma_w^2$ are chosen such that $\chi = 1$. Then 
    \begin{equation}
        Tr(\mK_{n,l}) = l. 
    \end{equation}
\end{corollary}



\subsection{Hermite Expansions}\label{appendix:background_hermite}
We say that a function $f:\reals \rightarrow \reals$ is square integrable w.r.t.\ the standard Gaussian measure $\gamma = e^{-x^2/2} / \sqrt{2 \pi}$ if $\expec_{x \sim \cN(0,1)}[f(x)^2]< \infty$. 
We denote by $L^2(\reals,\gamma)$ the space of all such functions. The probabilist's Hermite polynomials are given by
\begin{align*}
	H_k(x)={(-1)}^ke^{x^2/2} \frac{d^{k}}{d x^{k}} e^{-x^2/2}, \quad k=0,1,\ldots .
\end{align*}
The first three Hermite polynomials are $H_0(x)=1$, $H_1(x)=x$, $H_2(x)=(x^2-1)$. 
Let $h_k(x)=\tfrac{H_k(x)}{\sqrt{k!}}$ denote the normalized probabilist's Hermite polynomials. The normalized Hermite polynomials form a complete orthonormal basis in $L^2(\reals,\gamma)$ \cite[\S 11]{donnellbook}: in all that follows, whenever we reference the Hermite polynomials, we will be referring to the normalized Hermite polynomials. The Hermite expansion of a function $\phi \in L^2(\reals , \gamma)$ is given by 
\begin{align}\label{eq:HermiteExp}
	\phi(x)= \sum_{k=0}^\infty \mu_k(\phi) h_k(x),
\end{align}
where 
\begin{equation} \label{eq:norm_prob_hermite_coeffs}
    \mu_k(\phi) = \expec_{X \sim \cN(0,1)}[\phi(X)h_k(X)]
\end{equation}
is the $k$th normalized probabilist's Hermite coefficient of $\phi$. In what follows we shall make use of the following identities.
\begin{align}
\forall k \geq 1,\, h_{k}^{\prime}(x) &=\sqrt{k} h_{k-1}(x), \label{eq:HP1}\\
\forall k \geq 1,\, x h_{k}(x)&=\sqrt{k+1}h_{k+1}(x)+\sqrt{k} h_{k-1}(x). \label{eq:HP2}
\end{align}
\begin{align}
	\begin{array}{c}
		h_{k}(0)=\left\{\begin{array}{ll}
			0, & \text { if } k \text { is odd } \\
			\frac{1}{\sqrt{k !}}(-1)^{\frac{k}{2}}(k-1) ! ! & \text { if } k \text { is even }
		\end{array}\right., \\ \text{ where }
		k ! !=\left\{\begin{array}{ll}
			1, & k \leq 0 \\
			k \cdot(k-2) \cdots 5 \cdot 3 \cdot 1, & k>0 \text { odd } \\
			k \cdot(k-2) \cdots 6 \cdot 4 \cdot 2, & k>0 \text { even }.
		\end{array}\right.
	\end{array}\label{eq:HP3}
\end{align}

We also remark that the more commonly encountered physicist's Hermite polynomials, which we denote $\tilde{H}_k$, are related to the normalized probablist's polynomials as follows,
\[
    h_k(z) = \frac{2^{-k/2}\tilde{H}_k(z/\sqrt{2})}{\sqrt{k!}}.
\]

The Hermite expansion of the activation function deployed will play a key role in determining the coefficients of the NTK power series. In particular, the Hermite coefficients of ReLU are as follows.

\begin{lemma} \label{lemma:hermite_relu}
     \cite{dual_view} For $\phi(z) = \max\{0,z\}$ the Hermite coefficients are given by
     \begin{equation}
         \mu_k(\phi) =
         \begin{cases}
            1/\sqrt{2 \pi}, &\text{$k=0$},\\
            1/2, &\text{$k=1$},\\
            (k-3)!!/\sqrt{2\pi k!}, &\text{$k$ even and $k\geq 2$},\\
            0 , &\text{$k$ odd and $k>3$}.
         \end{cases}
     \end{equation}
\end{lemma}
