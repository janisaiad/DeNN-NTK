
This paper shows how to give a fine-grained analysis of the optimization trajectory and the generalization ability of overparameterized two-layer neural networks trained by gradient descent. 
%The deep nets are randomly initialized and thus are similar to \textquotedblleft random kitchen sinks\textquotedblright ~\citep{rahimi2009weighted}. A small amount of training suffices to train the kitchen sink to fit interesting classes of 
%functions. 
We believe that our approach can also be useful in analyzing overparameterized deep neural networks and other machine learning models.
