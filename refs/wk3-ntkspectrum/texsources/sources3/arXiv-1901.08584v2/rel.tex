In this section we survey previous works on optimization and generalization aspects of neural networks.

%landscape does not work
\paragraph{Optimization.}
%For optimization, 
Many papers tried to characterize geometric landscapes of objective functions~\citep{safran2017spurious,zhou2017critical,freeman2016topology,hardt2016identity,nguyen2017loss,kawaguchi2016deep,venturi2018neural,soudry2016no,du2018power,soltanolkotabi2018theoretical,haeffele2015global}.
The hope is to leverage recent advance in first-order algorithms~\citep{ge2015escaping,lee2016gradient,jin2017escape} which showed that if the landscape satisfies (1) all local minima are global and (2) all saddle points are strict (i.e., there exists a negative curvature), then first-order methods can escape all saddle points and find a global minimum.
Unfortunately, these desired properties do not hold even for simple non-linear shallow neural networks~\citep{yun2018critical} or 3-layer linear neural networks~\citep{kawaguchi2016deep}.

%The main problem with the landscape approach is that in order to apply the generic algorithmic results, one needs to show the above two benign geometric properties on the whole parameter space.
%While it is also possible to show these good properties hold locally, e.g., around the initialization region, in order to obtain a complete convergence result, one needs to analyze the region SGD can explore which necessarily requires a  characterization of the trajectory of SGD.
%However, recall the primary goal is to show SGD can find a global minimum, it makes more sense to directly show the trajectory generated by randomly initialized SGD towards a global minimum.
%
%\simon{TO DO: check Nadav's blog and Sanjeev's slides to see if there are anything else to say} 



%recent trajectory based analysis for optimization with strong assumptions
Another approach is to directly analyze trajectory of the optimization method and to show convergence to global minimum.
A series of papers made strong assumptions on input distribution as well as realizability of labels, and showed global convergence of (stochastic) gradient descent for some shallow neural networks~\citep{tian2017analytical,soltanolkotabi2017learning,brutzkus2017globally,du2017convolutional,du2017spurious,li2017convergence}.
Some local convergence results have also been proved~\citep{zhong2017recovery,zhang2018learning}.
However, these assumptions are not satisfied in practice.


%mean field papers
For two-layer neural networks, a line of papers used mean field analysis to establish that for infinitely wide neural networks, the empirical distribution of the neural network parameters can be described as a Wasserstein gradient flow~\citep{mei2018mean,chizat2018global,sirignano2018mean,rotskoff2018neural,wei2018margin}.
However, it is unclear whether this framework can explain the behavior of first-order methods on finite-size neural networks. % can minimize the empirical risk in polynomial time.

%recent breakthroughs
Recent breakthroughs were made in understanding optimization of overparameterized neural networks through the trajectory-based approach.
They proved global polynomial time convergence of (stochastic) gradient descent on non-linear neural networks for minimizing empirical risk.
Their proof techniques can be roughly classified into two categories.
\citet{li2018learning,allen2018convergence,zou2018stochastic} analyzed the trajectory of parameters and showed that on the trajectory, the objective function satisfies certain gradient dominance property.
On the other hand, \cite{du2018global,du2018provably} analyzed the trajectory of network predictions on training samples and showed that it enjoys a strongly-convex-like property.
%Our analysis on the convergence of gradient descent builds upon \cite{du2018provably} but 
%we give a more refined analysis on the trajectory which enables us to explain Observation~\ref{obs:conv}.
%Interestingly, our analysis on generalization relies on characterizing the trajectory of the parameters, using similar ideas in \cite{li2018learning}.



%norm-based bounds
\paragraph{Generalization.}
It is well known that the VC-dimension of neural networks is at least linear in the number of parameters~\citep{bartlett2017nearly}, and therefore classical VC theory cannot explain the generalization ability of modern neural networks with more parameters than training samples.
% where there are more parameters in the neural networks than the number of samples and modern neural networks have VC-dimension that is at least linear in the number of parameters~\citep{bartlett2017nearly}.
Researchers have proposed norm-based generalization bounds~\citep{bartlett2002rademacher,bartlett2017spectrally,neyshabur2015norm,neyshabur2017pac,neyshabur2018the,konstantinos2017pac,golowich2017size,li2018tighter} and compression-based bounds~\citep{arora2018stronger}.
%However, to our knowledge, all these bounds still have dependence on the width of neural networks (see discussions in \cite{golowich2017size}).
\citet{dziugaite2017computing,zhou2018nonvacuous} used the PAC-Bayes approach to compute non-vacuous generalization bounds for MNIST and ImageNet, respectively.
All these bounds are \emph{posterior} in nature -- they depend on certain properties of the \emph{trained} neural networks.
Therefore, %these bounds are not indicative for e.g. neural network architecture selection, because 
one has to finish training a neural network to know whether it can generalize.
Comparing with these results, our generalization bound only depends on training data and can be calculated without actually training the neural network.
%\simon{@zhiyuan: can you double check this paragraph?}


%a priori generalization bound
Another line of work assumed the existence of a true model, and showed that the (regularized) empirical risk minimizer has good generalization with sample complexity that depends on the true model~\citep{du2018many,ma2018priori,imaizumi2018deep}.
%On the other hand, some recent works put assumptions on the complexity of the target function and show the global minimizer of the (regularized) empirical loss function can learn these functions with near optimal statistical convergence rates~\citep{du2018many,ma2018priori,imaizumi2018deep}.
%However, these papers ignore the optimization algorithm and it is not clearly why the target functions in these papers are what actually we want to learn in practice.
These papers ignored the difficulty of optimization, while we are able to prove generalization of the solution found by gradient descent.
Furthermore, our generic generalization bound does not assume the existence of any true model.

%Comparing with these papers, our result is optimization-algorithm-dependent and the bound depends on a general complexity measure instead of the complexity of the underlying target function.





%For algorithm-dependent bounds..\simon{maybe we don't need them because we already mentioned them.}

%yuanzhi's generalization paper
Our paper is closely related to~\citep{allen2018learning} which showed that two-layer overparametrized neural networks trained by randomly initialized stochastic gradient descent can learn a class of infinite-order smooth functions. % that can be decomposed by low-degree polynomials.
In contrast, our generalization bound depends on a data-dependent complexity measure that can be computed for any dataset, without assuming any ground-truth model.
Furthermore, as a consequence of our generic bound, we also show that two-layer neural networks can learn a class of infinite-order smooth functions, with a less strict requirement for smoothness.
%Our result is more general because our generalization bound is depends a general complexity measure and as a direct corollary we show we can learn low-degree polynomials as well.
%Furthermore, our bound is completely independent of the number of hidden units, while there is a poly-logarithmic dependence in \cite{allen2018learning}.
\citet{allen2018learning} also studied the generalization performance of three-layer neural nets.



Lastly, our work is related to kernel methods, especially recent discoveries of the connection between deep learning and kernels~\citep{jacot2018neural,chizat2018note,daniely2016toward,daniely2017sgd}.
%In fact, our generalization bound has the similar form of that of kernel SVM.
%However, our proof does not rely on existing analysis of kernel methods.
%Instead, we take a trajectory-based approach which enables us to analyze optimization and generalization of neural nets at the same time.
%How to make an explicit connection between our approach and kernels is an interesting open problem.
Our analysis utilized several properties of a related kernel from the ReLU activation (c.f.~Equation~\eqref{eqn:H_infy_defn}). 


%\simon{Maybe mention other empirical works as well?}
%\simon{expressiveness?}