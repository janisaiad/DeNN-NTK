

%% related works

@inproceedings{yang2019:GP,
  title={Tensor programs {I}: Wide feedforward or recurrent neural networks of any architecture are Gaussian processes},
  author={G. Yang},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={9951--9960},
  year={2019}
}

@article{neal1996:GP,
  title={Priors for infinite networks},
  author={R. M. Neal},
  journal={Bayesian Learning for Neural Networks},
  pages={29--53},
  year={1996}
}

@inproceedings{cho2009:GP,
  title={Kernel methods for deep learning},
  author={Y. Cho and L. Saul},
  booktitle={Advances in Neural Information Processing Systems 22},
  pages={342--350},
  year={2009}
}

@inproceedings{novak2018:GP,
  title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
  author={R. Novak and L. Xiao and Y. Bahri and J. Lee and G. Yang and J. Hron and D. A. Abolafia and J. Pennington and J. Sohl-dickstein},
  booktitle={Proceedings of the 6th International Conference on Learning Representations},
  year={2018}
}

@inproceedings{garriga2019:GP,
  title={Deep Convolutional Networks as shallow Gaussian Processes},
  author={A. Garriga-Alonso and C. Rasmussen and L. Aitchison},
  booktitle={Proceedings of the 7th International Conference on Learning Representations},
  year={2019}
}

@inproceedings{lee2018:NNGP,
  title={Deep Neural Networks as Gaussian Processes},
  author={J. Lee and Y. Bahri and R. Novak and S. S. Schoenholz and J. Pennington and J. Sohl-Dickstein},
  booktitle={Proceedings of the 6th International Conference on Learning Representations},
  year={2018}
}

@article{pang2019:NNGP,
  title={Neural-net-induced Gaussian process regression for function approximation and {PDE} solution},
  author={G. Pang and L. Yang and G. E. Karniadakis},
  journal={Journal of Computational Physics},
  volume={384},
  pages={270--288},
  year={2019}
}

@article{park2020:NNGP,
  title={Towards {NNGP}-guided neural architecture search},
  author={D. S. Park and J. Lee and D. Peng and Y. Cao and J. Sohl-Dickstein},
  journal={arXiv:2011.06006},
  year={2020}
}

@article{zhang2022:NNGP,
  title={Neural network gaussian processes by increasing depth},
  author={S.-Q. Zhang and F. Wang and F.-L. Fan},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}


@inproceedings{pleiss2022:NNGP,
  title={The limitations of large width in neural networks: A deep Gaussian process perspective},
  author={G. Pleiss and J. P. Cunningham},
  booktitle={Advances in Neural Information Processing Systems 34},
  pages={3349--3363},
  year={2021}
}


@inproceedings{jacot2018:NTK,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={A. Jacot and F. Gabriel and C. Hongler},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={8580 -- 8589},
  year={2018}
}

@inproceedings{du2019:NTK,
  title={Gradient descent finds global minima of deep neural networks},
  author={S. S. Du and J. Lee and H. Li and L. Wang and X. Zhai},
  booktitle={Proceedings of the 36th International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}

@inproceedings{arora2019:NTK,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={S. Arora and S. S. Du and W. Hu and Z. Li and R. Wang},
  booktitle={Proceedings of the 36th International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@inproceedings{du2019:GNTK,
  title={Graph neural tangent kernel: Fusing graph neural networks with graph kernels},
  author={S. S. Du and K. Hou and R. R. Salakhutdinov and B. Poczos and R. Wang and K. Xu},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={5723 -- 5733},
  year={2019}
}

@inproceedings{huang2021:NTK,
  title={{FL-NTK}: A neural tangent kernel-based framework for federated learning analysis},
  author={B. Huang and X. Li and Z. Song and X. Yang},
  booktitle={Proceedings of the 38th International Conference on Machine Learning},
  pages={4423--4434},
  year={2021}
}

@article{mahankali2023:NTK,
  title={Beyond {NTK} with vanilla gradient descent: A mean-field analysis of neural networks with polynomial width, samples, and time},
  author={A. Mahankali and J. Z. Haochen and K. Dong and M. Glasgow and T. Ma},
  journal={arXiv:2306.16361},
  year={2023}
}

@inproceedings{malladi2023:NTK,
  title={A kernel-based view of language model fine-tuning},
  author={S. Malladi and A. Wettig and D. Yu and D. Chen and S. Arora},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  pages={23610--23641},
  year={2023}
}

@inproceedings{hron2020:attention,
  title={Infinite attention: {NNGP} and {NTK} for deep attention networks},
  author={J. Hron and Y. Bahri and J. Sohl-Dickstein and R. Novak},
  booktitle={Proceedings of the 37th International Conference on Machine Learning},
  pages={4376--4386},
  year={2020}
}

@article{avidan2023:connecting,
  title={Connecting {NTK} and {NNGP}: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime},
  author={Y. Avidan and Q. Li and H. Sompolinsky},
  journal={arXiv:2309.04522},
  year={2023}
}

@book{mezard1987:Replica,
  title={Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications},
  author={M. M{\'e}zard and G. Parisi and M. A. Virasoro},
  year={1987},
  publisher={World Scientific Publishing Company}
}

@inproceedings{arora2019:NNGP,
  title={On exact computation with an infinitely wide neural net},
  author={S. Arora and S. S. Du and W. hu and Z. Li and R. R. Salakhutdinov and R. Wang},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={8141--8150},
  year={2019}
}


%% proof 
@inproceedings{bracale2020:asymptotic,
	title={Large-width functional asymptotics for deep gaussian neural networks},
	author={D. Bracale and S. Favaro and S. Fortini and S. Peluchetti},
	booktitle={Proceedings of the 8th International Conference on Learning Representations},
	year={2020}
}

@book{stroock1997:Kolmogorov,
	title={Multidimensional Diffusion Processes},
	author={D. Stroock and S. Varadhan},
	year={1997},
	publisher={Springer Science \& Business Media}
}

@article{zhang2021:arise,
  title={ARISE: ApeRIodic SEmi-parametric Process for Efficient Markets without Periodogram and Gaussianity Assumptions},
  author={S.-Q. Zhang and Z.-H. Zhou},
  journal={arXiv:2111.06222},
  year={2021}
}

@inproceedings{nguyen2021:eigenvalues,
	title={Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks},
	author={Q. Nguyen and M. Mondelli and G. Montufar},
	booktitle={Proceedings of the 38th International Conference on Machine Learning},
	pages={8119--8129},
	year={2021}
}

@article{salas1999gershgorin,
  title={Gershgorin's theorem for matrices of operators},
  author={Salas, Hector N},
  journal={Linear Algebra and its Applications},
  volume={291},
  number={1-3},
  pages={15--36},
  year={1999}
}

@inproceedings{lee2020finite,
  title={Finite versus infinite neural networks: An empirical study},
  author={J. Lee and S. Schoenholz and J. Pennington and B. Adlam and L. Xiao and R. Novak and J. Sohl-Dickstein},
  booktitle={Advances in Neural Information Processing Systems 33},
  pages={15156--15172},
  year={2020}
}

@article{poggio2020theoretical,
  title={Theoretical issues in deep networks},
  author={T. Poggio and A. Banburski and Q. Liao},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30039--30045},
  year={2020}
}

@book{van2000asymptotic,
  title={Asymptotic Statistics},
  author={Van der Vaart, A. W.},
  year={2000},
  publisher={Cambridge University Press}
}

@book{billingsley2013convergence,
  title={Convergence of Probability Measures},
  author={P. Billingsley},
  year={2013},
  publisher={John Wiley \& Sons}
}