\begin{thebibliography}{12}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{Danielyetal2016}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock 29, 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2016/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf}.

\bibitem[Hayou et~al.(2019)Hayou, Doucet, and Rousseau]{Hayouetal2019}
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau.
\newblock On the impact of the activation function on deep neural networks
  training.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  2672--2680. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/hayou19a.html}.

\bibitem[Hayou et~al.(2022)Hayou, Doucet, and Rousseau]{Hayouetal2022}
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau.
\newblock The curse of depth in kernel regime.
\newblock In Melanie~F. Pradier, Aaron Schein, Stephanie Hyland, Francisco
  J.~R. Ruiz, and Jessica~Z. Forde, editors, \emph{Proceedings on "I (Still)
  Can't Believe It's Not Better!" at NeurIPS 2021 Workshops}, volume 163 of
  \emph{Proceedings of Machine Learning Research}, pages 41--47. PMLR, 13 Dec
  2022.
\newblock URL \url{https://proceedings.mlr.press/v163/hayou22a.html}.

\bibitem[Horv{\'a}th(2023)]{Horvath2023}
L{\'a}szl{\'o} Horv{\'a}th.
\newblock Integral inequalities using signed measures corresponding to
  majorization.
\newblock \emph{Revista de la Real Academia de Ciencias Exactas, F{\'i}sicas y
  Naturales. Serie A. Matem{\'a}ticas}, 117\penalty0 (2):\penalty0 80, Mar
  2023.
\newblock ISSN 1579-1505.
\newblock \doi{10.1007/s13398-023-01409-7}.
\newblock URL \url{https://doi.org/10.1007/s13398-023-01409-7}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{Jacotetal2018}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}.

\bibitem[{Jakovčević Stor} et~al.(2015){Jakovčević Stor}, Slapničar, and
  Barlow]{Jakovcevicetal2015}
N.~{Jakovčević Stor}, I.~Slapničar, and J.L. Barlow.
\newblock Forward stable eigenvalue decomposition of rank-one modifications of
  diagonal matrices.
\newblock \emph{Linear Algebra and its Applications}, 487:\penalty0 301--315,
  2015.
\newblock ISSN 0024-3795.
\newblock \doi{https://doi.org/10.1016/j.laa.2015.09.025}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0024379515005406}.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{Pooleetal2016}
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya
  Ganguli.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf}.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{Schoenholzetal2017}
Samuel~S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=H1W1UN9gg}.

\bibitem[Seleznova and Kutyniok(2022)]{Seleznovaetal22}
Mariia Seleznova and Gitta Kutyniok.
\newblock Neural tangent kernel beyond the infinite-width limit: Effects of
  depth and initialization.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 19522--19560. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/seleznova22a.html}.

\bibitem[Terj\'ek and Gonz\'alez-S\'anchez(2025)]{mlpsateoc2}
D\'avid Terj\'ek and Diego Gonz\'alez-S\'anchez.
\newblock {MLP}s at the {EOC}: Concentration of the {NTK}, 2025.

\bibitem[Xiao et~al.(2020)Xiao, Pennington, and Schoenholz]{Xiaoetal2020}
Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz.
\newblock Disentangling trainability and generalization in deep neural
  networks.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 10462--10472. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/xiao20b.html}.

\bibitem[Yang et~al.(2024)Yang, Yu, Zhu, and Hayou]{Yangetal2024}
Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou.
\newblock Tensor programs {VI}: Feature learning in infinite depth neural
  networks.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=17pVDnpwwl}.

\end{thebibliography}
