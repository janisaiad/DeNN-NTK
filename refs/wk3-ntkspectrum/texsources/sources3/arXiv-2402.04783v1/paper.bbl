\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  242--252. PMLR, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  322--332. PMLR, 2019.

\bibitem[Chen et~al.(2021)Chen, Liu, and Wang]{chen2021learning}
Chen, Y., Liu, S., and Wang, X.
\newblock Learning continuous image representation with local implicit image function.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  8628--8638, 2021.

\bibitem[Chng et~al.(2022)Chng, Ramasinghe, Sherrah, and Lucey]{chng2022gaussian}
Chng, S.-F., Ramasinghe, S., Sherrah, J., and Lucey, S.
\newblock Gaussian activated neural radiance fields for high fidelity reconstruction and pose estimation.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXIII}, pp.\  264--280. Springer, 2022.

\bibitem[Davidson \& Szarek(2001)Davidson and Szarek]{davidson2001local}
Davidson, K.~R. and Szarek, S.~J.
\newblock Local operator theory, random matrices and banach spaces.
\newblock \emph{Handbook of the geometry of Banach spaces}, 1\penalty0 (317-366):\penalty0 131, 2001.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pp.\  1675--1685. PMLR, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Gerace et~al.(2020)Gerace, Loureiro, Krzakala, M{\'e}zard, and Zdeborov{\'a}]{gerace2020generalisation}
Gerace, F., Loureiro, B., Krzakala, F., M{\'e}zard, M., and Zdeborov{\'a}, L.
\newblock Generalisation error in learning with random features and the hidden manifold model.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3452--3462. PMLR, 2020.

\bibitem[Hastie et~al.(2022)Hastie, Montanari, Rosset, and Tibshirani]{hastie2022surprises}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (2):\penalty0 949--986, 2022.

\bibitem[Horn et~al.(1994)Horn, Horn, and Johnson]{horn1994topics}
Horn, R.~A., Horn, R.~A., and Johnson, C.~R.
\newblock \emph{Topics in matrix analysis}.
\newblock Cambridge university press, 1994.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA, 2000. Morgan Kaufmann.

\bibitem[Li et~al.(2022)Li, Li, Sitzmann, Agrawal, and Torralba]{li20223d}
Li, Y., Li, S., Sitzmann, V., Agrawal, P., and Torralba, A.
\newblock 3d neural scene representations for visuomotor control.
\newblock In \emph{Conference on Robot Learning}, pp.\  112--123. PMLR, 2022.

\bibitem[Liao \& Couillet(2018)Liao and Couillet]{liao2018spectrum}
Liao, Z. and Couillet, R.
\newblock On the spectrum of random features maps of high dimensional data.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3063--3071. PMLR, 2018.

\bibitem[Liao et~al.(2020)Liao, Couillet, and Mahoney]{liao2020random}
Liao, Z., Couillet, R., and Mahoney, M.~W.
\newblock A random matrix analysis of random fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding double descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 13939--13950, 2020.

\bibitem[Mildenhall et~al.(2021)Mildenhall, Srinivasan, Tancik, Barron, Ramamoorthi, and Ng]{mildenhall2021nerf}
Mildenhall, B., Srinivasan, P.~P., Tancik, M., Barron, J.~T., Ramamoorthi, R., and Ng, R.
\newblock Nerf: Representing scenes as neural radiance fields for view synthesis.
\newblock \emph{Communications of the ACM}, 65\penalty0 (1):\penalty0 99--106, 2021.

\bibitem[Montanari \& Zhong(2022)Montanari and Zhong]{montanari2022interpolation}
Montanari, A. and Zhong, Y.
\newblock The interpolation phase transition in neural networks: Memorization and generalization under lazy training.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (5):\penalty0 2816--2847, 2022.

\bibitem[Nguyen et~al.(2021)Nguyen, Mondelli, and Montufar]{nguyen2021tight}
Nguyen, Q., Mondelli, M., and Montufar, G.~F.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  8119--8129. PMLR, 2021.

\bibitem[Nguyen \& Mondelli(2020)Nguyen and Mondelli]{nguyen2020global}
Nguyen, Q.~N. and Mondelli, M.
\newblock Global convergence of deep networks with one wide layer followed by pyramidal topology.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 11961--11972, 2020.

\bibitem[Oymak \& Soltanolkotabi(2020)Oymak and Soltanolkotabi]{oymak2020toward}
Oymak, S. and Soltanolkotabi, M.
\newblock Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 1\penalty0 (1):\penalty0 84--105, 2020.

\bibitem[Pennington \& Worah(2017)Pennington and Worah]{pennington2017nonlinear}
Pennington, J. and Worah, P.
\newblock Nonlinear random matrix theory for deep learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Pennington et~al.(2018)Pennington, Schoenholz, and Ganguli]{pennington2018emergence}
Pennington, J., Schoenholz, S., and Ganguli, S.
\newblock The emergence of spectral universality in deep networks.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  1924--1932. PMLR, 2018.

\bibitem[Ramasinghe \& Lucey(2022)Ramasinghe and Lucey]{ramasinghe2022beyond}
Ramasinghe, S. and Lucey, S.
\newblock Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXIII}, pp.\  142--158. Springer, 2022.

\bibitem[Schur(1911)]{schur1911bemerkungen}
Schur, J.
\newblock Bemerkungen zur theorie der beschr{\"a}nkten bilinearformen mit unendlich vielen ver{\"a}nderlichen.
\newblock 1911.

\bibitem[Sitzmann et~al.(2020)Sitzmann, Martel, Bergman, Lindell, and Wetzstein]{sitzmann2020implicit}
Sitzmann, V., Martel, J., Bergman, A., Lindell, D., and Wetzstein, G.
\newblock Implicit neural representations with periodic activation functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 7462--7473, 2020.

\bibitem[Skorokhodov et~al.(2021)Skorokhodov, Ignatyev, and Elhoseiny]{skorokhodov2021adversarial}
Skorokhodov, I., Ignatyev, S., and Elhoseiny, M.
\newblock Adversarial generation of continuous images.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  10753--10764, 2021.

\bibitem[Song \& Yang(2019)Song and Yang]{song2019quadratic}
Song, Z. and Yang, X.
\newblock Quadratic suffices for over-parametrization via matrix chernoff bound.
\newblock \emph{arXiv preprint arXiv:1906.03593}, 2019.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil, Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R.
\newblock Fourier features let networks learn high frequency functions in low dimensional domains.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 7537--7547, 2020.

\bibitem[Vershynin(2018)]{vershynin2018high}
Vershynin, R.
\newblock \emph{High-dimensional probability: An introduction with applications in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Virmaux \& Scaman(2018)Virmaux and Scaman]{virmaux2018lipschitz}
Virmaux, A. and Scaman, K.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Zou \& Gu(2019)Zou and Gu]{zou2019improved}
Zou, D. and Gu, Q.
\newblock An improved analysis of training over-parameterized deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
