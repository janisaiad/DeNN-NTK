\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usetikzlibrary{math}

% Mathematical commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\evmin}[1]{\lambda_{\min}\left(#1\right)}
\newcommand{\evmax}[1]{\lambda_{\max}\left(#1\right)}
\newcommand{\svmin}[1]{\sigma_{\min}\left(#1\right)}
\newcommand{\tr}{\text{tr}}
\newcommand{\KNTK}{K_{\text{NTK}}}
\newcommand{\Kinf}{K^{\infty}}
\newcommand{\Sd}{\mathbb{S}^{d-1}}
\newcommand{\Lap}{\Delta}
\newcommand{\Ls}{\mathcal{L}_s}
\newcommand{\limiting}[1]{#1^{\infty}}

\usetheme{Madrid}
\usecolortheme{default}

\title{Spectral Analysis of the Neural Tangent Kernel}
\subtitle{Sobolev Training and Eigenvalue Scaling Laws}
\author{Comprehensive Synthesis and Analysis}
\date{\today}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\section{Introduction and Objectives}

\begin{frame}{Key Objectives and Scaling Laws}
\textbf{Three fundamental objectives:}
\begin{enumerate}
\item \textbf{Eigenvalue scaling laws}: Derive decay rates $\mu_\ell \sim \ell^{-\alpha}$ for NTK operator eigenvalues
\item \textbf{Spectral impact on learning}: Understand how spectral properties determine learning dynamics
\item \textbf{Matrix vs. Operator relationship}: Analyze scaling laws for discrete matrix eigenvalues with respect to depth $l$ and data size $n$
\end{enumerate}

\textbf{NTK Definition:}
$$K^{\infty}(x_i, x_j) = \left\langle \frac{\partial f(\mathbf{x}_i; \theta)}{\partial \theta}, \frac{\partial f(\mathbf{x}_j; \theta)}{\partial \theta} \right\rangle$$
\end{frame}

\begin{frame}{Motivation: Optimization Bounds under NTK Perspective}
\begin{itemize}
\item \textbf{Primary objective:} Establish optimization bounds under NTK perspective for neural network regression and PDE solving - like showing that with probability $1-\delta$, after time $t$ of training, $\|f_t - f^*\|_2 \leq \epsilon$ for appropriate choices of network width and depth

\item \textbf{Existing foundation:} Yang \& He established robust generalization and approximation bounds for deep super-ReLU networks under Sobolev loss

\item \textbf{Critical gap:} Optimization bounds are largely unexplored in the feature learning regime and after the initialization - we know generalization and approximation capabilities but not training efficiency
\item \textbf{Research impact:} Particularly relevant for PDE applications where fine control of Fourier components determines numerical solution quality, but optimization bounds remain largely unexplored under Sobolev loss settings
\end{itemize}
\end{frame}

\begin{frame}{Research Goals and NTK Perspective}
\begin{itemize}
\item \textbf{Convergence rates:} Establish comprehensive understanding of how NTK spectral properties determine optimization convergence rates in practice

\item \textbf{Matrix conditioning:} Characterize intricate relationships between network depth, data size, and optimization difficulty through matrix analysis

\item \textbf{Frequency control:} Proved that Sobolev training modifies the underlying spectrum in a disentangled manner - the composite operator $\mathcal{T}_s = K^{\infty} \circ P_s$ factorizes independently on both sphere and torus domains, enabling precise control of frequency components

\item \textbf{PDE applications:} Direct relevance for Deep Ritz Method, Deep Galerkin Method, and Physics-Informed Neural Networks (PINNs) where spectral properties determine convergence rates and solution accuracy
\end{itemize}
\end{frame}

\section{Optimization Bounds via Spectral Factorization}

\begin{frame}{Fundamental Factorization Strategy}
\begin{itemize}
\item \textbf{Key factorization:} Sobolev-modified learning operator as $\mathcal{T}_s = K^{\infty} \circ P_s$ disentangles architecture from regularization effects

\item \textbf{Commutation property:} $[K^{\infty}, P_s] = 0$ from rotational invariance allows separate study of each component
\item \textbf{Spectral factorization:} The NTK data matrix spectrum remains separable as a product $\lambda_i(KP_s) = \lambda_i(K) \cdot \lambda_i(P_s)$, allowing us to disentangle architectural effects (through $K$) from loss function effects (through $P_s$)

\item \textbf{Training dynamics:} This spectral factorization dictates the training trajectory, with architectural choices affecting $\lambda_i(K)$ - including depth $L$, width $N$, and sophisticated profiles like $m_\ell = m\ell^2$ for MLPs at edge of chaos - while Sobolev order $s$ independently controls $\lambda_i(P_s)$
\end{itemize}
\end{frame}


\begin{frame}{Choice of Spherical Domain}
\textbf{Focus on spherical domains} $\mathbb{S}^{d-1}$:

\textbf{Advantages:}
\begin{itemize}
\item Computational tractability via spherical harmonic symmetrization
\item Explicit spectral decompositions
\item Rotational invariance
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
\item No uniform sampling measure on the sphere
\item Requires spectrum analysis via inverse cosine distance matrix approximation
\item Motivates exploration of alternative domains
\end{itemize}

\textbf{Crucial distinction:}
\begin{itemize}
\item \textbf{NTK Matrix}: Discrete sampled version $K \in \mathbb{R}^{n \times n}$
\item \textbf{NTK Operator}: Continuous integral operator $(\mathcal{L}f)(x) = \int K^{\infty}(x,y)f(y)dy$
\end{itemize}
\end{frame}

\begin{frame}{Initialization: Edge of Chaos}
\textbf{All analysis assumes Edge of Chaos (EOC) initialization:}

\begin{itemize}
\item Weights initialized as $w_{ij} \sim \mathcal{N}(0, \sigma_w^2/\text{fan-in})$
\item For ReLU networks: $\sigma_w^2 = 2$ to maintain unit variance
\item Prevents activation explosion or vanishing with depth
\end{itemize}

\textbf{Key parameter:} $\Delta_\phi = \frac{b^2}{a^2+b^2} = 0.5$ (for standard ReLU)

\textbf{Cosine map:}
\[ \varrho(\rho) = \rho + \Delta_\phi \frac{2}{\pi}\left( \sqrt{1-\rho^2} - \rho \arccos(\rho) \right) \]
\end{frame}

\section{NTK Matrix Structure and Spectrum}

\begin{frame}{NTK Matrix Structure}
\textbf{Example for 3 points} $x_1, x_2, x_3 \in \mathbb{R}^d$:
\[
K^{\infty} = \begin{pmatrix} 
k(x_1,x_1) & k(x_1,x_2) & k(x_1,x_3) \\
k(x_2,x_1) & k(x_2,x_2) & k(x_2,x_3) \\
k(x_3,x_1) & k(x_3,x_2) & k(x_3,x_3)
\end{pmatrix}
\]

\textbf{General NTK for depth $l$ networks:}
\begin{align*}
K^{\infty}(\mathbf{x}_1, \mathbf{x}_2) &= \|\mathbf{x}_1\| \|\mathbf{x}_2\| \left( \sum_{k=1}^l \varrho^{\circ (k-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right) \right. \\
&\quad \left. \times \prod_{k'=k}^{l-1} \varrho'\left(\varrho^{\circ (k'-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right)\right) \right) \mathbf{I}_{m_l}
\end{align*}

\textbf{Special case - 2-layer ReLU networks:}
\[
k(x_i,x_j) = x_i^T x_j \cdot \arccos(-\langle x_i,x_j \rangle) + \sqrt{1-\langle x_i,x_j \rangle^2}
\]
\end{frame}

\begin{frame}{NTK Operator and Properties}
\textbf{NTK operator action:}
\[
(K^{\infty} f)(x) = \int_{\mathbb{S}^{d-1}} k(x,y)f(y)d\sigma(y)
\]

\textbf{Key properties:}
\begin{itemize}
\item Symmetric positive definite operator
\item Eigenfunctions: spherical harmonics $Y_{\ell,p}$
\item Eigenvalues decay polynomially: $\mu_\ell \sim \ell^{-d}$
\end{itemize}

\textbf{Matrix spectrum results:}

\textbf{Condition number:}
\[ \kappa(K^{\infty}) \sim 1 + \frac{n}{3} + \mathcal{O}(n \xi / l) \]

\textbf{Eigenvalue distribution:}
\[ \lambda_{\text{min}} \sim \frac{3l}{4n}, \quad \lambda_{\text{max}} \sim \frac{3l}{4} \pm \xi \text{ where } \xi \sim \log(l) \]
\end{frame}

\section{Sobolev Training and Spectral Modification}

\begin{frame}{NTK-Sobolev Operator Framework}
\textbf{Key innovation:} Modification of the standard $L^2$ loss to incorporate high-order derivatives.

\textbf{Spherical harmonics transform:} $\mathcal{F}$: $L^2(\mathbb{S}^d) \to \bigoplus_{\ell=0}^{\infty} \mathbb{C}^{N(d,\ell)}$

\textbf{Sobolev operator $P_s$ defined in Fourier space:}
\[ P_s = \sum_{\ell=0}^{\ell_{\max}} \sum_{p=1}^{N(d,\ell)} (1+\ell)^{2s}P_{\ell,p} \]

where $P_{\ell,p} = a_{\ell,p}a_{\ell,p}^T$ with spectral coefficients $(a_{\ell,p})_i = c_iY_{\ell,p}(x_i)$.

\textbf{Dimension of spherical harmonic spaces:}
\[ N(d,\ell) \sim \frac{2\ell^{d-2}}{(d-2)!} \quad \text{as } \ell \to \infty \]
\end{frame}

\begin{frame}{Commutation Property}
\begin{theorem}[Spherical Harmonics as Common Eigenfunctions]
For rotationally invariant kernels on $\mathbb{S}^{d-1}$:
\begin{itemize}
\item NTK operator $K^{\infty}$ and Sobolev operator $P_s$ share spherical harmonics $Y_{\ell,p}$ as eigenfunctions
\item Due to rotational invariance and Schur's lemma
\item Enables direct spectral modification analysis
\end{itemize}
\end{theorem}

\begin{theorem}[Commutation Property]
The NTK operator $K^{\infty}$ and Sobolev operator $P_s$ commute:
\[ [K^{\infty}, P_s] = 0 \]
This holds for any sampling distribution $\rho(x)$ on $\mathbb{S}^{d-1}$.
\end{theorem}
\end{frame}

\section{Main Proofs}

\begin{frame}{Proof 1: Sobolev Loss as Fractional Laplacian}
\begin{theorem}[Fractional Laplacian Representation of Sobolev Loss]
For a function $f \in H^s(\mathbb{S}^{d-1})$ with $s > 0$, the Sobolev loss can be written as:
\[ \mathcal{L}_s[f] = \int_{\mathbb{S}^{d-1}} f(x) (I + (-\Delta)^{1/2})^s f(x) d\sigma(x) \]
where $(I + (-\Delta)^{1/2})^s$ is the fractional Laplacian operator of order $s$.
\end{theorem}

\textbf{Proof sketch:}
\begin{itemize}
\item Spherical harmonic expansion: $f(x) = \sum_{\ell,p} \hat{f}_{\ell,p} Y_{\ell,p}(x)$
\item Laplacian action: $(-\Delta)^{1/2}_{\mathbb{S}^{d-1}} Y_{\ell,p}(x) = \sqrt{\ell(\ell + d - 2)} Y_{\ell,p}(x)$
\item Bilinear form via orthonormality of spherical harmonics
\end{itemize}
\end{frame}

\begin{frame}{Proof 2: NTK Operator Multiplication by Sobolev Operator}
\begin{theorem}[NTK-Sobolev Composition]
Under Sobolev training, the learning operator is given by the composition:
\[ \mathcal{T}_s = K^{\infty} \circ (I + (-\Delta)^{1/2})^s \]
\end{theorem}

\textbf{Proof:}
\begin{itemize}
\item Standard gradient descent: $\frac{df}{dt} = -K^{\infty}(f - y)$
\item Sobolev loss: $\mathcal{L}_s(\theta) = \frac{1}{2}\|f(\cdot; \theta) - y\|^2_{H^s}$
\item Chain rule via fractional operator
\item Resulting dynamics: $\frac{df}{dt} = -K^{\infty} \circ (I + (-\Delta)^{1/2})^s (f - y)$
\end{itemize}
\end{frame}

\begin{frame}{Proof 3: Spectral Properties of the Composite Operator}
\begin{theorem}[Spectrum of NTK-Sobolev Operator]
The eigenvalues of the composite operator $\mathcal{T}_s = K^{\infty} \circ (I + (-\Delta)^{1/2})^s$ are given by the product of individual eigenvalues:
\[ \mu_\ell^{(\mathcal{T}_s)} = \mu_\ell^{(K)} \cdot (1 + \sqrt{\ell(\ell + d - 2)})^s \]
\end{theorem}

\textbf{Direct consequence:} Commutation and common eigenfunctions allow simple multiplication of spectra.
\end{frame}

\begin{frame}{Proof 4: Commutation of Discrete Matrix Operators}
\begin{theorem}[Matrix Commutation]
The discrete matrices $K$ and $P_s$ commute: $KP_s = P_sK$.
\end{theorem}

\textbf{Proof:}
\begin{itemize}
\item NTK expansion: $K = \sum_{\ell,p} \mu_\ell^{(K)} a_{\ell,p} a_{\ell,p}^T$
\item Sobolev expansion: $P_s = \sum_{\ell,p} (1 + \sqrt{\ell(\ell + d - 2)})^s a_{\ell,p} a_{\ell,p}^T$
\item Orthogonality: $a_{\ell,p}^T a_{\ell',p'} = \delta_{\ell,\ell'} \delta_{p,p'} \|a_{\ell,p}\|^2$
\item Scalar commutation: $KP_s = P_sK$
\end{itemize}

\textbf{Underlying reason:} Reflects commutation of continuous operators $K^{\infty}$ and $P_s$.
\end{frame}

\begin{frame}{Proof 5: Eigenvalue Scaling Laws}
\begin{theorem}[Asymptotic Scaling Laws]
For the NTK-Sobolev operator, eigenvalues follow the scaling laws:
\[ \lambda_\ell \sim \ell^{-d} \cdot (1 + \sqrt{\ell(\ell + d - 2)})^s \]
\end{theorem}

\textbf{Asymptotic analysis:} For large $\ell$,
\[ \lambda_\ell^{(\mathcal{T}_s)} \sim C(d, L) \ell^{-d} \cdot (1 + \ell)^s = C(d, L) \ell^{s-d} \]

\textbf{Critical spectral behavior:}
\begin{itemize}
\item $s < d$: Eigenvalue decay - regularizing effect
\item $s = d$: Logarithmic corrections - critical regime
\item $s > d$: Eigenvalue growth - high-frequency amplification
\end{itemize}
\end{frame}

\section{Practical Implementation and Discretization}

\begin{frame}{From Sobolev Loss to Discrete Matrix Operator}
\textbf{Two integral formulations:}

\textbf{Formulation 1 - Uniform Lebesgue measure:}
\[ \mathcal{L}_s[f] = \int_{\mathbb{S}^{d-1}} f(x) (I + (-\Delta)^{1/2})^s f(x) d\sigma(x) \]

\textbf{Formulation 2 - Sampling measure:}
\[ \mathcal{L}_s[f] = \int_{\mathbb{S}^{d-1}} f(x) (I + (-\Delta)^{1/2})^s f(x) d\mu_n(x) \]
where $\mu_n = \frac{1}{n}\sum_{i=1}^n \delta_{x_i}$

\textbf{Discrete implementation:}
\[ \mathcal{L}_s[f] = \frac{1}{n}\sum_{i=1}^n f(x_i) \left[(I + (-\Delta)^{1/2})^s f\right](x_i) \]

\textbf{Matrix form:} $\mathcal{L}_s[f] \approx \mathbf{f}^T P_s \mathbf{f}$
\end{frame}

\begin{frame}{Practical Implementation Considerations}
\textbf{Construction of matrix $P_s$:}
\[ (P_s)_{ij} = \sum_{\ell=0}^{\ell_{\max}} \sum_{p=1}^{N(d,\ell)} (1 + \sqrt{\ell(\ell + d - 2)})^s Y_{\ell,p}(x_i) Y_{\ell,p}(x_j) \]

\textbf{Computational complexity:}
\begin{itemize}
\item Spherical harmonics evaluation: $\mathcal{O}(n^2 \ell_{\max}^{d-1})$
\item Truncation strategy: balance spectral accuracy and feasibility
\item Alternative with gradients: $\mathcal{O}(nd)$ instead of $\mathcal{O}(n^2\ell_{\max}^{d-1})$
\end{itemize}

\textbf{Fourier decomposition:}
\[ \mathcal{L}_s[f] = \sum_{\ell=0}^{\infty} \sum_{p=1}^{N(d,\ell)} (1+\ell)^{2s} |\hat{f}_{\ell,p}|^2 \]

\textbf{Dimension growth:}
\[ N(d,\ell) \sim \frac{2\ell^{d-2}}{(d-2)!} \quad \text{determines complexity} \]
\end{frame}

\section{Deep NTK Analysis}

\begin{frame}{Deep NTK Properties}
\textbf{Limiting NTK at EOC for $L$-layer networks:}
\begin{align*}
K^{\infty}(\mathbf{x}_1, \mathbf{x}_2) &= \|\mathbf{x}_1\| \|\mathbf{x}_2\| \left( \sum_{k=1}^l \varrho^{\circ (k-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right) \right. \\
&\quad \left. \times \prod_{k'=k}^{l-1} \varrho'\left(\varrho^{\circ (k'-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right)\right) \right) \mathbf{I}_{m_l}
\end{align*}

\textbf{Eigenvalue decay:} For $L$-layer ReLU networks with $L \geq 3$:
\[ \mu_k \sim C(d, L)k^{-d} \]
where $C(d, L)$ depends on parity of $k$ and grows quadratically with $L$.

\textbf{For normalized NTK} $\kappa^L_{\text{NTK}}/L$: $C(d, L)$ grows linearly with $L$.
\end{frame}

\begin{frame}{Inverse Cosine Distance Matrix Analysis}
\textbf{Inverse cosine distance matrix} $W_k$ for depth $k$:
\[ {W_k}_{i,i} = 0, \quad {W_k}_{i_1,i_2} = \left( \frac{1 - \rho_k(x_{i_1},x_{i_2})}{2} \right)^{-\frac{1}{2}} \text{ for } i_1 \neq i_2 \]

\textbf{Near-affine behavior:}
\begin{itemize}
\item NTK matrix $K^{\infty} \approx A \cdot W_l + B$ (affine dependence)
\item Spectral bounds transfer from $W_k$ to NTK via this affine relationship
\item Error terms: $O(k^{-1})$ - decreases with depth
\end{itemize}

\textbf{Implication:} This relationship enables indirect analysis of NTK spectral properties via simpler geometric matrices.
\end{frame}

\section{Deep Narrow Neural Networks}

\begin{frame}{Scaled NTK at Initialization}
\begin{theorem}[Scaled NTK at Initialization]
For $f^L_\theta$ initialized appropriately, as $L \to \infty$:
\[ \tilde{\Theta}^L_0(x, x') \xrightarrow{p} \tilde{\Theta}^\infty(x, x') \]
where
\[ \tilde{\Theta}^\infty(x, x') = (x^T x' + 1 + \E_g[\sigma(g(x))\sigma(g(x'))]) I_{d_{out}} \]
with $g \sim \text{GP}(0, \rho^2 d_{in}^{-1} x^T x' + \beta^2)$.
\end{theorem}

\textbf{Alternative formulation:} $\kappa_1(\cos(u) \cdot v)$ where:
\begin{itemize}
\item $v = \frac{1}{1 + \beta^2/\alpha^2}$, where $\beta$ is bias variance
\item $\alpha = \frac{\|x\| \|x'\| \rho}{d_{in}}$, where $\cos(u)$ is cosine distance
\end{itemize}
\end{frame}

\begin{frame}{Comparison with Two-Layer Kernel}
\textbf{Infinite-width two-layer ReLU NTK on the sphere:}
\[
  K^{(2)}(x,y)=\|x\|\,\|y\|\,\Bigl(\tfrac{\pi-\theta}{\pi}\,\cos \theta + \tfrac{\sin\theta}{\pi}\Bigr)
\]
where $\theta=\arccos(\langle x,y\rangle)$. Spherical harmonic eigenvalues:
\[
  \mu_\ell^{(2)}\;\asymp\;\ell^{-(d+1)}
\]

\textbf{In the deep-narrow limit:}
\[
   \mu_\ell^{(\text{dn})}\;\asymp\;C(L,d)\,\ell^{-d}, \qquad C(L,d)\propto L
\]

\textbf{Consequences:}
\begin{itemize}
  \item Maximum eigenvalue of $K^{(\text{dn})}$ about $L$ times larger than for $K^{(2)}$
  \item Flatter spectrum: $\kappa\bigl(K^{(\text{dn})}\bigr)\approx\kappa\bigl(K^{(2)}\bigr)/L$
\end{itemize}

\textbf{Conclusion:} Depth partially compensates for poor conditioning of the two-layer kernel.
\end{frame}

\begin{frame}{Research Perspectives for this immediate findings}
\textbf{Architectural modifications:}
\begin{itemize}
\item Unit concatenation: Combine multiple narrow networks
\item Skip connections: ResNet-style connections
\end{itemize}

\textbf{Initialization studies:}
\begin{itemize}
\item Alternative initializations
\item $\beta \to 0$ limit: $v \to 1$ when bias variance vanishes
\item Complex kernel structures
\end{itemize}

\textbf{Theoretical extensions:}
\begin{itemize}
\item Extension of Hayou \& Yang's work on ResNets (but in a mean field approach that is not easily generalizable)
\item Mean field analysis for deep narrow frameworks
\item Experimental validation on practical tasks
\end{itemize}
\end{frame}



\section{Alternative Domains (my proper contribution)}

Those questions were told to be interesting but not to be investigated now in the literature.
To apply integration by parts well, we need to have a domain without a boundary !



\begin{frame}{Gaussian Domain: $L^2(\mathbb{R}^d, \mu)$}
\textbf{Framework:} Gaussian measure $d\mu = (2\pi)^{-d/2} e^{-\|x\|^2/2} dx$

\textbf{Ornstein-Uhlenbeck operator:} $\mathcal{L}_{OU} = -\Delta + x \cdot \nabla$

\textbf{Eigenfunctions:} Hermite polynomials $H_{\alpha}(x)$ with eigenvalues $|\alpha|$

\textbf{NTK is not translation-invariant} $K^{\infty}(x, y) = k(x - y)$ but it is invariant under rotations

\textbf{Composite spectrum:} $\lambda_\alpha \sim \hat{k}(|\alpha|^{1/2}) \cdot (1 + |\alpha|)^s$

\textbf{Critical insight:} Gaussian measure introduces natural regularization via exponential decay of the kernel's Fourier transform.
\end{frame}



\begin{frame}{Toroidal Domain: $\mathbb{T}^d$ (proper contribution)}

  Suppose you have a dataset over $[0,1]^d$ and you want to learn a function on this domain.

  You can use a neural network with periodic boundary conditions, you glue opposite faces of the unit cube $[0,1]^d$ to get a torus.

\textbf{Fourier basis:} Fourier modes as orthonormal basis
\[
   e_k(x) \;=\; e^{2\pi i k\cdot x}, \qquad k\in\mathbb Z^d
\]

\textbf{Sobolev operator:} Diagonal in this basis
\[
   P_s^{(\mathbb{T}^d)}\,e_k\;=\;\bigl(1+\|k\|^2\bigr)^{s}\,e_k
\]

\textbf{Composite spectrum:} $\lambda_k^{(KP_s)} = \widehat{k}_{\text{per}}(k)\,(1+\|k\|^2)^{s} \sim \|k\|^{2s-\alpha}$

\textbf{Computational cost:} FFT enables $\mathcal O\bigl(N^d\log N\bigr)$ for matrix-vector products.
But I've made computations by hand, we can do very fast experiments for a whole range of init/arch/functions and this case is not yet validated in the literature.
(done in the report)
\end{frame}



\section{Reconciliation and Unified Framework (my proper contribution)}

\begin{frame}{Unified Spectral Viewpoint}
\textbf{Reconciling the geometric view of NTK and functional view of Sobolev training:}

\textbf{Shared invariance:}
\begin{itemize}
\item Matrices $K$ and $P_s$ rotationally invariant
\item Entries depend only on inner product $\langle x_i, x_j \rangle$
\end{itemize}

\textbf{Proposed strategy:}
\begin{enumerate}
\item Approximate NTK spectrum via inverse cosine distance matrix
\item Analyze Sobolev operator spectrum
\item Combine via spectrum product and validate experimentally
\end{enumerate}
\end{frame}

\begin{frame}{Zonal Kernel Representation of $P_s$}
\begin{theorem}[Zonal Kernel Representation]
The Sobolev matrix $P_s$ can be written as a zonal kernel:
\[ (P_s)_{ij} = p_s(\langle x_i, x_j \rangle) \]
where
\[ p_s(t) = \sum_{\ell=0}^{\ell_{\max}} (1 + \sqrt{\ell(\ell + d - 2)})^s \frac{N(d,\ell)}{\text{Area}(\mathbb{S}^{d-1})} P_\ell^{((d-2)/2)}(t) \]
\end{theorem}

\textbf{Eigenvalue bounds for $P_s$:}
\[ 1 \leq \lambda_i(P_s) \leq (1 + \ell_{\max})^s \]

\textbf{Condition number:} $\kappa(P_s) = (1 + \ell_{\max})^s$

\textbf{Practical implications:}
\begin{itemize}
\item Zonal kernel $p_s(t)$ can be precomputed
\item Gegenbauer expansion enables fast multipole-type acceleration
\item Numerical stability analysis guided by condition number
\end{itemize}
\end{frame}

\section{Conclusions and Future Directions}

\begin{frame}{Summary of Key Results}
\textbf{Main contributions:}

\begin{enumerate}
\item \textbf{Eigenvalue scaling laws}: Relations $\mu_\ell \sim \ell^{-\alpha}$ and impact on learning dynamics

\item \textbf{Matrix vs. Operator distinction}: Fundamental difference between discrete and continuous eigenvalues


\item \textbf{Deep network analysis}: Scaling laws for condition numbers and eigenvalue distributions

\item \textbf{Spherical harmonic framework}: Commutation property $[K^{\infty}, P_s] = 0$ and common eigenfunctions
\end{enumerate}

\textbf{Key formula:} $\lambda_\ell \sim \ell^{s-d}$ determines spectral properties and learning dynamics.
\end{frame}



\begin{frame}{Immediate simple challenges and open questions (not to investigate now)}
\begin{enumerate}
\item \textbf{Unified framework}: Different papers with varying domains, initializations, architectures, activations

\item Extension of harmonic analysis to general spaces $L^2(\gamma)$

\item  Extension from ReLU to general inhomogeneous activations

\item \textbf{Experimental validation}: Systematic verification of theoretical predictions because some of the results are not yet validated
\end{enumerate}

\textbf{Sobolev perspective:} Increase exponent $s$ with data size $n$ to counterbalance conditioning of $K$. Precise estimate of $\kappa(P_s)$ remains an open problem.
\end{frame}



  
\begin{frame}{Component-wise Investigation}
  \begin{itemize}
  \item \textbf{Disentangling NTK and Sobolev:} Factorization $\mathcal{T}_s = K^{\infty} \circ P_s$ simplifies research by enabling independent investigation of each component
  
  \item For a specific domain, we can investigate the spectrum of $P_s$ via experimental frameworks with Fourier analysis
  
  \item Study of $K^{\infty}$ under finite-width corrections opens new questions about lazy training regime
  \end{itemize}
  \end{frame}



\begin{frame}{Perturbation Theory Framework}
  \begin{itemize}
    \item Systematic expansion in powers of $1/n$ for computation of training dynamics
    
    \item Experimental validation of $\mathcal{O}(1/n)$ training dynamics under gradient flow
    
    \item Finite-width networks require analysis beyond leading-order asymptotics
    
    \item Diagrammatic techniques reveal correlation function scaling at infinite width
    
    \item Three key modifications to infinite-width behavior:
    \begin{itemize}
      \item Initial NTK $\Theta_0$ receives width-dependent corrections
      \item Network updates become nonlinear in learning rate
      \item NTK becomes time-dependent during training
    \end{itemize}
    
    \item Perturbative expansion: $H = H_0 + \frac{1}{n}H_1 + \frac{1}{n^2}H_2 + ...$
    
    \item Each $H_k$ term captures neuron interactions at finite width
    
    \item Expansion controls finite-size effects in optimization
  \end{itemize}
\end{frame}
  


\begin{frame}{Finite-Width Corrections and NTK Hierarchy}
  \textbf{Finite-width NTK expansion:}
  \begin{align}
  \Theta^{NTH}(x_1, x_2) &= \Theta(x_1, x_2) + n^{-1}\mathbb{E}[\mathcal{O}^{(1)}_{2,0}(x_1, x_2)] \nonumber \\
  &\quad - n^{-1}\mathbb{E}[\mathcal{O}^{(1)}_{3,0}(x_1, x_2, \vec{x})\Theta^{-1}(\vec{x}, \vec{x})f^{(0)}_0(\vec{x})] \nonumber \\
  &\quad + n^{-1}\vec{y}^T \Theta^{-1}(\vec{x}, \vec{x})\mathbb{E}[\mathcal{O}^{(1)}_{4,0}(x_1, x_2, \vec{x}, \vec{x})] \Theta^{-1}(\vec{x}, \vec{x})\vec{y} \nonumber
  \end{align}
  
  \begin{itemize}
  \item \textbf{Degradation capture:} Corrections show how idealized infinite-width behavior degrades in practice
  
  \item \textbf{Mathematical machinery:} Requires tensor algebra and careful treatment of statistical dependencies
  
  \item \textbf{Interaction terms:} Tensors $\mathcal{O}^{(1)}_{4,0}$ and $\mathcal{O}^{(1)}_{3,0}$ represent higher-order interactions affecting spectral properties
  
  \item \textbf{Impact analysis:} Detailed investigation needed to understand effect on optimization dynamics
  \end{itemize}
  \end{frame}



  \begin{frame}{Advanced Mathematical Tools and Deeper-vs-Wider Question}
  \begin{itemize}
  \item \textbf{What we already know:} We already have spectrum estimates for the NTK matrix in infinite-width, we need to treat the other terms
  
  \item \textbf{Higher-order corrections:} Higher orders capture complex neuronal interactions at finite width
  
  \item \textbf{Deeper-vs-wider:} Then we fix parameter budget $P$, express corrections in terms of depth $L$ and width $N = P/L$
  
  \item \textbf{Optimization guarantees:} Analytically determine whether increasing depth or width provides better optimization guarantees
  \end{itemize}
  \end{frame}
  
  
  \begin{frame}{NTK Analysis without Lazy Training Regime}
  \begin{itemize}
  \item \textbf{New territory:} From that investigation of NTK matrix $K^{\infty}$ we can extend to the feature learning regime
  
  \item \textbf{Existing approaches:} Previous work (e.g., Banerjee et al.) bounds $\lambda_{\min}(K)$ but doesn't assume NTK regime operation
  
  \item \textbf{Key difference:} Our approach explicitly assumes NTK regime validity and investigates finite-width corrections within this setting
  
  \item \textbf{Novel questions:} Opens theoretical questions about interplay between kernel constancy, spectral properties, and architectural parameters
  \end{itemize}
  \end{frame}



\begin{frame}{Research Roadmap}
\textbf{Near-term objectives:}
\begin{itemize}
\item Compute the NTK pertubation matrix to get immediate spectrum bounds for finite width
\item Also Unify initialization schemes (for eg no bias and EOC) and architectural assumptions
\item Study narrow NTK behavior to identify complexifications
\end{itemize}

\textbf{Long-term goals:}
\begin{itemize}
\item $P_s$ matrix is a zonal kernel, can be investigated trying to get a better conditionning number multiplicative constant
\item Develop more general theory for deep narrow networks (for other initialization without UAT)
\end{itemize}

\textbf{Vision:} The convergence of spectral analysis, harmonic analysis, and neural network theory opens promising avenues for understanding and controlling learning dynamics of deep networks via their spectral properties.
\end{frame}

\begin{frame}{Acknowledgments and Questions}
\begin{center}
\textbf{Thank you for your attention!}

\vspace{1cm}

\textbf{Questions and Discussion}

\vspace{1cm}

\end{center}
\end{frame}

\end{document}
