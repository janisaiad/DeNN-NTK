\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{tocloft}
\usepackage{tikz}
% Page setup
\geometry{a4paper, margin=2.5cm}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{NTK Eigenvalue Bounds: Comprehensive Analysis}
\fancyhead[R]{\thepage}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}

% Mathematical commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\evmin}[1]{\lambda_{\min}\left(#1\right)}
\newcommand{\evmax}[1]{\lambda_{\max}\left(#1\right)}
\newcommand{\svmin}[1]{\sigma_{\min}\left(#1\right)}
\newcommand{\tr}{\text{tr}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\u}{\mathbf{u}}
\newcommand{\v}{\mathbf{v}}
\newcommand{\odot}{\odot}
\newcommand{\bigOmg}{\Omega}
\newcommand{\bigTheta}{\Theta}
\newcommand{\Pr}{\text{Pr}}
\newcommand{\KNTK}{K_{\text{NTK}}}
\newcommand{\KLim}{K^{(L)}}
\newcommand{\KEmp}{\overline{K}^{(L)}}
\newcommand{\KCosine}{K_L}
\newcommand{\lambdaMin}{\lambda_{\min}}
\newcommand{\sigmaMin}{\sigma_{\min}}
\newcommand{\Order}{\mathcal{O}}
\newcommand{\TildeOrder}{\tilde{\mathcal{O}}}
\newcommand{\OmegaNotation}{\Omega}
\newcommand{\TildeOmega}{\tilde{\Omega}}
\newcommand{\cN}{\mathcal{N}}

\title{Spectral Analysis of the Neural Tangent Kernel:\\
Sobolev Training and Eigenvalue Scaling Laws}

\author{Synthesis and Analysis}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document provides a comprehensive analysis of the Neural Tangent Kernel (NTK) spectrum and its modification through Sobolev-type training. We present eigenvalue scaling laws with respect to decay rates $\mu_\ell \sim \ell^{-\alpha}$, showing how spectral properties impact learning dynamics. The analysis covers both the discrete NTK matrix spectrum and continuous NTK operator eigenvalues, demonstrating how matrix spectral analysis reveals learning behavior. Key results include scaling laws for eigenvalues with respect to network depth $l$ and data size $n$, with condition numbers $\kappa(K^{\infty}) \sim 1 + \frac{n}{3} + \mathcal{O}(n \xi / l)$ and eigenvalue distributions $\lambda \sim \frac{l}{4} \pm \xi$ where $\xi \sim \log(l)$. We show how Sobolev training modifies the spectrum through the operator $P_s$, enabling control over learned frequency components via the spectral exponent $2s-d$. The framework extends to deep narrow networks and includes analysis of inverse cosine distance matrices with near-affine NTK behavior.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction: Eigenvalue Scaling Laws and Learning Dynamics}

The Neural Tangent Kernel (NTK) has emerged as a fundamental tool for understanding the training dynamics of deep neural networks. For a neural network $f(\x; \theta)$ with parameters $\theta$, the NTK is defined as:
$$K^{\infty}(x_i, x_j) = \left\langle \frac{\partial f(\x_i; \theta)}{\partial \theta}, \frac{\partial f(\x_j; \theta)}{\partial \theta} \right\rangle$$

\subsection{Key Objectives and Domain Choice}

This document addresses three fundamental objectives:
\begin{enumerate}
\item Eigenvalue scaling laws: Derive decay rates $\mu_\ell \sim \ell^{-\alpha}$ for NTK operator eigenvalues
\item Spectral impact on learning: Understand how spectral properties determine learning dynamics  
\item Matrix vs. Operator relationship: Analyze scaling laws for discrete matrix eigenvalues with respect to network depth $l$ and data size $n$
\end{enumerate}

\textbf{Focus on Spherical Domains}: Throughout this analysis, we primarily focus on the unit sphere $\mathbb{S}^{d-1}$ as our domain of choice. This choice is motivated by computational tractability through spherical harmonic symmetrization, which provides explicit spectral decompositions. However, this convenience comes at a cost: there is no uniform sampling measure on the sphere, which necessitates analyzing the spectrum indirectly through the inverse cosine distance matrix approximation. This limitation drives our exploration of alternative domains (Gaussian and toroidal) in later sections.

\subsection{NTK Matrix vs. NTK Operator}

A crucial distinction exists between:
\begin{itemize}
\item \textbf{NTK Matrix}: Discrete sampled version $K \in \mathbb{R}^{n \times n}$ with entries $K_{ij} = K^{\infty}(x_i, x_j)$
\item \textbf{NTK Operator}: Continuous integral operator $(\mathcal{L}f)(x) = \int K^{\infty}(x,y)f(y)dy$
\end{itemize}

\textbf{Warning}: The eigenvalues of the sampled NTK matrix are \emph{not} the same as the NTK operator eigenvalues. The matrix spectrum provides discrete approximations that depend on the sampling strategy and data distribution.

\subsection{Initialization: Edge of Chaos}

All analysis assumes Edge of Chaos (EOC) initialization:
\begin{itemize}
\item Initialize weights as $w_{ij} \sim \mathcal{N}(0, \sigma_w^2/\text{fan-in})$ 
\item For ReLU networks: $\sigma_w^2 = 2$ to maintain unit variance through layers
\item This ensures activations neither explode nor vanish with depth
\end{itemize}

\newpage

\section{NTK Matrix Structure and Spectrum}

\subsection{Example NTK Matrix Structure}

Consider a dataset of 3 points $x_1, x_2, x_3 \in \mathbb{R}^d$. The NTK matrix has entries:
\[
K^{\infty} = \begin{pmatrix} 
k(x_1,x_1) & k(x_1,x_2) & k(x_1,x_3) \\
k(x_2,x_1) & k(x_2,x_2) & k(x_2,x_3) \\
k(x_3,x_1) & k(x_3,x_2) & k(x_3,x_3)
\end{pmatrix}
\]

For general depth $l$ networks at EOC, the NTK kernel function is:
\begin{align}
K^{\infty}(\mathbf{x}_1, \mathbf{x}_2) &= \|\mathbf{x}_1\| \|\mathbf{x}_2\| \left( \sum_{k=1}^l \varrho^{\circ (k-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right) \right. \\
&\quad \left. \times \prod_{k'=k}^{l-1} \varrho'\left(\varrho^{\circ (k'-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right)\right) \right) \mathbf{I}_{m_l}
\end{align}

For 2-layer ReLU networks, this simplifies to:
\[
k(x_i,x_j) = x_i^T x_j \cdot \arccos(-\langle x_i,x_j \rangle) + \sqrt{1-\langle x_i,x_j \rangle^2}
\]

\subsection{NTK Operator Structure}

For functions $f,g \in L^2(\mathbb{S}^{d-1})$, the NTK operator acts as:
\[
(K^{\infty} f)(x) = \int_{\mathbb{S}^{d-1}} k(x,y)f(y)d\sigma(y)
\]

\textbf{Key Properties}:
\begin{itemize}
\item Symmetric positive definite operator
\item Eigenfunctions are spherical harmonics $Y_{\ell,p}$
\item Eigenvalues decay polynomially: $\mu_\ell \sim \ell^{-d}$
\end{itemize}

\subsection{Matrix Spectrum Results}

The key scaling laws for the discrete NTK matrix are:

\textbf{Condition Number}:
\[ \kappa(K^{\infty}) \sim 1 + \frac{n}{3} + \mathcal{O}(n \xi / l) \]
The condition number grows linearly with data size $n$ but improves with depth $l$.

\textbf{Eigenvalue Distribution}:
\[ \lambda_{\text{min}} \sim \frac{3l}{4n}, \quad \lambda_{\text{max}} \sim \frac{3l}{4} \pm \xi \text{ where } \xi \sim \log(l) \]

Both eigenvalues scale linearly with depth, but the condition number grows as $n$.

\textbf{Key Insight}: The training dynamics are dictated by the NTK matrix eigenvalues, not the operator eigenvalues. Deeper networks ($l \uparrow$) improve conditioning but with diminishing returns.

\section{Sobolev Training and NTK Spectral Modification}

\subsection{NTK-Sobolev Operator Framework}

The key innovation in Sobolev training is the modification of the standard $L^2$ loss to incorporate high-order derivatives. The NTK-Sobolev operator $P_s$ acts on the sphere $\mathbb{S}^{d-1}$ with the commutation property:

\textbf{Spherical Harmonics Transform}: Let $\mathcal{F}$ denote the mapping from $L^2(\mathbb{S}^d)$ to the spectral domain $\bigoplus_{\ell=0}^{\infty} \mathbb{C}^{N(d,\ell)}$, where $N(d,\ell)$ is the dimension of the $\ell$-th spherical harmonic space.

The Sobolev operator $P_s$ is defined through its action in Fourier space:
\[ P_s = \sum_{\ell=0}^{\ell_{\max}} \sum_{p=1}^{N(d,\ell)} (1+\ell)^{2s}P_{\ell,p} \]

where $P_{\ell,p} = a_{\ell,p}a_{\ell,p}^T$ with spectral coefficients $(a_{\ell,p})_i = c_iY_{\ell,p}(x_i)$.
\section{Main Proofs}

\subsection{Proof 1: Sobolev Loss as Fractional Laplacian}

\begin{theorem}[Fractional Laplacian Representation of Sobolev Loss]
For a function $f \in H^s(\mathbb{S}^{d-1})$ with $s > 0$, the Sobolev loss can be written as:
\[ \mathcal{L}_s[f] = \int_{\mathbb{S}^{d-1}} f(x) (I + (-\Delta)^{1/2})^s f(x) d\sigma(x) \]
where $(I + (-\Delta)^{1/2})^s$ is the fractional Laplacian operator of order $s$.
\end{theorem>

\begin{proof}
Consider the spherical harmonic expansion $f(x) = \sum_{\ell=0}^{\infty} \sum_{p=1}^{N(d,\ell)} \hat{f}_{\ell,p} Y_{\ell,p}(x)$. The spherical Laplacian acts on these harmonics as eigenvalue operators: $(-\Delta)^{1/2}_{\mathbb{S}^{d-1}} Y_{\ell,p}(x) = \sqrt{\ell(\ell + d - 2)} Y_{\ell,p}(x)$.

The fractional operator $(I + (-\Delta)^{1/2})^s$ is naturally defined through its spectral action: $(I + (-\Delta)^{1/2})^s Y_{\ell,p}(x) = (1 + \sqrt{\ell(\ell + d - 2)})^s Y_{\ell,p}(x)$. For the unit sphere where $d = 2$, this simplifies to $(1 + \ell)^s Y_{\ell,p}(x)$.

Computing the bilinear form using orthonormality of spherical harmonics:
\begin{align}
\int_{\mathbb{S}^{d-1}} f(x) (I + (-\Delta)^{1/2})^s f(x) d\sigma(x) &= \sum_{\ell=0}^{\infty} \sum_{p=1}^{N(d,\ell)} (1 + \sqrt{\ell(\ell + d - 2)})^s |\hat{f}_{\ell,p}|^2
\end{align}

The standard Sobolev norm is defined as $\|f\|_{H^s}^2 = \sum_{\ell,p} (1 + \ell)^{2s} |\hat{f}_{\ell,p}|^2$. The connection becomes clear when we observe that for large $\ell$, the expressions $(1 + \ell)^{2s}$ and $(1 + \sqrt{\ell(\ell + d - 2)})^s$ are asymptotically equivalent up to normalization constants. The precise relationship depends on the specific Sobolev space convention, but the essential spectral structure remains identical.
\end{proof}

\subsection{Alternative Proof: Sobolev Loss on Unbounded Domains via Integration by Parts}

\begin{theorem}[Sobolev Loss on Unbounded Domains]
For functions $f \in H^s(\mathbb{R}^d)$ with $s \geq 1$ and sufficient decay at infinity, the Sobolev loss can be expressed via integration by parts as:
\[ \mathcal{L}_s[f] = \int_{\mathbb{R}^d} f(x)^2 dx + \int_{\mathbb{R}^d} \|\nabla f(x)\|^2 dx + \text{higher order terms} \]
\end{theorem}

\begin{proof}
\textbf{Starting Point}
For $s = 1$, the $H^1$ Sobolev norm is:
\[ \|f\|_{H^1}^2 = \|f\|_{L^2}^2 + \|\nabla f\|_{L^2}^2 = \int_{\mathbb{R}^d} f(x)^2 dx + \int_{\mathbb{R}^d} \|\nabla f(x)\|^2 dx \]

\textbf{Integration by Parts (Stokes' Theorem)}
The key insight is that we can relate the gradient term to the Laplacian using integration by parts. For functions with sufficient decay at infinity (so boundary terms vanish):
\begin{align}
\int_{\mathbb{R}^d} \|\nabla f(x)\|^2 dx &= \int_{\mathbb{R}^d} \nabla f(x) \cdot \nabla f(x) dx \\
&= -\int_{\mathbb{R}^d} f(x) \Delta f(x) dx \quad \text{(by integration by parts)} \\
&= \int_{\mathbb{R}^d} f(x) (-\Delta) f(x) dx
\end{align}

Therefore:
\[ \|f\|_{H^1}^2 = \int_{\mathbb{R}^d} f(x) (I + (-\Delta)^{1/2})^2 f(x) dx \]

\textbf{Extension to Higher Orders}
For general $s \geq 1$, repeated integration by parts yields:
\[ \|f\|_{H^s}^2 = \int_{\mathbb{R}^d} f(x) (I + (-\Delta)^{1/2})^{2s} f(x) dx \]

\textbf{Fractional Case}
For fractional $s$, this extends to:
\[ \mathcal{L}_s[f] = \int_{\mathbb{R}^d} f(x) (I + (-\Delta)^{1/2})^s f(x) dx \]

The integration by parts approach via Stokes' theorem transforms the gradient squared terms into Laplacian terms, providing the connection between differential and spectral formulations.
\end{proof}

\subsection{Case of Limited Regularity: Fourier-Based Proof}

\begin{remark}[Non-Differentiable Functions]
When functions are not twice differentiable, the integration by parts approach fails due to insufficient regularity. In such cases, the \textbf{Fourier-based proof becomes the master approach}.

For $f \in L^2(\mathbb{R}^d)$ with Fourier transform $\hat{f}(\xi)$, the fractional Sobolev norm is defined directly in frequency space:
\[ \|f\|_{H^s}^2 = \int_{\mathbb{R}^d} (1 + |\xi|^2)^s |\hat{f}(\xi)|^2 d\xi \]

This definition:
\begin{itemize}
\item Requires no differentiability assumptions on $f$
\item Extends naturally to negative Sobolev indices $s < 0$
\item Provides the most general framework for Sobolev spaces
\item Reduces to classical definitions when sufficient regularity exists
\end{itemize}

The operator $(I + (-\Delta)^{1/2})^s$ acts in Fourier space as multiplication by $(1 + |\xi|)^s$, making this the fundamental definition from which all other formulations are derived.
\end{remark}

\subsection{Proof 2: NTK Operator Multiplication by Sobolev Operator}

\begin{theorem}[NTK-Sobolev Composition]
Under Sobolev training, the learning operator is given by the composition:
\[ \mathcal{T}_s = K^{\infty} \circ (I + (-\Delta)^{1/2})^s \]
where $K^{\infty}$ is the NTK operator and $(I + (-\Delta)^{1/2})^s$ is the fractional Laplacian.
\end{theorem>

\begin{proof}
Standard gradient descent on the $L^2$ loss $\mathcal{L}(\theta) = \frac{1}{2}\|f(\cdot; \theta) - y\|^2_{L^2}$ gives parameter dynamics $\frac{d\theta}{dt} = -\nabla_\theta \mathcal{L}(\theta)$. In the NTK regime, this translates to function dynamics $\frac{df}{dt} = -K^{\infty}(f - y)$.

When we replace the $L^2$ loss with the Sobolev loss $\mathcal{L}_s(\theta) = \frac{1}{2}\|f(\cdot; \theta) - y\|^2_{H^s}$, the gradient computation changes fundamentally. Using our fractional Laplacian representation from Theorem 1, the Sobolev loss becomes:
\[ \mathcal{L}_s(\theta) = \frac{1}{2}\int (f(x; \theta) - y(x)) (I + (-\Delta)^{1/2})^s (f(x; \theta) - y(x)) d\sigma(x) \]

The gradient with respect to parameters now involves the chain rule through the fractional operator:
\[ \nabla_\theta \mathcal{L}_s(\theta) = \int \nabla_\theta f(x; \theta) \cdot (I + (-\Delta)^{1/2})^s (f(x; \theta) - y(x)) d\sigma(x) \]

Since the NTK is defined as $K^{\infty}(x, x') = \langle \nabla_\theta f(x; \theta), \nabla_\theta f(x'; \theta) \rangle$, the function dynamics become:
\[ \frac{df}{dt} = -\int K^{\infty}(x, x') (I + (-\Delta)^{1/2})^s (f(x') - y(x')) d\sigma(x') = -K^{\infty} \circ (I + (-\Delta)^{1/2})^s (f - y) \]

Therefore, the learning operator is indeed $\mathcal{T}_s = K^{\infty} \circ (I + (-\Delta)^{1/2})^s$. The rotational invariance of both operators ensures they share the same spherical harmonic eigenfunctions, making this composition well-defined and commutative.
\end{proof}

\subsection{Proof 3: Spectral Properties of the Composite Operator}

\begin{theorem}[Spectrum of NTK-Sobolev Operator]
The eigenvalues of the composite operator $\mathcal{T}_s = K^{\infty} \circ (I + (-\Delta)^{1/2})^s$ are given by the product of individual eigenvalues: $\mu_\ell^{(\mathcal{T}_s)} = \mu_\ell^{(K)} \cdot (1 + \sqrt{\ell(\ell + d - 2)})^s$.
\end{theorem>

\begin{proof}
\textit{[To be completed]}
\end{proof}

\subsection{Proof 4: Commutation of Discrete Matrix Operators}

\begin{theorem}[Matrix Commutation]
The discrete matrices $K$ and $P_s$ commute: $KP_s = P_sK$.
\end{theorem}

\begin{proof}
We establish commutation by expanding both matrices in terms of spherical harmonic projectors using the sampling measure (sum of Dirac masses).

\textbf{NTK Matrix Expansion}
The NTK matrix can be written using its spectral decomposition with respect to spherical harmonics:
\[ K_{ij} = K^{\infty}(x_i, x_j) = \sum_{\ell=0}^{\infty} \sum_{p=1}^{N(d,\ell)} \mu_\ell^{(K)} (a_{\ell,p})_i (a_{\ell,p})_j \]
where $(a_{\ell,p})_i = c_i Y_{\ell,p}(x_i)$ with quadrature weights $c_i$ and $\mu_\ell^{(K)}$ are the NTK operator eigenvalues.

This gives us the matrix form:
\[ K = \sum_{\ell=0}^{\infty} \sum_{p=1}^{N(d,\ell)} \mu_\ell^{(K)} a_{\ell,p} a_{\ell,p}^T \]

\textbf{Sobolev Matrix Expansion}
Similarly, the Sobolev matrix has the expansion:
\[ P_s = \sum_{\ell=0}^{\infty} \sum_{p=1}^{N(d,\ell)} (1 + \sqrt{\ell(\ell + d - 2)})^s a_{\ell,p} a_{\ell,p}^T \]

\textbf{Matrix Product Computation}
Computing the product $KP_s$:
\begin{align}
KP_s &= \left(\sum_{\ell,p} \mu_\ell^{(K)} a_{\ell,p} a_{\ell,p}^T\right) \left(\sum_{\ell',p'} (1 + \sqrt{\ell'(\ell' + d - 2)})^s a_{\ell',p'} a_{\ell',p'}^T\right) \\
&= \sum_{\ell,p} \sum_{\ell',p'} \mu_\ell^{(K)} (1 + \sqrt{\ell'(\ell' + d - 2)})^s a_{\ell,p} (a_{\ell,p}^T a_{\ell',p'}) a_{\ell',p'}^T
\end{align}

\textbf{Orthogonality and Commutation}
The key observation is that $a_{\ell,p}^T a_{\ell',p'} = \delta_{\ell,\ell'} \delta_{p,p'} \|a_{\ell,p}\|^2$ due to the orthogonality of spherical harmonics under the sampling measure. Therefore:
\begin{align}
KP_s &= \sum_{\ell,p} \mu_\ell^{(K)} (1 + \sqrt{\ell(\ell + d - 2)})^s a_{\ell,p} a_{\ell,p}^T \\
P_sK &= \sum_{\ell,p} (1 + \sqrt{\ell(\ell + d - 2)})^s \mu_\ell^{(K)} a_{\ell,p} a_{\ell,p}^T
\end{align}

Since multiplication of scalars commutes, we have $KP_s = P_sK$.

\textbf{Underlying Reason}: This commutation property reflects the fact that both $K^{\infty}$ and $P_s$ operators commute in the continuous setting because they share spherical harmonics as eigenfunctions due to rotational invariance.
\end{proof}

\subsection{Proof 5: Eigenvalue Scaling Laws}

\begin{theorem}[Asymptotic Scaling Laws]
For the NTK-Sobolev operator, eigenvalues follow the scaling laws $\lambda_\ell \sim \ell^{-d} \cdot (1 + \sqrt{\ell(\ell + d - 2)})^s$.
\end{theorem}

\begin{proof}
\textbf{Individual Operator Eigenvalues}
From previous analysis:
\begin{itemize}
\item NTK operator eigenvalues: $\mu_\ell^{(K)} \sim C(d, L) \ell^{-d}$ for large $\ell$
\item Sobolev operator eigenvalues: $\mu_\ell^{(P_s)} = (1 + \sqrt{\ell(\ell + d - 2)})^s$
\end{itemize}

\textbf{Composite Operator Spectrum}
Since the operators commute and share eigenfunctions, the eigenvalues of the composite operator are products:
\[ \lambda_\ell^{(\mathcal{T}_s)} = \mu_\ell^{(K)} \cdot \mu_\ell^{(P_s)} = C(d, L) \ell^{-d} \cdot (1 + \sqrt{\ell(\ell + d - 2)})^s \]

\textbf{Asymptotic Analysis}
For large $\ell$, we have $\sqrt{\ell(\ell + d - 2)} \sim \ell \sqrt{1 + (d-2)/\ell} \sim \ell$ for $d$ fixed. Therefore:
\[ \lambda_\ell^{(\mathcal{T}_s)} \sim C(d, L) \ell^{-d} \cdot (1 + \ell)^s = C(d, L) \ell^{s-d} \]

\textbf{Critical Scaling Behavior}
The spectral behavior depends on the relationship between $s$ and $d$:
\begin{itemize}
\item $s < d$: Eigenvalues decay $(\lambda_\ell \to 0)$ - regularizing effect
\item $s = d$: Logarithmic corrections appear - critical regime  
\item $s > d$: Eigenvalues grow $(\lambda_\ell \to \infty)$ - amplifying high frequencies
\end{itemize}

This scaling law $\lambda_\ell \sim \ell^{s-d}$ determines the spectral properties and learning dynamics of Sobolev-trained networks, with the critical exponent $s - d$ controlling frequency bias.
\end{proof}

\subsection{Synthesis: Unified Framework for NTK-Sobolev Analysis}

\textbf{Summary}: Rotational invariance of both NTK and Sobolev operators ensures matrix commutation $KP_s = P_sK$, with composite eigenvalues $\lambda_\ell \sim \ell^{s-d}$ determining learning dynamics.

\begin{remark}[Practical Implementation Warning]
\textbf{Dataset Context}: In practical machine learning settings, we only have access to function values $f(x_i)$ at sampled points, \emph{not} the gradients $\nabla f(x_i)$. This makes the Fourier-based approach the most relevant for theoretical analysis and implementation.

\textbf{Gradient Availability}: If gradients were available (e.g., through physics-informed neural networks or when the true function derivatives are known), the theoretical results presented here remain valid. However, the practical loss implementation in PyTorch should then utilize:
\begin{itemize}
\item Automatic differentiation (autograd) to compute gradients
\item Direct gradient-based Sobolev norms: $\|f\|_{H^1}^2 = \|f\|_{L^2}^2 + \|\nabla f\|_{L^2}^2$
\item Higher-order derivative computations via successive autograd calls
\end{itemize}

The spectral analysis framework developed here provides the theoretical foundation regardless of the implementation approach.
\end{remark}

\subsection{From Sobolev Loss to Discrete Matrix Operator}

\subsubsection{Two Integral Formulations}

\textbf{Formulation 1: Uniform Lebesgue Measure on the Sphere}
\[ \mathcal{L}_s[f] = \int_{\mathbb{S}^{d-1}} f(x) (I + (-\Delta)^{1/2})^s f(x) d\sigma(x) \]
where $d\sigma(x)$ is the uniform surface measure on $\mathbb{S}^{d-1}$ with $\int_{\mathbb{S}^{d-1}} d\sigma(x) = \text{Area}(\mathbb{S}^{d-1})$.

\textbf{Formulation 2: Sampling Measure from Dataset}
\[ \mathcal{L}_s[f] = \int_{\mathbb{S}^{d-1}} f(x) (I + (-\Delta)^{1/2})^s f(x) d\mu_n(x) \]
where $\mu_n = \frac{1}{n}\sum_{i=1}^n \delta_{x_i}$ is the empirical measure from our sampled dataset $\{x_i\}_{i=1}^n$.

\textbf{Discrete Implementation}:
Under the sampling measure $\mu_n$, the integral becomes:
\[ \mathcal{L}_s[f] = \frac{1}{n}\sum_{i=1}^n f(x_i) \left[(I + (-\Delta)^{1/2})^s f\right](x_i) \]

This leads to the matrix form $\mathcal{L}_s[f] \approx \mathbf{f}^T P_s \mathbf{f}$ where $\mathbf{f} = (f(x_1), \ldots, f(x_n))^T$.

\textbf{Relationship Between Formulations}:
The uniform measure provides the theoretical foundation, while the sampling measure $\mu_n$ represents the practical implementation. As $n \to \infty$ with appropriate sampling, $\mu_n \to d\sigma$ in the weak sense, ensuring convergence of the discrete formulation to the continuous theory.

\subsubsection{Practical Implementation Considerations}

\begin{enumerate}
\item \textbf{Data-Only Setting}: Since we only have function values $\{f(x_i)\}_{i=1}^n$, the matrix $P_s$ must be constructed via:
\[ (P_s)_{ij} = \sum_{\ell=0}^{\ell_{\max}} \sum_{p=1}^{N(d,\ell)} (1 + \sqrt{\ell(\ell + d - 2)})^s Y_{\ell,p}(x_i) Y_{\ell,p}(x_j) \]

\item \textbf{Computational Complexity}: The matrix construction requires:
\begin{itemize}
\item Evaluation of spherical harmonics $Y_{\ell,p}(x_i)$ for all $(\ell,p)$ and data points
\item Summation over $\mathcal{O}(\ell_{\max}^{d-1})$ harmonics (due to dimension growth $N(d,\ell) \sim \ell^{d-2}$)
\item Total complexity: $\mathcal{O}(n^2 \ell_{\max}^{d-1})$ for matrix construction
\end{itemize}

\item \textbf{Truncation Strategy}: In practice, $\ell_{\max}$ must be chosen to balance:
\begin{itemize}
\item Spectral accuracy (larger $\ell_{\max}$ captures more frequencies)
\item Computational feasibility (smaller $\ell_{\max}$ reduces cost)
\item Statistical stability (avoid overfitting to high-frequency noise)
\end{itemize}

\item \textbf{Alternative with Gradients}: If gradients $\{\nabla f(x_i)\}$ were available, direct computation becomes possible:
\[ \mathcal{L}_s[f] \approx \frac{1}{n}\sum_{i=1}^n \left[ f(x_i)^2 + s \|\nabla f(x_i)\|^2 + \text{higher order terms} \right] \]
This approach scales as $\mathcal{O}(nd)$ instead of $\mathcal{O}(n^2\ell_{\max}^{d-1})$.
\end{enumerate}

\textbf{Fourier decomposition}: For $f(x) = \sum_{\ell,p} \hat{f}_{\ell,p} Y_{\ell,p}(x)$:
\[ \mathcal{L}_s[f] = \sum_{\ell=0}^{\infty} \sum_{p=1}^{N(d,\ell)} (1+\ell)^{2s} |\hat{f}_{\ell,p}|^2 \]

\textbf{Discretization}: At points $\{x_i\}_{i=1}^n$:
\[ \hat{f}_{\ell,p} \approx \sum_{i=1}^n c_i f(x_i) Y_{\ell,p}(x_i) \]

\textbf{Final matrix form}: $\mathcal{L}_s[f] \approx f^T P_s f$ where $f = (f(x_1), \ldots, f(x_n))^T$.

\subsection{Spectral Properties and Dimension Growth}

The dimension of spherical harmonic spaces grows as:
\[ N(d,\ell) \sim \frac{2\ell^{d-2}}{(d-2)!} \quad \text{as } \ell \to \infty \]

This exponential growth in dimension determines the computational complexity of the Sobolev operator discretization.

\subsection{Common Eigenfunctions Property}

\begin{theorem}[Spherical Harmonics as Common Eigenfunctions]
For rotationally invariant kernels on $\mathbb{S}^{d-1}$:
\begin{itemize}
\item NTK operator $K^{\infty}$ and Sobolev operator $P_s$ share spherical harmonics $Y_{\ell,p}$ as eigenfunctions
\item This is due to rotational invariance and Schur's lemma for irreducible representations
\item Enables direct spectral modification analysis
\end{itemize}
\end{theorem}

\begin{theorem}[Commutation Property]
The NTK operator $K^{\infty}$ and Sobolev operator $P_s$ commute:
\[ [K^{\infty}, P_s] = 0 \]
This holds for any sampling distribution $\rho(x)$ on $\mathbb{S}^{d-1}$ and implies that spectral decompositions are compatible.
\end{theorem}

\newpage

\section{Deep NTK Analysis and Theoretical Results}

\subsection{NTK at Edge of Chaos}

For MLPs with $(a,b)$-ReLU activation $\phi(s) = as + b|s|$, the Edge of Chaos initialization requires:
\begin{itemize}
\item Initialization: $\sigma^2 = (a^2+b^2)^{-1}$
\item Key parameter: $\Delta_\phi = \frac{b^2}{a^2+b^2}$ (for standard ReLU: $\Delta_\phi = 0.5$)
\item Cosine map: $\varrho(\rho) = \rho + \Delta_\phi \frac{2}{\pi}\left( \sqrt{1-\rho^2} - \rho \arccos(\rho) \right)$
\end{itemize}

The Edge of Chaos is the unique initialization that remains invariant to network depth, preventing activation explosion or vanishing.

\section{Reconciling NTK matrix spectrum and Sobolev Training}

\subsection{A Unified Spectral Viewpoint: Commutation and Eigenvalue Products}

The reconciliation between the geometric view of the NTK (approximated by the inverse cosine distance matrix) and the functional view of Sobolev training lies in their shared algebraic structure.

\begin{itemize}
    \item \textbf{Shared Invariance}: Both the NTK matrix $K$ and the discrete Sobolev operator matrix $P_s$ are rotationally invariant. This means their entries $(K)_{ij}$ and $(P_s)_{ij}$ depend only on the inner product $\langle x_i, x_j \rangle$.
    
    \item \textbf{Matrix Commutation}: As a consequence, both matrices can be expressed as functions of the Gram matrix $G$ (where $G_{ij} = \langle x_i, x_j \rangle$). For two such matrices, $K=f(G)$ and $P_s=g(G)$, they commute:
    \[ K P_s = f(G)g(G) = g(G)f(G) = P_s K \]
    This is the discrete analogue of the commutation of their underlying continuous operators.
    
    \item \textbf{Simultaneous Diagonalization}: Since they commute, $K$ and $P_s$ are simultaneously diagonalizable. They share the same set of eigenvectors, which are the eigenvectors of the Gram matrix $G$.
    
    \item \textbf{Product of Eigenvalues}: The spectrum of the Sobolev-modified NTK operator, $K P_s$, which governs the learning dynamics, is directly given by the product of the individual spectra:
    \[ \lambda_i(K P_s) = \lambda_i(K) \cdot \lambda_i(P_s) \]
    (after aligning the eigenvector bases).
\end{itemize}

\textbf{Key Insight}: The challenge of analyzing the complex operator $K P_s$ is reduced to separately analyzing the spectra of $K$ and $P_s$. We can leverage the geometric approximation for $K$ and combine it with a spectral analysis of $P_s$.

\subsection{A Path Forward: Geometric Approximation and Experimental Validation}

Based on the unified spectral viewpoint, a clear research path emerges to develop a predictive model for Sobolev training dynamics.

\textbf{Goal}: Develop a semi-analytic model for the spectrum of the Sobolev-modified NTK matrix $K P_s$.

\textbf{Proposed Strategy}:

\begin{enumerate}
    \item \textbf{Approximate the NTK Spectrum}:
    Use the established near-affine relationship between the NTK and the inverse cosine distance matrix $W_l$:
    \[ K \approx A W_l + B \]
    The spectrum of the geometric matrix $W_l$ can be analyzed to provide an approximation for the eigenvalues $\lambda_i(K)$.

    \item \textbf{Analyze the Sobolev Operator Spectrum}:
    The matrix $P_s$ is a kernel matrix with explicit eigenvalues $(1 + \sqrt{\ell(\ell + d - 2)})^s$ for each harmonic degree $\ell$.

    \item \textbf{Combine Spectra and Validate}:
    The core prediction is that the eigenvalues of the learning operator are given by the product of the component eigenvalues:
    \[ \lambda_i(K P_s) \approx \lambda_i(A W_l + B) \cdot \lambda_i(P_s) \]
    
    \textbf{Experimental Validation Plan}:
    \begin{itemize}
        \item For a dataset on $\mathbb{S}^{d-1}$, numerically compute the matrices $K$, $P_s$, and the product $K P_s$.
        \item Compare the analytically predicted spectrum with the true, numerically computed spectrum of $K P_s$.
        \item Investigate the quality of this approximation as a function of the Sobolev parameter $s$, dimension $d$, data size $n$, and network depth $l$.
    \end{itemize}
\end{enumerate}



\subsection{Extended Analysis: Zonal Kernel Representation of P_s}

The matrix $P_s$ admits a remarkable representation as a zonal kernel, which provides deeper insight into its spectral properties.

\begin{theorem}[Zonal Kernel Representation of P_s]
The Sobolev matrix $P_s$ can be written as a zonal kernel:
\[ (P_s)_{ij} = p_s(\langle x_i, x_j \rangle) \]
where the kernel function $p_s(t)$ has the explicit form:
\[ p_s(t) = \sum_{\ell=0}^{\ell_{\max}} (1 + \sqrt{\ell(\ell + d - 2)})^s \frac{N(d,\ell)}{\text{Area}(\mathbb{S}^{d-1})} P_\ell^{((d-2)/2)}(t) \]
with $P_\ell^{(\alpha)}(t)$ being Gegenbauer polynomials and $N(d,\ell) = \binom{\ell+d-1}{d-1} - \binom{\ell+d-3}{d-1}$.
\end{theorem}

\begin{proof}
Starting from the spherical harmonic expansion:
\[ (P_s)_{ij} = \sum_{\ell=0}^{\ell_{\max}} \sum_{p=1}^{N(d,\ell)} (1 + \sqrt{\ell(\ell + d - 2)})^s Y_{\ell,p}(x_i) Y_{\ell,p}(x_j) \]

Applying the addition theorem for spherical harmonics:
\[ \sum_{p=1}^{N(d,\ell)} Y_{\ell,p}(x) Y_{\ell,p}(y) = \frac{N(d,\ell)}{\text{Area}(\mathbb{S}^{d-1})} P_\ell^{((d-2)/2)}(\langle x, y \rangle) \]

Substituting this identity yields the desired zonal kernel representation.
\end{proof}

\subsection{Spectral Analysis via Gegenbauer Polynomial Properties}

The zonal kernel representation enables direct spectral analysis using properties of Gegenbauer polynomials.

\begin{proposition}[Asymptotic Behavior of p_s(t)]
For large $\ell$, the kernel $p_s(t)$ exhibits the following asymptotic behavior:

\textbf{Case 1: $s < d/2$}
\[ p_s(t) \sim C_s \sum_{\ell=0}^{\infty} \ell^{s-1} P_\ell^{((d-2)/2)}(t) \]
The series converges for $t \in (-1, 1)$ and exhibits algebraic decay.

\textbf{Case 2: $s = d/2$} 
\[ p_s(t) \sim C_s \sum_{\ell=0}^{\infty} \ell^{-1} \log(\ell) P_\ell^{((d-2)/2)}(t) \]
Logarithmic corrections appear at the critical dimension.

\textbf{Case 3: $s > d/2$}
\[ p_s(t) \sim C_s \sum_{\ell=0}^{\infty} \ell^{s-1} P_\ell^{((d-2)/2)}(t) \]
The series may diverge, requiring careful regularization.
\end{proposition}

\subsection{Explicit Forms for Low Dimensions}

\textbf{Dimension d = 2 (Circle $\mathbb{S}^1$)}:
Gegenbauer polynomials reduce to Chebyshev polynomials: $P_\ell^{(0)}(t) = T_\ell(t)$
\[ p_s(t) = \frac{1}{2\pi} \sum_{\ell=0}^{\infty} (1 + \ell)^s T_\ell(t) \]

For $s = 1$: $p_1(t) = \frac{1}{2\pi} \sum_{\ell=0}^{\infty} (1 + \ell) T_\ell(t) = \frac{1}{2\pi} \frac{d}{dt}\left[\frac{1}{1-t}\right] = \frac{1}{2\pi(1-t)^2}$

\textbf{Dimension d = 3 (Sphere $\mathbb{S}^2$)}:
Gegenbauer polynomials become Legendre polynomials: $P_\ell^{(1/2)}(t) = P_\ell(t)$
\[ p_s(t) = \frac{1}{4\pi} \sum_{\ell=0}^{\infty} (1 + \ell)^s (2\ell + 1) P_\ell(t) \]

\subsection{Matrix Conditioning and Eigenvalue Bounds}

The zonal kernel representation provides explicit bounds on the eigenvalues of $P_s$.

\textbf{Eigenvalue Bounds for P_s}: Since $P_s$ has known spectral structure, its eigenvalues satisfy:
\[ 1 \leq \lambda_i(P_s) \leq (1 + \ell_{\max})^s \]

The condition number is simply $\kappa(P_s) = (1 + \ell_{\max})^s$, growing exponentially in both $s$ and $\ell_{\max}$.

\textbf{Practical Implications}:
\begin{itemize}
\item The zonal kernel $p_s(t)$ can be precomputed and stored for efficient matrix construction
\item Eigenvalue bounds guide the choice of regularization parameters
\item The explicit Gegenbauer expansion enables fast multipole-type acceleration methods
\item Condition number analysis informs numerical stability considerations
\end{itemize}



\section{Deep Narrow Neural Networks}

\subsection{Scaled NTK at Initialization}

\begin{theorem}[Theorem 1: Scaled NTK at Initialization]
For $f^L_\theta$ initialized appropriately, as $L \to \infty$:
\[ \tilde{\Theta}^L_0(x, x') \xrightarrow{p} \tilde{\Theta}^\infty(x, x') \]
where
\[ \tilde{\Theta}^\infty(x, x') = (x^T x' + 1 + \E_g[\sigma(g(x))\sigma(g(x'))]) I_{d_{out}} \]
with $g \sim \text{GP}(0, \rho^2 d_{in}^{-1} x^T x' + \beta^2)$ (Gaussian random field).
\end{theorem}

For comparison, the infinite–width two–layer ReLU NTK on the sphere is
\[
  K^{(2)}(x,y)=\|x\|\,\|y\|\,\Bigl(\tfrac{\pi-\theta}{\pi}\,\cos \theta + \tfrac{\sin\theta}{\pi}\Bigr),\qquad \theta=\arccos(\langle x,y\rangle).
\]
Equivalently, in dot-product form
\[
  k^{(2)}(x,y)=\langle x,y\rangle\,\arccos\bigl(-\langle x,y\rangle\bigr)+\sqrt{1-\langle x,y\rangle^{2}}.
\]

\textbf{Alternative formulation}: $\kappa_1(\cos(u) \cdot v)$ where:
\begin{itemize}
\item $v = \frac{1}{1 + \beta^2/\alpha^2}$, where $\beta$ is the bias variance
\item $\alpha = \frac{\|x\| \|x'\| \rho}{d_{in}}$, where $\cos(u)$ is the cosine distance between $x, x'$
\end{itemize}

This framework provides a promising direction for analyzing deep narrow networks through limited expansion analysis.

\subsection{Research Perspectives for Deep Narrow Networks}

\textbf{Architectural modifications}:
   \begin{itemize}
\item \textbf{Unit concatenation}: Combine multiple narrow networks to increase expressivity
\item \textbf{Skip connections}: Investigate ResNet-style connections:
  \[ \tilde{\Theta}^\infty_{\text{skip}}(x, x') = \tilde{\Theta}^\infty(x, x') + \text{skip terms} \]
   \end{itemize}

\textbf{Initialization studies}:
\begin{itemize}
\item \textbf{Alternative initializations}: Explore different schemes beyond current setup
\item \textbf{$\beta \to 0$ limit}: Analyze behavior when bias variance vanishes:
  \[ v = \frac{1}{1 + \beta^2/\alpha^2} \to 1 \text{ as } \beta \to 0 \]
\item \textbf{Complex kernel structures}: Find initializations that yield more intricate kernel forms
\end{itemize}

\textbf{Theoretical extensions}:
\begin{itemize}
\item Extension of Hayou \& Yang's work on ResNets showing "wide and deep limits commute"
\item Mean field analysis for deep narrow frameworks
\item Careful analysis of stochasticity in initialization (not dropout)
\item Experimental validation on practical tasks
\end{itemize}

\section{Conclusions and Future Directions}

\subsection{Summary of Key Results}

This document presents a comprehensive analysis of Neural Tangent Kernel spectral properties and their modification through Sobolev training. The main contributions include:

\begin{enumerate}
\item \textbf{Eigenvalue scaling laws}: Established relationships $\mu_\ell \sim \ell^{-\alpha}$ for NTK operator eigenvalues and their impact on learning dynamics

\item \textbf{Matrix vs. Operator distinction}: Clarified the fundamental difference between discrete NTK matrix eigenvalues and continuous operator eigenvalues, showing how sampling affects spectral properties

\item \textbf{Sobolev training framework}: Demonstrated how the operator $P_s$ modifies the NTK spectrum via the spectral exponent $2s-d$, enabling control over learned frequency components

\item \textbf{Deep network analysis}: Provided scaling laws for condition numbers $\kappa(K^{\infty}) \sim 1 + \frac{n}{3} + \mathcal{O}(n \xi / l)$ and eigenvalue distributions with respect to depth $l$ and data size $n$

\item \textbf{Spherical harmonic framework}: Established the commutation property $[K^{\infty}, P_s] = 0$ and common eigenfunctions for rotationally invariant kernels
\end{enumerate}

\subsection{Research Challenges and Open Questions}

\begin{enumerate}
\item \textbf{Unified framework}: Different papers assume varying domains, initializations, architectures, and activations. A unified theoretical framework remains to be developed.

\item \textbf{Beyond spherical domains}: Extension of harmonic analysis from spheres to general spaces $L^2(\gamma)$ represents a significant theoretical challenge.

\item \textbf{Optimal width scaling}: Achieving linear scaling in $\lambda_{\min}$ with minimal width requirements.

\item \textbf{Broad activation coverage}: Extending from ReLU to general inhomogeneous activations using harmonic analysis.

\item \textbf{Experimental validation}: Systematic experimental verification of theoretical predictions across different architectures and tasks.
\end{enumerate}

\subsection{Research Roadmap}

\textbf{Near-term objectives}:
\begin{itemize}
\item Study narrow NTK behavior to identify simplifications before developing general approaches
\item Incorporate Sobolev framework into spherical harmonic analysis with experimental validation
\item Unify initialization schemes and architectural assumptions across different theoretical frameworks
\end{itemize}

\textbf{Long-term goals}:
\begin{itemize}
\item Extend harmonic analysis from spherical to general domains with experimental validation
\item Develop complete theory for deep narrow networks with practical applications
\item Create unified spectral theory encompassing all major NTK variants and training modifications
\end{itemize}

The convergence of spectral analysis, harmonic analysis, and neural network theory opens promising avenues for understanding and controlling the learning dynamics of deep networks through their spectral properties.

\paragraph{Sobolev perspective.}  Because the learning operator under Sobolev training is $\mathcal T_s = KP_s$, a practical strategy is to tune the exponent $s$ as a function of the data size~$n$.  When $n$ grows the condition number of $K$ deteriorates ($\kappa(K)\sim n$), yet multiplying by $P_s$ amplifies high–frequency modes and can flatten the composite spectrum.  Making $s$ larger for large $n$ therefore counterbalances the conditioning of $K$.  A precise estimate of $\kappa(P_s)$ is still missing and remains an open problem for future work.

\newpage

\section{Deep NTK Analysis}

\subsection{Deep NTK Properties}

The limiting NTK at EOC for $L$-layer networks is given by:
\begin{align}
K^{\infty}(\mathbf{x}_1, \mathbf{x}_2) &= \|\mathbf{x}_1\| \|\mathbf{x}_2\| \left( \sum_{k=1}^l \varrho^{\circ (k-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right) \right. \\
&\quad \left. \times \prod_{k'=k}^{l-1} \varrho'\left(\varrho^{\circ (k'-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right)\right) \right) \mathbf{I}_{m_l}
\end{align}

\textbf{Eigenvalue Decay}: For $L$-layer ReLU networks with $L \geq 3$:
\[ \mu_k \sim C(d, L)k^{-d} \]
where $C(d, L)$ depends on the parity of $k$ and grows quadratically with $L$.

For the normalized NTK $\kappa^L_{\text{NTK}}/L$, the constant $C(d, L)$ grows linearly with $L$.

\subsection{Inverse Cosine Distance Matrix Analysis}

\textbf{Inverse cosine distance matrix} $W_k$ for depth $k$:
\[ {W_k}_{i,i} = 0, \quad {W_k}_{i_1,i_2} = \left( \frac{1 - \rho_k(x_{i_1},x_{i_2})}{2} \right)^{-\frac{1}{2}} \text{ for } i_1 \neq i_2 \]

\textbf{Near-affine behavior}:
\begin{itemize}
\item NTK matrix $K^{\infty} \approx A \cdot W_l + B$ (affine dependence)
\item Spectral bounds transfer from $W_k$ to NTK via this affine relationship  
\item Error terms: $O(k^{-1})$ - decreases with depth
\end{itemize}

This relationship enables indirect analysis of NTK spectral properties through simpler geometric matrices.

\section{Deep Narrow Neural Networks}

\subsection{Scaled NTK at Initialization}

\begin{theorem}[Theorem 1: Scaled NTK at Initialization]
For $f^L_\theta$ initialized appropriately, as $L \to \infty$:
\[ \tilde{\Theta}^L_0(x, x') \xrightarrow{p} \tilde{\Theta}^\infty(x, x') \]
where
\[ \tilde{\Theta}^\infty(x, x') = (x^T x' + 1 + \E_g[\sigma(g(x))\sigma(g(x'))]) I_{d_{out}} \]
with $g \sim \text{GP}(0, \rho^2 d_{in}^{-1} x^T x' + \beta^2)$ (Gaussian random field).
\end{theorem}

For comparison, the infinite–width two–layer ReLU NTK on the sphere is
\[
  K^{(2)}(x,y)=\|x\|\,\|y\|\,\Bigl(\tfrac{\pi-\theta}{\pi}\,\cos \theta + \tfrac{\sin\theta}{\pi}\Bigr),\qquad \theta=\arccos(\langle x,y\rangle).
\]
Equivalently, in dot-product form
\[
  k^{(2)}(x,y)=\langle x,y\rangle\,\arccos\bigl(-\langle x,y\rangle\bigr)+\sqrt{1-\langle x,y\rangle^{2}}.
\]

\textbf{Alternative formulation}: $\kappa_1(\cos(u) \cdot v)$ where:
\begin{itemize}
\item $v = \frac{1}{1 + \beta^2/\alpha^2}$, where $\beta$ is the bias variance
\item $\alpha = \frac{\|x\| \|x'\| \rho}{d_{in}}$, where $\cos(u)$ is the cosine distance between $x, x'$
\end{itemize}

This framework provides a promising direction for analyzing deep narrow networks through limited expansion analysis.

\subsection{Research Perspectives for Deep Narrow Networks}

\textbf{Architectural modifications}:
   \begin{itemize}
\item \textbf{Unit concatenation}: Combine multiple narrow networks to increase expressivity
\item \textbf{Skip connections}: Investigate ResNet-style connections:
  \[ \tilde{\Theta}^\infty_{\text{skip}}(x, x') = \tilde{\Theta}^\infty(x, x') + \text{skip terms} \]
   \end{itemize}

\textbf{Initialization studies}:
\begin{itemize}
\item \textbf{Alternative initializations}: Explore different schemes beyond current setup
\item \textbf{$\beta \to 0$ limit}: Analyze behavior when bias variance vanishes:
  \[ v = \frac{1}{1 + \beta^2/\alpha^2} \to 1 \text{ as } \beta \to 0 \]
\item \textbf{Complex kernel structures}: Find initializations that yield more intricate kernel forms
\end{itemize}

\textbf{Theoretical extensions}:
\begin{itemize}
\item Extension of Hayou \& Yang's work on ResNets showing "wide and deep limits commute"
\item Mean field analysis for deep narrow frameworks
\item Careful analysis of stochasticity in initialization (not dropout)
\item Experimental validation on practical tasks
\end{itemize}

\section{Alternative Domains: Gaussian and Toroidal Extensions}

Beyond spherical domains, two alternative geometries offer distinct computational advantages while preserving the essential NTK-Sobolev spectral structure.

\subsection{Gaussian Domain: $L^2(\mathbb{R}^d, \mu)$}

\textbf{Framework}: Gaussian measure $d\mu = (2\pi)^{-d/2} e^{-\|x\|^2/2} dx$ with Ornstein-Uhlenbeck operator $\mathcal{L}_{OU} = -\Delta + x \cdot \nabla$

\textbf{Eigenfunctions}: Hermite polynomials $H_{\alpha}(x)$ with eigenvalues $|\alpha| = \sum_{i=1}^d \alpha_i$

\textbf{NTK Structure}: Translation-invariant $K^{\infty}(x, y) = k(x - y)$ with Fourier eigenvalues $\mu_\omega = \hat{k}(\omega)$

\textbf{Composite Spectrum}: $\lambda_\alpha \sim \hat{k}(|\alpha|^{1/2}) \cdot (1 + |\alpha|)^s$ provides natural high-frequency regularization

\subsection{Toroidal Domain: $\mathbb{T}^d$}

The torus $\mathbb{T}^d$ is obtained by gluing opposite faces of the unit cube $[0,1]^d$.  Functions on $[0,1]^d$ are thus extended periodically:
\[
  f(x+e_i) \;=\; f(x), \quad e_i \text{ the } i\text{-th unit vector.}
\]

\paragraph{Fourier basis.}  An orthonormal eigenbasis of both translation-invariant kernels and Sobolev operators is given by the Fourier modes
\[
   e_k(x) \;=\; e^{2\pi i k\cdot x}, \qquad k\in\mathbb Z^d.
\]

\paragraph{Sobolev operator.}  On the torus the discrete Sobolev operator is diagonal in this basis:
\[
   P_s^{(\mathbb{T}^d)}\,e_k\;=\;\bigl(1+\|k\|^2\bigr)^{s}\,e_k.
\]

\paragraph{Periodised NTK.}  Any translation-invariant kernel $k$ can be made periodic
\[
   k_{\text{per}}(x,y)=\sum_{n\in\mathbb Z^d} k\bigl(x, y+n\bigr),
\]
whose Fourier eigenvalues are $\widehat{k}_{\text{per}}(k)$.  Hence the NTK matrix is circulant and shares the Fourier eigenbasis.  Its eigenvalues are
\[
   \lambda_k^{(K)}=\widehat{k}_{\text{per}}(k).
\]

\paragraph{Composite spectrum.}  Because $K$ and $P_s$ commute we get
\[
   \lambda_k^{(KP_s)} = \widehat{k}_{\text{per}}(k)\,(1+\|k\|^2)^{s}
   \;\sim\; \|k\|^{2s-\alpha}
\]
when $\widehat{k}_{\text{per}}(k)\sim\|k\|^{-\alpha}$ for large $\|k\|$.

\paragraph{Optimisation bound.}  For a periodic extension $\tilde f$ of a target function $f$ defined on $[0,1]^d$ we have the bound
\[
   \mathcal L_{[0,1]^d}[f]\;\le\; C_d\,\mathcal L_{\mathbb T^d}[\tilde f],
\]
where $C_d$ grows exponentially with $d$ (curse of dimensionality).  In practice this restricts the toroidal analysis to moderate dimensions.

\paragraph{Computational cost.}  Thanks to the FFT we obtain
\[
   \text{matrix–vector product in }\mathcal O\bigl(N^d\log N\bigr), \quad
   \text{storage in }\mathcal O\bigl(N^d\bigr).
\]

This concise description retains every mathematical ingredient while avoiding unnecessary narrative.

\subsection{Domain Selection Criteria}

\begin{itemize}
\item \textbf{Sphere $\mathbb{S}^{d-1}$}: Natural for directional data, rotation invariance
\item \textbf{Gaussian $\mathbb{R}^d$}: Infinite domain with natural regularization  
\item \textbf{Torus $\mathbb{T}^d$}: Finite domain, efficient computation, no boundary effects
\end{itemize}

\newpage

\begin{itemize}
\item \textbf{Translation invariance}: Favors Fourier modes
\item \textbf{Gaussian weighting}: Favors Hermite polynomials
\end{itemize}

\subsubsection{Sobolev Training in Gaussian Setting}

\textbf{Modified Sobolev Operator}:
\[ P_s^{(Gauss)} = (I + \mathcal{L}_{OU})^s \]

\textbf{Eigenvalue Modification}:
\[ \mu_\alpha^{(P_s)} = (1 + |\alpha|)^s \]

\textbf{Composite Learning Operator}:
The learning dynamics are governed by:
\[ \mathcal{T}_s^{(Gauss)} = K^{\infty} \circ (I + \mathcal{L}_{OU})^s \]

\begin{theorem}[Gaussian Domain Scaling Laws]
For the Gaussian setting, eigenvalues scale as:
\[ \lambda_\alpha \sim \hat{k}(|\alpha|^{1/2}) \cdot (1 + |\alpha|)^s \]
where the first term decays exponentially for typical NTK kernels, while the second grows polynomially.
\end{theorem}

\textbf{Critical Insight}: The Gaussian measure introduces natural regularization through exponential decay of the kernel's Fourier transform, making high-frequency components automatically suppressed.

\subsection{Extension 2: Toroidal Domain [0,1]^d}

\subsubsection{Torus Construction and Periodicity}

We transform the unit cube $[0,1]^d$ into a torus $\mathbb{T}^d$ by identifying opposite faces:
\[ \mathbb{T}^d = [0,1]^d / \sim \text{ where } x \sim y \text{ if } x_i - y_i \in \mathbb{Z} \text{ for all } i \]

\textbf{Advantage}: Periodic boundary conditions eliminate boundary effects while preserving the discrete structure.

\subsubsection{Fourier Analysis on the Torus}

\textbf{Orthonormal Basis}: The torus admits Fourier modes as eigenfunctions:
\[ e_k(x) = e^{2\pi i k \cdot x}, \quad k \in \mathbb{Z}^d \]

\textbf{Sobolev Operator on Torus}:
\[ P_s^{(Torus)} = \sum_{k \in \mathbb{Z}^d} (1 + \|k\|^2)^s |k\rangle\langle k| \]

\textbf{Discrete Implementation}:
For $N$ points per dimension, we work with the finite torus $(\mathbb{Z}/N\mathbb{Z})^d$ and discrete Fourier modes.

\subsubsection{NTK on Torus: Periodic Kernels}

\textbf{Kernel Periodization}: Any kernel $k(x,y)$ can be made periodic:
\[ k_{per}(x,y) = \sum_{n \in \mathbb{Z}^d} k(x, y + n) \]

\textbf{Spectral Representation}:
\[ k_{per}(x,y) = \sum_{k \in \mathbb{Z}^d} \hat{k}_{per}(k) e^{2\pi i k \cdot (x-y)} \]

\begin{theorem}[Toroidal Commutation]
The NTK matrix $K$ and Sobolev matrix $P_s$ on the torus commute:
\[ KP_s = P_sK \]
because both are circulant matrices diagonalized by the discrete Fourier transform.
\end{theorem}

\subsubsection{Computational Advantages}

\textbf{Fast Fourier Transform}: Matrix-vector products can be computed in $\mathcal{O}(N^d \log N)$ time using FFT.

\textbf{Eigenvalue Computation}: Explicit eigenvalues are:
\begin{align}
\lambda_k^{(K)} &= \hat{k}_{per}(k) \\
\lambda_k^{(P_s)} &= (1 + \|k\|^2)^s \\
\lambda_k^{(KP_s)} &= \hat{k}_{per}(k) \cdot (1 + \|k\|^2)^s
\end{align}

\textbf{Scaling Laws}: Similar to the spherical case:
\[ \lambda_k \sim \|k\|^{-\alpha} \cdot (1 + \|k\|^2)^s = \|k\|^{2s-\alpha} \]

\subsubsection{Function Symmetrization: From [0,1]^d to Torus}

\textbf{Motivation}: To obtain optimization bounds using the toroidal NTK framework, we need to work with periodic functions. Any function $f: [0,1]^d \to \mathbb{R}$ can be extended to a periodic function through systematic symmetrization.

\begin{theorem}[Symmetrization Extension]
Given a function $f: [0,1]^d \to \mathbb{R}$, we can construct a periodic extension $\tilde{f}: \mathbb{R}^d \to \mathbb{R}$ such that:
\begin{enumerate}
\item $\tilde{f}(x) = f(x)$ for $x \in [0,1]^d$
\item $\tilde{f}$ is periodic with period 2 in each coordinate
\item $\tilde{f}$ is even with respect to reflection about $x_i = 0.5$ for each $i$
\end{enumerate}
\end{theorem}

\textbf{Construction Algorithm}:
For each coordinate $x_i$, we perform symmetric extension:
\begin{align}
\tilde{f}(x_1, \ldots, x_i, \ldots, x_d) = \begin{cases}
f(x_1, \ldots, x_i, \ldots, x_d) & \text{if } x_i \in [0,1] \\
f(x_1, \ldots, 2-x_i, \ldots, x_d) & \text{if } x_i \in [1,2] \\
\tilde{f}(x_1, \ldots, x_i + 2, \ldots, x_d) & \text{otherwise (periodicity)}
\end{cases}
\end{align}

\textbf{Geometric Interpretation}:
\begin{itemize}
\item \textbf{Original function}: Defined on unit cube $[0,1]^d$
\item \textbf{First reflection}: Create $2^d$ copies by reflecting across each face
\item \textbf{Periodic extension}: Tile the resulting $[0,2]^d$ cube periodically
\end{itemize}

\begin{proposition}[Smoothness Preservation]
If $f \in C^k([0,1]^d)$ and satisfies the boundary condition $\frac{\partial^j f}{\partial x_i^j}\big|_{x_i=0} = \frac{\partial^j f}{\partial x_i^j}\big|_{x_i=1} = 0$ for $j = 1, \ldots, k-1$, then $\tilde{f} \in C^k(\mathbb{T}^d)$.
\end{proposition>

Optimization Bound Transfer:
Let $\mathcal{L}_{\text{torus}}[\tilde{f}]$ be the loss function on the torus. Then:
\[ \mathcal{L}_{[0,1]^d}[f] \leq C \cdot \mathcal{L}_{\text{torus}}[\tilde{f}] \]
where the constant $C$ depends on the dimension $d$ and the symmetrization procedure. Note that $C$ suffers from the curse of dimensionality, typically growing exponentially as $C \sim 2^{cd}$ for some constant $c > 0$, which limits the practical applicability to moderate dimensions.

\begin{proof}[Proof Sketch]
The key insight is that the original function $f$ is recovered by restricting $\tilde{f}$ to $[0,1]^d$. The Sobolev norm of $\tilde{f}$ controls the Sobolev norm of $f$ due to the systematic construction, with additional regularity coming from the reflection symmetries.
\end{proof}

\textbf{Practical Advantages}:
\begin{enumerate}
\item \textbf{Universal applicability}: Any function on $[0,1]^d$ can be analyzed
\item \textbf{Preservation of structure}: Key properties (smoothness, polynomial approximation bounds) transfer
\item \textbf{Computational efficiency}: Leverage FFT-based algorithms
\item \textbf{Theoretical tractability}: Full spectral analysis applies
\end{enumerate}

\subsubsection{Practical Implementation}

\textbf{Matrix Construction}:
\begin{enumerate}
\item Symmetrize the target function $f$ to obtain $\tilde{f}$
\item Compute periodized kernel values $k_{per}(x_i, x_j)$
\item Apply FFT to obtain $\hat{k}_{per}(k)$
\item Construct $P_s$ in Fourier space: diagonal with entries $(1 + \|k\|^2)^s$
\item Compute $KP_s$ as element-wise product in Fourier domain
\item Extract results for original domain $[0,1]^d$
\end{enumerate}

\textbf{Complexity Comparison}:
\begin{itemize}
\item \textbf{Sphere}: $\mathcal{O}(n^2 L^{d-1})$ for harmonics up to degree $L$
\item \textbf{Torus}: $\mathcal{O}(N^d \log N)$ for $N^d$ grid points
\item \textbf{Symmetrized Torus}: $\mathcal{O}(N^d \log N)$ with 4× memory overhead in 2D
\end{itemize}

\subsubsection{Key Computational Advantage: Orthogonality by Sampling}

\textbf{Fundamental Difference}: The toroidal domain provides a crucial computational advantage over spherical domains through the orthogonality properties of Fourier modes under uniform sampling.

\textbf{Fourier Orthogonality}: For uniform sampling on $[0,1]$, discrete Fourier modes are exactly orthogonal:
\[ \frac{1}{N} \sum_{j=0}^{N-1} e^{2\pi i k_1 j/N} e^{-2\pi i k_2 j/N} = \delta_{k_1, k_2 \bmod N} \]

\textbf{Explicit Eigenvector Construction}:
For the $d$-dimensional torus with $N^d$ uniform grid points $\mathbf{x}_{j_1,\ldots,j_d} = (j_1/N, \ldots, j_d/N)$, the eigenvectors of any circulant matrix (including $K$ and $P_s$) are:
\[ \mathbf{v}_{\mathbf{k}}[j_1, \ldots, j_d] = \frac{1}{N^{d/2}} \prod_{i=1}^d e^{2\pi i k_i j_i/N} \]
where $\mathbf{k} = (k_1, \ldots, k_d) \in \{0, 1, \ldots, N-1\}^d$.

\textbf{Explicit Eigenvalue Access}:
Since we know the eigenvectors explicitly, we can compute eigenvalues of $K$ directly:
\[ \lambda_{\mathbf{k}}(K) = \mathbf{v}_{\mathbf{k}}^* K \mathbf{v}_{\mathbf{k}} = \sum_{i,j=1}^{N^d} K_{ij} e^{2\pi i \mathbf{k} \cdot (\mathbf{x}_i - \mathbf{x}_j)/N} \]

\textbf{Real-Valued Representation}:
For real functions, we can use cosine and sine modes:
\begin{align}
\text{Cosine modes:} \quad &\mathbf{c}_{\mathbf{k}}[j_1, \ldots, j_d] = \sqrt{\frac{2^d}{N^d}} \prod_{i=1}^d \cos(2\pi k_i j_i/N) \\
\text{Sine modes:} \quad &\mathbf{s}_{\mathbf{k}}[j_1, \ldots, j_d] = \sqrt{\frac{2^d}{N^d}} \prod_{i=1}^d \sin(2\pi k_i j_i/N)
\end{align}

\begin{remark}[Contrast with Spherical Harmonics]
\textbf{Spherical Domain Limitation}: Spherical harmonics $Y_{\ell,p}(\mathbf{x})$ are \emph{not} orthogonal under uniform sampling on the sphere. This creates several challenges:

\begin{itemize}
\item \textbf{No explicit eigenvectors}: The eigenvectors of the NTK matrix $K$ cannot be written in closed form
\item \textbf{Specialized quadratures required}: Gauss-Legendre or other sophisticated integration schemes needed
\item \textbf{Computational overhead}: Matrix diagonalization required to find eigenvectors
\item \textbf{Approximation errors}: Discrete orthogonality only holds asymptotically
\end{itemize}

\textbf{Toroidal Advantage}: The exact orthogonality of Fourier modes under uniform sampling means:
\begin{itemize}
\item \textbf{Explicit spectral decomposition}: All eigenvectors known analytically
\item \textbf{Fast algorithms}: FFT provides $\mathcal{O}(N^d \log N)$ eigenvalue computation
\item \textbf{Exact discrete analysis}: No approximation errors in spectral decomposition
\item \textbf{Implementation simplicity}: Standard FFT libraries suffice
\end{itemize}
\end{remark}

\textbf{Practical Implications}:
\begin{enumerate}
\item \textbf{Direct eigenvalue computation}: $\lambda_{\mathbf{k}}(K) = \text{FFT}[\text{first row of } K]_{\mathbf{k}}$
\item \textbf{Efficient matrix-vector products}: $K\mathbf{v} = \text{IFFT}[\text{FFT}[\mathbf{v}] \odot \boldsymbol{\lambda}]$
\item \textbf{Spectral filtering}: Easy implementation of frequency-dependent operations
\item \textbf{Condition number analysis}: Direct access to $\kappa(K) = \max_{\mathbf{k}} \lambda_{\mathbf{k}} / \min_{\mathbf{k}} \lambda_{\mathbf{k}}$
\end{enumerate>

This orthogonality property fundamentally changes the computational landscape, making toroidal domains particularly attractive for large-scale NTK analysis and optimization bound derivation.

\subsubsection{Kronecker Structure and Vandermonde Diagonalization}

\textbf{Tensor Product Structure}: The key computational advantage stems from the separable structure of the hypercube domain, which allows us to express all operations as Kronecker products.

\textbf{Kronecker Factorization}: The $d$-dimensional DFT factorizes as:
\[ \mathbf{F}_d = \mathbf{F}_1 \otimes \mathbf{F}_1 \otimes \cdots \otimes \mathbf{F}_1 \]

\textbf{1D DFT Matrix Structure}: The fundamental building block is the Vandermonde-type matrix:
\[ (\mathbf{F}_1)_{jk} = \frac{1}{\sqrt{N}} e^{2\pi i jk/N}, \quad j,k = 0, 1, \ldots, N-1 \]

This is a scaled DFT matrix with the crucial property:
\[ \mathbf{F}_1^* \mathbf{F}_1 = \mathbf{I}_N \]

\textbf{Sobolev Loss Computation via Kronecker Products}:
The Sobolev matrix $P_s$ on the hypercube can be written as:
\[ P_s = \sum_{\mathbf{k}} (1 + \|\mathbf{k}\|^2)^s \mathbf{v}_{\mathbf{k}} \mathbf{v}_{\mathbf{k}}^* \]

Using the Kronecker structure:
\[ P_s = \mathbf{F}_d^* \text{diag}((1 + \|\mathbf{k}\|^2)^s) \mathbf{F}_d \]

\textbf{Practical Algorithm}: 
\begin{enumerate}
\item \textbf{Forward FFT}: $\hat{\mathbf{f}} = \mathbf{F}_d \mathbf{f}$ using $d$ successive 1D FFTs
\item \textbf{Spectral multiplication}: $\hat{\mathbf{g}}_{\mathbf{k}} = (1 + \|\mathbf{k}\|^2)^s \hat{\mathbf{f}}_{\mathbf{k}}$ (element-wise)
\item \textbf{Inverse FFT}: $\mathbf{g} = \mathbf{F}_d^* \hat{\mathbf{g}}$ using $d$ successive 1D IFFTs
\end{enumerate>

\textbf{DFT as Vandermonde Matrix}:
The 1D DFT matrix is a Vandermonde matrix with roots of unity:
\[ (\mathbf{F}_1)_{jk} = \frac{1}{\sqrt{N}} \omega_N^{jk} \text{ where } \omega_N = e^{2\pi i/N} \]

\textbf{Key Insight}: The DFT matrix $\mathbf{F}_1$ diagonalizes circulant matrices, which are precisely the discretizations of translation-invariant operators on the torus.

\textbf{Eigenvalue Access}:
For circulant matrix $\mathbf{C}$ with first row $\mathbf{c}$:
\[ \text{eigenvalues}(\mathbf{C}) = \text{FFT}(\mathbf{c}) \]

\textbf{Computational Complexity Analysis}:
\begin{itemize}
\item \textbf{Naive approach}: $\mathcal{O}(N^{2d})$ for full matrix operations
\item \textbf{Kronecker FFT}: $\mathcal{O}(d \cdot N^d \log N)$ for all spectral operations
\item \textbf{Memory}: $\mathcal{O}(N^d)$ instead of $\mathcal{O}(N^{2d})$ for explicit matrices
\end{itemize>

\textbf{Implementation Strategy}:
\begin{enumerate}
\item \textbf{Precompute spectral multipliers}: Store $(1 + \|\mathbf{k}\|^2)^s$ for all frequency modes
\item \textbf{Use in-place FFTs}: Minimize memory allocation
\item \textbf{Leverage symmetry}: Real functions have conjugate symmetric spectra
\item \textbf{Batch processing}: Apply to multiple functions simultaneously
\end{enumerate>

\begin{remark}[Numerical Implementation Plan]
We will implement this framework with the following components:

\textbf{Core Functions}:
\begin{itemize}
\item \texttt{sobolev\_transform(f, s)}: Apply $(I + (-\Delta)^{1/2})^s$ to function $f$
\item \texttt{ntk\_eigenvalues(kernel\_type, N, d)}: Compute NTK eigenvalues via FFT
\item \texttt{composite\_spectrum(K\_evals, s)}: Multiply NTK and Sobolev spectra
\item \texttt{optimization\_bounds(f, K, s)}: Derive convergence rates
\end{itemize}

\textbf{Optimization Focus}:
\begin{itemize}
\item Exploit NumPy/SciPy's optimized FFT implementations
\item Use vectorized operations for spectral multiplications
\item Implement memory-efficient algorithms for large $N^d$
\item Provide both exact and approximate variants for different precision needs
\end{itemize>

This implementation will demonstrate the practical advantages of the toroidal framework for large-scale NTK-Sobolev analysis.
\end{remark>

\subsection{Domain Selection Criteria}

\begin{itemize}
\item \textbf{Sphere $\mathbb{S}^{d-1}$}: Natural for directional data, rotation invariance
\item \textbf{Gaussian $\mathbb{R}^d$}: Infinite domain with natural regularization  
\item \textbf{Torus $\mathbb{T}^d$}: Finite domain, efficient computation, no boundary effects
\end{itemize}

\newpage

\subsection{Comparison with Two–Layer NTK}

For reference, the (infinite-width) two–layer ReLU NTK on the sphere reads
\[
  K^{(2)}(x,y)=\|x\|\,\|y\|\,\Bigl(\tfrac{\pi-\theta}{\pi}\,\cos \theta + \tfrac{\sin\theta}{\pi}\Bigr),
\]
where $\theta=\arccos(\langle x,y\rangle)$.  Its spherical–harmonic eigenvalues satisfy
\[
  \mu_\ell^{(2)}\;\asymp\;\ell^{-(d+1)}.
\]

In the deep–narrow limit the limiting kernel (Theorem~1) has eigenvalues
\[
   \mu_\ell^{(\text{dn})}\;\asymp\;C(L,d)\,\ell^{-d}, \qquad C(L,d)\propto L,
\]
which decay more slowly and grow linearly with depth $L$.  Consequently
\begin{itemize}
  \item the top eigenvalue of $K^{(\text{dn})}$ is roughly $L$ times larger than for $K^{(2)}$;
  \item the spectrum is flatter, hence the condition number
        $\kappa\bigl(K^{(\text{dn})}\bigr)\approx\kappa\bigl(K^{(2)}\bigr)/L$ improves with depth.
\end{itemize}

This comparison highlights that depth compensates partly for the poor conditioning intrinsic to the two–layer kernel.

\end{document}
