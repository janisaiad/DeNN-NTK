\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usetikzlibrary{math}

% Mathematical commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\evmin}[1]{\lambda_{\min}\left(#1\right)}
\newcommand{\evmax}[1]{\lambda_{\max}\left(#1\right)}
\newcommand{\svmin}[1]{\sigma_{\min}\left(#1\right)}
\newcommand{\tr}{\text{tr}}
\newcommand{\KNTK}{K_{\text{NTK}}}
\newcommand{\Kinf}{K^{\infty}}
\newcommand{\Sd}{\mathbb{S}^{d-1}}
\newcommand{\Lap}{\Delta}
\newcommand{\Ls}{\mathcal{L}_s}
\newcommand{\limiting}[1]{#1^{\infty}}

\usetheme{Madrid}
\usecolortheme{default}

\title{Spectral Analysis of the Neural Tangent Kernel}
\subtitle{Sobolev Training and Eigenvalue Scaling Laws}
\author{Comprehensive Synthesis and Analysis}
\date{\today}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\section{Introduction and Objectives}

\begin{frame}{Key Objectives and Scaling Laws}
\textbf{Three fundamental objectives:}
\begin{enumerate}
\item \textbf{Eigenvalue scaling laws}: Derive decay rates $\mu_\ell \sim \ell^{-\alpha}$ for NTK operator eigenvalues
\item \textbf{Spectral impact on learning}: Understand how spectral properties determine learning dynamics
\item \textbf{Matrix vs. Operator relationship}: Analyze scaling laws for discrete matrix eigenvalues with respect to depth $l$ and data size $n$
\end{enumerate}

\textbf{NTK Definition:}
$$K^{\infty}(x_i, x_j) = \left\langle \frac{\partial f(\mathbf{x}_i; \theta)}{\partial \theta}, \frac{\partial f(\mathbf{x}_j; \theta)}{\partial \theta} \right\rangle$$
\end{frame}

\begin{frame}{Choice of Spherical Domain}
\textbf{Focus on spherical domains} $\mathbb{S}^{d-1}$:

\textbf{Advantages:}
\begin{itemize}
\item Computational tractability via spherical harmonic symmetrization
\item Explicit spectral decompositions
\item Rotational invariance
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
\item No uniform sampling measure on the sphere
\item Requires spectrum analysis via inverse cosine distance matrix approximation
\item Motivates exploration of alternative domains
\end{itemize}

\textbf{Crucial distinction:}
\begin{itemize}
\item \textbf{NTK Matrix}: Discrete sampled version $K \in \mathbb{R}^{n \times n}$
\item \textbf{NTK Operator}: Continuous integral operator $(\mathcal{L}f)(x) = \int K^{\infty}(x,y)f(y)dy$
\end{itemize}
\end{frame}

\begin{frame}{Initialization: Edge of Chaos}
\textbf{All analysis assumes Edge of Chaos (EOC) initialization:}

\begin{itemize}
\item Weights initialized as $w_{ij} \sim \mathcal{N}(0, \sigma_w^2/\text{fan-in})$
\item For ReLU networks: $\sigma_w^2 = 2$ to maintain unit variance
\item Prevents activation explosion or vanishing with depth
\end{itemize}

\textbf{Key parameter:} $\Delta_\phi = \frac{b^2}{a^2+b^2} = 0.5$ (for standard ReLU)

\textbf{Cosine map:}
\[ \varrho(\rho) = \rho + \Delta_\phi \frac{2}{\pi}\left( \sqrt{1-\rho^2} - \rho \arccos(\rho) \right) \]
\end{frame}

\section{NTK Matrix Structure and Spectrum}

\begin{frame}{NTK Matrix Structure}
\textbf{Example for 3 points} $x_1, x_2, x_3 \in \mathbb{R}^d$:
\[
K^{\infty} = \begin{pmatrix} 
k(x_1,x_1) & k(x_1,x_2) & k(x_1,x_3) \\
k(x_2,x_1) & k(x_2,x_2) & k(x_2,x_3) \\
k(x_3,x_1) & k(x_3,x_2) & k(x_3,x_3)
\end{pmatrix}
\]

\textbf{General NTK for depth $l$ networks:}
\begin{align*}
K^{\infty}(\mathbf{x}_1, \mathbf{x}_2) &= \|\mathbf{x}_1\| \|\mathbf{x}_2\| \left( \sum_{k=1}^l \varrho^{\circ (k-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right) \right. \\
&\quad \left. \times \prod_{k'=k}^{l-1} \varrho'\left(\varrho^{\circ (k'-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right)\right) \right) \mathbf{I}_{m_l}
\end{align*}

\textbf{Special case - 2-layer ReLU networks:}
\[
k(x_i,x_j) = x_i^T x_j \cdot \arccos(-\langle x_i,x_j \rangle) + \sqrt{1-\langle x_i,x_j \rangle^2}
\]
\end{frame}

\begin{frame}{NTK Operator and Properties}
\textbf{NTK operator action:}
\[
(K^{\infty} f)(x) = \int_{\mathbb{S}^{d-1}} k(x,y)f(y)d\sigma(y)
\]

\textbf{Key properties:}
\begin{itemize}
\item Symmetric positive definite operator
\item Eigenfunctions: spherical harmonics $Y_{\ell,p}$
\item Eigenvalues decay polynomially: $\mu_\ell \sim \ell^{-d}$
\end{itemize}

\textbf{Matrix spectrum results:}

\textbf{Condition number:}
\[ \kappa(K^{\infty}) \sim 1 + \frac{n}{3} + \mathcal{O}(n \xi / l) \]

\textbf{Eigenvalue distribution:}
\[ \lambda_{\text{min}} \sim \frac{3l}{4n}, \quad \lambda_{\text{max}} \sim \frac{3l}{4} \pm \xi \text{ where } \xi \sim \log(l) \]
\end{frame}

\section{Sobolev Training and Spectral Modification}

\begin{frame}{NTK-Sobolev Operator Framework}
\textbf{Key innovation:} Modification of the standard $L^2$ loss to incorporate high-order derivatives.

\textbf{Spherical harmonics transform:} $\mathcal{F}$: $L^2(\mathbb{S}^d) \to \bigoplus_{\ell=0}^{\infty} \mathbb{C}^{N(d,\ell)}$

\textbf{Sobolev operator $P_s$ defined in Fourier space:}
\[ P_s = \sum_{\ell=0}^{\ell_{\max}} \sum_{p=1}^{N(d,\ell)} (1+\ell)^{2s}P_{\ell,p} \]

where $P_{\ell,p} = a_{\ell,p}a_{\ell,p}^T$ with spectral coefficients $(a_{\ell,p})_i = c_iY_{\ell,p}(x_i)$.

\textbf{Dimension of spherical harmonic spaces:}
\[ N(d,\ell) \sim \frac{2\ell^{d-2}}{(d-2)!} \quad \text{as } \ell \to \infty \]
\end{frame}

\begin{frame}{Commutation Property}
\begin{theorem}[Spherical Harmonics as Common Eigenfunctions]
For rotationally invariant kernels on $\mathbb{S}^{d-1}$:
\begin{itemize}
\item NTK operator $K^{\infty}$ and Sobolev operator $P_s$ share spherical harmonics $Y_{\ell,p}$ as eigenfunctions
\item Due to rotational invariance and Schur's lemma
\item Enables direct spectral modification analysis
\end{itemize}
\end{theorem}

\begin{theorem}[Commutation Property]
The NTK operator $K^{\infty}$ and Sobolev operator $P_s$ commute:
\[ [K^{\infty}, P_s] = 0 \]
This holds for any sampling distribution $\rho(x)$ on $\mathbb{S}^{d-1}$.
\end{theorem}
\end{frame}

\section{Main Proofs}

\begin{frame}{Proof 1: Sobolev Loss as Fractional Laplacian}
\begin{theorem}[Fractional Laplacian Representation of Sobolev Loss]
For a function $f \in H^s(\mathbb{S}^{d-1})$ with $s > 0$, the Sobolev loss can be written as:
\[ \mathcal{L}_s[f] = \int_{\mathbb{S}^{d-1}} f(x) (I + (-\Delta)^{1/2})^s f(x) d\sigma(x) \]
where $(I + (-\Delta)^{1/2})^s$ is the fractional Laplacian operator of order $s$.
\end{theorem}

\textbf{Proof sketch:}
\begin{itemize}
\item Spherical harmonic expansion: $f(x) = \sum_{\ell,p} \hat{f}_{\ell,p} Y_{\ell,p}(x)$
\item Laplacian action: $(-\Delta)^{1/2}_{\mathbb{S}^{d-1}} Y_{\ell,p}(x) = \sqrt{\ell(\ell + d - 2)} Y_{\ell,p}(x)$
\item Bilinear form via orthonormality of spherical harmonics
\end{itemize}
\end{frame}

\begin{frame}{Proof 2: NTK Operator Multiplication by Sobolev Operator}
\begin{theorem}[NTK-Sobolev Composition]
Under Sobolev training, the learning operator is given by the composition:
\[ \mathcal{T}_s = K^{\infty} \circ (I + (-\Delta)^{1/2})^s \]
\end{theorem}

\textbf{Proof:}
\begin{itemize}
\item Standard gradient descent: $\frac{df}{dt} = -K^{\infty}(f - y)$
\item Sobolev loss: $\mathcal{L}_s(\theta) = \frac{1}{2}\|f(\cdot; \theta) - y\|^2_{H^s}$
\item Chain rule via fractional operator
\item Resulting dynamics: $\frac{df}{dt} = -K^{\infty} \circ (I + (-\Delta)^{1/2})^s (f - y)$
\end{itemize}
\end{frame}

\begin{frame}{Proof 3: Spectral Properties of the Composite Operator}
\begin{theorem}[Spectrum of NTK-Sobolev Operator]
The eigenvalues of the composite operator $\mathcal{T}_s = K^{\infty} \circ (I + (-\Delta)^{1/2})^s$ are given by the product of individual eigenvalues:
\[ \mu_\ell^{(\mathcal{T}_s)} = \mu_\ell^{(K)} \cdot (1 + \sqrt{\ell(\ell + d - 2)})^s \]
\end{theorem}

\textbf{Direct consequence:} Commutation and common eigenfunctions allow simple multiplication of spectra.
\end{frame}

\begin{frame}{Proof 4: Commutation of Discrete Matrix Operators}
\begin{theorem}[Matrix Commutation]
The discrete matrices $K$ and $P_s$ commute: $KP_s = P_sK$.
\end{theorem}

\textbf{Proof:}
\begin{itemize}
\item NTK expansion: $K = \sum_{\ell,p} \mu_\ell^{(K)} a_{\ell,p} a_{\ell,p}^T$
\item Sobolev expansion: $P_s = \sum_{\ell,p} (1 + \sqrt{\ell(\ell + d - 2)})^s a_{\ell,p} a_{\ell,p}^T$
\item Orthogonality: $a_{\ell,p}^T a_{\ell',p'} = \delta_{\ell,\ell'} \delta_{p,p'} \|a_{\ell,p}\|^2$
\item Scalar commutation: $KP_s = P_sK$
\end{itemize}

\textbf{Underlying reason:} Reflects commutation of continuous operators $K^{\infty}$ and $P_s$.
\end{frame}

\begin{frame}{Proof 5: Eigenvalue Scaling Laws}
\begin{theorem}[Asymptotic Scaling Laws]
For the NTK-Sobolev operator, eigenvalues follow the scaling laws:
\[ \lambda_\ell \sim \ell^{-d} \cdot (1 + \sqrt{\ell(\ell + d - 2)})^s \]
\end{theorem}

\textbf{Asymptotic analysis:} For large $\ell$,
\[ \lambda_\ell^{(\mathcal{T}_s)} \sim C(d, L) \ell^{-d} \cdot (1 + \ell)^s = C(d, L) \ell^{s-d} \]

\textbf{Critical spectral behavior:}
\begin{itemize}
\item $s < d$: Eigenvalue decay - regularizing effect
\item $s = d$: Logarithmic corrections - critical regime
\item $s > d$: Eigenvalue growth - high-frequency amplification
\end{itemize}
\end{frame}

\section{Practical Implementation and Discretization}

\begin{frame}{From Sobolev Loss to Discrete Matrix Operator}
\textbf{Two integral formulations:}

\textbf{Formulation 1 - Uniform Lebesgue measure:}
\[ \mathcal{L}_s[f] = \int_{\mathbb{S}^{d-1}} f(x) (I + (-\Delta)^{1/2})^s f(x) d\sigma(x) \]

\textbf{Formulation 2 - Sampling measure:}
\[ \mathcal{L}_s[f] = \int_{\mathbb{S}^{d-1}} f(x) (I + (-\Delta)^{1/2})^s f(x) d\mu_n(x) \]
where $\mu_n = \frac{1}{n}\sum_{i=1}^n \delta_{x_i}$

\textbf{Discrete implementation:}
\[ \mathcal{L}_s[f] = \frac{1}{n}\sum_{i=1}^n f(x_i) \left[(I + (-\Delta)^{1/2})^s f\right](x_i) \]

\textbf{Matrix form:} $\mathcal{L}_s[f] \approx \mathbf{f}^T P_s \mathbf{f}$
\end{frame}

\begin{frame}{Practical Implementation Considerations}
\textbf{Construction of matrix $P_s$:}
\[ (P_s)_{ij} = \sum_{\ell=0}^{\ell_{\max}} \sum_{p=1}^{N(d,\ell)} (1 + \sqrt{\ell(\ell + d - 2)})^s Y_{\ell,p}(x_i) Y_{\ell,p}(x_j) \]

\textbf{Computational complexity:}
\begin{itemize}
\item Spherical harmonics evaluation: $\mathcal{O}(n^2 \ell_{\max}^{d-1})$
\item Truncation strategy: balance spectral accuracy and feasibility
\item Alternative with gradients: $\mathcal{O}(nd)$ instead of $\mathcal{O}(n^2\ell_{\max}^{d-1})$
\end{itemize}

\textbf{Fourier decomposition:}
\[ \mathcal{L}_s[f] = \sum_{\ell=0}^{\infty} \sum_{p=1}^{N(d,\ell)} (1+\ell)^{2s} |\hat{f}_{\ell,p}|^2 \]

\textbf{Dimension growth:}
\[ N(d,\ell) \sim \frac{2\ell^{d-2}}{(d-2)!} \quad \text{determines complexity} \]
\end{frame}

\section{Deep NTK Analysis}

\begin{frame}{Deep NTK Properties}
\textbf{Limiting NTK at EOC for $L$-layer networks:}
\begin{align*}
K^{\infty}(\mathbf{x}_1, \mathbf{x}_2) &= \|\mathbf{x}_1\| \|\mathbf{x}_2\| \left( \sum_{k=1}^l \varrho^{\circ (k-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right) \right. \\
&\quad \left. \times \prod_{k'=k}^{l-1} \varrho'\left(\varrho^{\circ (k'-1)}\left(\rho_1^{\infty}(\mathbf{x}_1, \mathbf{x}_2)\right)\right) \right) \mathbf{I}_{m_l}
\end{align*}

\textbf{Eigenvalue decay:} For $L$-layer ReLU networks with $L \geq 3$:
\[ \mu_k \sim C(d, L)k^{-d} \]
where $C(d, L)$ depends on parity of $k$ and grows quadratically with $L$.

\textbf{For normalized NTK} $\kappa^L_{\text{NTK}}/L$: $C(d, L)$ grows linearly with $L$.
\end{frame}

\begin{frame}{Inverse Cosine Distance Matrix Analysis}
\textbf{Inverse cosine distance matrix} $W_k$ for depth $k$:
\[ {W_k}_{i,i} = 0, \quad {W_k}_{i_1,i_2} = \left( \frac{1 - \rho_k(x_{i_1},x_{i_2})}{2} \right)^{-\frac{1}{2}} \text{ for } i_1 \neq i_2 \]

\textbf{Near-affine behavior:}
\begin{itemize}
\item NTK matrix $K^{\infty} \approx A \cdot W_l + B$ (affine dependence)
\item Spectral bounds transfer from $W_k$ to NTK via this affine relationship
\item Error terms: $O(k^{-1})$ - decreases with depth
\end{itemize}

\textbf{Implication:} This relationship enables indirect analysis of NTK spectral properties via simpler geometric matrices.
\end{frame}

\section{Deep Narrow Neural Networks}

\begin{frame}{Scaled NTK at Initialization}
\begin{theorem}[Scaled NTK at Initialization]
For $f^L_\theta$ initialized appropriately, as $L \to \infty$:
\[ \tilde{\Theta}^L_0(x, x') \xrightarrow{p} \tilde{\Theta}^\infty(x, x') \]
where
\[ \tilde{\Theta}^\infty(x, x') = (x^T x' + 1 + \E_g[\sigma(g(x))\sigma(g(x'))]) I_{d_{out}} \]
with $g \sim \text{GP}(0, \rho^2 d_{in}^{-1} x^T x' + \beta^2)$.
\end{theorem}

\textbf{Alternative formulation:} $\kappa_1(\cos(u) \cdot v)$ where:
\begin{itemize}
\item $v = \frac{1}{1 + \beta^2/\alpha^2}$, where $\beta$ is bias variance
\item $\alpha = \frac{\|x\| \|x'\| \rho}{d_{in}}$, where $\cos(u)$ is cosine distance
\end{itemize}
\end{frame}

\begin{frame}{Comparison with Two-Layer Kernel}
\textbf{Infinite-width two-layer ReLU NTK on the sphere:}
\[
  K^{(2)}(x,y)=\|x\|\,\|y\|\,\Bigl(\tfrac{\pi-\theta}{\pi}\,\cos \theta + \tfrac{\sin\theta}{\pi}\Bigr)
\]
where $\theta=\arccos(\langle x,y\rangle)$. Spherical harmonic eigenvalues:
\[
  \mu_\ell^{(2)}\;\asymp\;\ell^{-(d+1)}
\]

\textbf{In the deep-narrow limit:}
\[
   \mu_\ell^{(\text{dn})}\;\asymp\;C(L,d)\,\ell^{-d}, \qquad C(L,d)\propto L
\]

\textbf{Consequences:}
\begin{itemize}
  \item Maximum eigenvalue of $K^{(\text{dn})}$ about $L$ times larger than for $K^{(2)}$
  \item Flatter spectrum: $\kappa\bigl(K^{(\text{dn})}\bigr)\approx\kappa\bigl(K^{(2)}\bigr)/L$
\end{itemize}

\textbf{Conclusion:} Depth partially compensates for poor conditioning of the two-layer kernel.
\end{frame}

\begin{frame}{Research Perspectives}
\textbf{Architectural modifications:}
\begin{itemize}
\item Unit concatenation: Combine multiple narrow networks
\item Skip connections: ResNet-style connections
\end{itemize}

\textbf{Initialization studies:}
\begin{itemize}
\item Alternative initializations
\item $\beta \to 0$ limit: $v \to 1$ when bias variance vanishes
\item Complex kernel structures
\end{itemize}

\textbf{Theoretical extensions:}
\begin{itemize}
\item Extension of Hayou \& Yang's work on ResNets
\item Mean field analysis for deep narrow frameworks
\item Experimental validation on practical tasks
\end{itemize}
\end{frame}

\section{Alternative Domains}

\begin{frame}{Gaussian Domain: $L^2(\mathbb{R}^d, \mu)$}
\textbf{Framework:} Gaussian measure $d\mu = (2\pi)^{-d/2} e^{-\|x\|^2/2} dx$

\textbf{Ornstein-Uhlenbeck operator:} $\mathcal{L}_{OU} = -\Delta + x \cdot \nabla$

\textbf{Eigenfunctions:} Hermite polynomials $H_{\alpha}(x)$ with eigenvalues $|\alpha|$

\textbf{NTK structure:} Translation-invariant $K^{\infty}(x, y) = k(x - y)$

\textbf{Composite spectrum:} $\lambda_\alpha \sim \hat{k}(|\alpha|^{1/2}) \cdot (1 + |\alpha|)^s$

\textbf{Critical insight:} Gaussian measure introduces natural regularization via exponential decay of the kernel's Fourier transform.
\end{frame}

\begin{frame}{Toroidal Domain: $\mathbb{T}^d$}
\textbf{Torus construction:} $\mathbb{T}^d$ obtained by gluing opposite faces of the unit cube $[0,1]^d$

\textbf{Fourier basis:} Fourier modes as orthonormal basis
\[
   e_k(x) \;=\; e^{2\pi i k\cdot x}, \qquad k\in\mathbb Z^d
\]

\textbf{Sobolev operator:} Diagonal in this basis
\[
   P_s^{(\mathbb{T}^d)}\,e_k\;=\;\bigl(1+\|k\|^2\bigr)^{s}\,e_k
\]

\textbf{Periodic NTK:} $k_{\text{per}}(x,y)=\sum_{n\in\mathbb Z^d} k\bigl(x, y+n\bigr)$

\textbf{Composite spectrum:} $\lambda_k^{(KP_s)} = \widehat{k}_{\text{per}}(k)\,(1+\|k\|^2)^{s} \sim \|k\|^{2s-\alpha}$

\textbf{Computational cost:} FFT enables $\mathcal O\bigl(N^d\log N\bigr)$ for matrix-vector products.
\end{frame}

\begin{frame}{Key Computational Advantage of Torus}
\textbf{Fundamental difference:} Orthogonality of Fourier modes under uniform sampling.

\textbf{Fourier orthogonality:}
\[ \frac{1}{N} \sum_{j=0}^{N-1} e^{2\pi i k_1 j/N} e^{-2\pi i k_2 j/N} = \delta_{k_1, k_2 \bmod N} \]

\textbf{Explicit construction of eigenvectors:}
\[ \mathbf{v}_{\mathbf{k}}[j_1, \ldots, j_d] = \frac{1}{N^{d/2}} \prod_{i=1}^d e^{2\pi i k_i j_i/N} \]

Since the diagonalization is already known (Fourier modes), no numerical eigenvector computation is needed: an FFT directly provides all eigenvalues. We can therefore, for each grid resolution $N$ and each network depth or width $L$, compute the complete spectrum of the NTK matrix sampled on the torus and empirically derive scaling laws ($\lambda_{\min}$, $\lambda_{\max}$, $\kappa$) with cost $\mathcal{O}(N^{d}\log N)$, much lower than on the sphere.
\end{frame}

\begin{frame}{Domain Selection Criteria}
\textbf{Domain comparison:}

\begin{itemize}
\item \textbf{Sphere $\mathbb{S}^{d-1}$}: Natural for directional data, rotational invariance
\item \textbf{Gaussian $\mathbb{R}^d$}: Infinite domain with natural regularization
\item \textbf{Torus $\mathbb{T}^d$}: Finite domain, efficient computation, no boundary effects
\end{itemize}

\textbf{Favored symmetries:}
\begin{itemize}
\item \textbf{Translation invariance}: Favors Fourier modes
\item \textbf{Gaussian weighting}: Favors Hermite polynomials
\end{itemize}

\textbf{Computational complexity:}
\begin{itemize}
\item \textbf{Sphere}: $\mathcal{O}(n^2 L^{d-1})$ for harmonics up to degree $L$
\item \textbf{Torus}: $\mathcal{O}(N^d \log N)$ for $N^d$ grid points
\item \textbf{Symmetrized torus}: $\mathcal{O}(N^d \log N)$ with 4× memory overhead in 2D
\end{itemize}
\end{frame}

\section{Reconciliation and Unified Framework}

\begin{frame}{Unified Spectral Viewpoint}
\textbf{Reconciling the geometric view of NTK and functional view of Sobolev training:}

\textbf{Shared invariance:}
\begin{itemize}
\item Matrices $K$ and $P_s$ rotationally invariant
\item Entries depend only on inner product $\langle x_i, x_j \rangle$
\end{itemize}

\textbf{Matrix commutation:} $K P_s = f(G)g(G) = g(G)f(G) = P_s K$

\textbf{Simultaneous diagonalization:} Same set of eigenvectors

\textbf{Product of eigenvalues:}
\[ \lambda_i(K P_s) = \lambda_i(K) \cdot \lambda_i(P_s) \]

\textbf{Proposed strategy:}
\begin{enumerate}
\item Approximate NTK spectrum via inverse cosine distance matrix
\item Analyze Sobolev operator spectrum
\item Combine via spectrum product and validate experimentally
\end{enumerate}
\end{frame}

\begin{frame}{Zonal Kernel Representation of $P_s$}
\begin{theorem}[Zonal Kernel Representation]
The Sobolev matrix $P_s$ can be written as a zonal kernel:
\[ (P_s)_{ij} = p_s(\langle x_i, x_j \rangle) \]
where
\[ p_s(t) = \sum_{\ell=0}^{\ell_{\max}} (1 + \sqrt{\ell(\ell + d - 2)})^s \frac{N(d,\ell)}{\text{Area}(\mathbb{S}^{d-1})} P_\ell^{((d-2)/2)}(t) \]
\end{theorem}

\textbf{Eigenvalue bounds for $P_s$:}
\[ 1 \leq \lambda_i(P_s) \leq (1 + \ell_{\max})^s \]

\textbf{Condition number:} $\kappa(P_s) = (1 + \ell_{\max})^s$

\textbf{Practical implications:}
\begin{itemize}
\item Zonal kernel $p_s(t)$ can be precomputed
\item Gegenbauer expansion enables fast multipole-type acceleration
\item Numerical stability analysis guided by condition number
\end{itemize}
\end{frame}

\section{Conclusions and Future Directions}

\begin{frame}{Summary of Key Results}
\textbf{Main contributions:}

\begin{enumerate}
\item \textbf{Eigenvalue scaling laws}: Relations $\mu_\ell \sim \ell^{-\alpha}$ and impact on learning dynamics

\item \textbf{Matrix vs. Operator distinction}: Fundamental difference between discrete and continuous eigenvalues

\item \textbf{Sobolev training framework}: NTK spectrum modification via spectral exponent $2s-d$

\item \textbf{Deep network analysis}: Scaling laws for condition numbers and eigenvalue distributions

\item \textbf{Spherical harmonic framework}: Commutation property $[K^{\infty}, P_s] = 0$ and common eigenfunctions
\end{enumerate}

\textbf{Key formula:} $\lambda_\ell \sim \ell^{s-d}$ determines spectral properties and learning dynamics.
\end{frame}

\begin{frame}{Research Challenges and Open Questions}
\begin{enumerate}
\item \textbf{Unified framework}: Different papers with varying domains, initializations, architectures, activations

\item \textbf{Beyond spherical domains}: Extension of harmonic analysis to general spaces $L^2(\gamma)$

\item \textbf{Optimal width scaling}: Achieve linear scaling in $\lambda_{\min}$ with minimal width

\item \textbf{Broad activation coverage}: Extension from ReLU to general inhomogeneous activations

\item \textbf{Experimental validation}: Systematic verification of theoretical predictions
\end{enumerate}

\textbf{Sobolev perspective:} Increase exponent $s$ with data size $n$ to counterbalance conditioning of $K$. Precise estimate of $\kappa(P_s)$ remains an open problem.
\end{frame}

\begin{frame}{Research Roadmap}
\textbf{Near-term objectives:}
\begin{itemize}
\item Study narrow NTK behavior to identify simplifications
\item Incorporate Sobolev framework into spherical harmonic analysis
\item Unify initialization schemes and architectural assumptions
\end{itemize}

\textbf{Long-term goals:}
\begin{itemize}
\item Extend harmonic analysis from spherical to general domains
\item Develop complete theory for deep narrow networks
\item Create unified spectral theory encompassing all major NTK variants
\end{itemize}

\textbf{Vision:} The convergence of spectral analysis, harmonic analysis, and neural network theory opens promising avenues for understanding and controlling learning dynamics of deep networks via their spectral properties.
\end{frame}

\begin{frame}{Acknowledgments and Questions}
\begin{center}
\textbf{Thank you for your attention!}

\vspace{1cm}

\textbf{Questions and Discussion}

\vspace{1cm}

\end{center}
\end{frame}

\end{document}
