\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu \& Li(2020)Allen-Zhu and Li]{allen2020backward}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Backward feature correction: How deep learning performs deep
  learning.
\newblock \emph{arXiv preprint arXiv:2001.04413}, 2020.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and
  Liang]{allen2019learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and
  Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2019{\natexlab{b}}.

\bibitem[Atkinson \& Han(2012)Atkinson and Han]{atkinson2012spherical}
Kendall Atkinson and Weimin Han.
\newblock \emph{Spherical harmonics and approximations on the unit sphere: an
  introduction}, volume 2044.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Azevedo \& Menegatto(2014)Azevedo and Menegatto]{azevedo2014sharp}
Douglas Azevedo and Valdir~Antonio Menegatto.
\newblock Sharp estimates for eigenvalues of integral operators generated by
  dot product kernels on the sphere.
\newblock \emph{Journal of Approximation Theory}, 177:\penalty0 57--68, 2014.

\bibitem[Bach(2013)]{bach2013sharp}
Francis Bach.
\newblock Sharp analysis of low-rank kernel matrix approximations.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2013.

\bibitem[Bach(2017{\natexlab{a}})]{bach2017breaking}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 18\penalty0
  (1):\penalty0 629--681, 2017{\natexlab{a}}.

\bibitem[Bach(2017{\natexlab{b}})]{bach2017equivalence}
Francis Bach.
\newblock On the equivalence between kernel quadrature rules and random feature
  expansions.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 18\penalty0
  (1):\penalty0 714--751, 2017{\natexlab{b}}.

\bibitem[Basri et~al.(2019)Basri, Jacobs, Kasten, and
  Kritchman]{basri2019convergence}
Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman.
\newblock The convergence rate of neural networks for learned functions of
  different frequencies.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Basri et~al.(2020)Basri, Galun, Geifman, Jacobs, Kasten, and
  Kritchman]{basri2020frequency}
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira
  Kritchman.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2020.

\bibitem[Belkin et~al.(2018)Belkin, Ma, and Mandal]{belkin2018understand}
Mikhail Belkin, Siyuan Ma, and Soumik Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem[Bietti \& Mairal(2019{\natexlab{a}})Bietti and
  Mairal]{bietti2019group}
Alberto Bietti and Julien Mairal.
\newblock Group invariance, stability to deformations, and complexity of deep
  convolutional representations.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 20\penalty0
  (25):\penalty0 1--49, 2019{\natexlab{a}}.

\bibitem[Bietti \& Mairal(2019{\natexlab{b}})Bietti and
  Mairal]{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019{\natexlab{b}}.

\bibitem[Brand et~al.(2020)Brand, Peng, Song, and Weinstein]{brand2020training}
Jan van~den Brand, Binghui Peng, Zhao Song, and Omri Weinstein.
\newblock Training (overparametrized) neural networks in near-linear time.
\newblock \emph{arXiv preprint arXiv:2006.11648}, 2020.

\bibitem[Cao et~al.(2019)Cao, Fang, Wu, Zhou, and Gu]{cao2019towards}
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu.
\newblock Towards understanding the spectral bias of deep learning.
\newblock \emph{arXiv preprint arXiv:1912.01198}, 2019.

\bibitem[Caponnetto \& De~Vito(2007)Caponnetto and
  De~Vito]{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0
  (3):\penalty0 331--368, 2007.

\bibitem[Chen \& Xu(2021)Chen and Xu]{chen2020deep}
Lin Chen and Sheng Xu.
\newblock Deep neural tangent kernel and laplace kernel have the same rkhs.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem[Chen et~al.(2020)Chen, Bai, Lee, Zhao, Wang, Xiong, and
  Socher]{chen2020towards}
Minshuo Chen, Yu~Bai, Jason~D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and
  Richard Socher.
\newblock Towards understanding hierarchical learning: Benefits of neural
  representations.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Chizat \& Bach(2018)Chizat and Bach]{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2018note}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Cho \& Saul(2009)Cho and Saul]{cho2009kernel}
Youngmin Cho and Lawrence~K Saul.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2009.

\bibitem[Cucker \& Smale(2002)Cucker and Smale]{cucker2002mathematical}
Felipe Cucker and Steve Smale.
\newblock On the mathematical foundations of learning.
\newblock \emph{Bulletin of the American mathematical society}, 39\penalty0
  (1):\penalty0 1--49, 2002.

\bibitem[Daniely(2017)]{daniely2017depth}
Amit Daniely.
\newblock Depth separation for neural networks.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2017.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2019bgradient}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2019agradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2019{\natexlab{b}}.

\bibitem[Efthimiou \& Frye(2014)Efthimiou and Frye]{costas2014spherical}
Costas Efthimiou and Christopher Frye.
\newblock \emph{Spherical harmonics in p dimensions}.
\newblock World Scientific, 2014.

\bibitem[El~Karoui(2010)]{el2010spectrum}
Noureddine El~Karoui.
\newblock The spectrum of kernel random matrices.
\newblock \emph{The Annals of Statistics}, 38\penalty0 (1):\penalty0 1--50,
  2010.

\bibitem[Eldan \& Shamir(2016)Eldan and Shamir]{eldan2016power}
Ronen Eldan and Ohad Shamir.
\newblock The power of depth for feedforward neural networks.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2016.

\bibitem[Fan \& Wang(2020)Fan and Wang]{fan2020spectra}
Zhou Fan and Zhichao Wang.
\newblock Spectra of the conjugate kernel and neural tangent kernel for
  linear-width neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Flajolet \& Sedgewick(2009)Flajolet and
  Sedgewick]{flajolet2009analytic}
Philippe Flajolet and Robert Sedgewick.
\newblock \emph{Analytic combinatorics}.
\newblock Cambridge University press, 2009.

\bibitem[Geifman et~al.(2020)Geifman, Yadav, Kasten, Galun, Jacobs, and
  Basri]{geifman2020similarity}
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen
  Basri.
\newblock On the similarity between the laplace and neural tangent kernels.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019linearized}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{arXiv preprint arXiv:1904.12191}, 2019.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2015.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{hornik1989multilayer}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Ismail(2005)]{ismail2005classical}
Mourad Ismail.
\newblock \emph{Classical and quantum orthogonal polynomials in one variable},
  volume~13.
\newblock Cambridge university press, 2005.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2018.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2018deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2018.

\bibitem[Lee et~al.(2020)Lee, Shen, Song, Wang, et~al.]{lee2020generalized}
Jason~D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, et~al.
\newblock Generalized leverage score sampling for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Li \& Liang(2018)Li and Liang]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Liang et~al.(2020)Liang, Rakhlin, and Zhai]{liang2019risk}
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai.
\newblock On the risk of minimum-norm interpolants and restricted lower
  isometry of kernels.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2020.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and
  Ghahramani]{matthews2018gaussian}
Alexander Matthews, Mark Rowland, Jiri Hron, Richard~E Turner, and Zoubin
  Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock \emph{arXiv preprint arXiv:1804.11271}, 2018.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Mhaskar \& Poggio(2016)Mhaskar and Poggio]{mhaskar2016deep}
Hrushikesh~N Mhaskar and Tomaso Poggio.
\newblock Deep vs. shallow networks: An approximation theory perspective.
\newblock \emph{Analysis and Applications}, 14\penalty0 (06):\penalty0
  829--848, 2016.

\bibitem[Minh et~al.(2006)Minh, Niyogi, and Yao]{minh2006mercer}
Ha~Quang Minh, Partha Niyogi, and Yuan Yao.
\newblock Mercerâ€™s theorem, feature maps, and smoothing.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2006.

\bibitem[Neal(1996)]{neal1996bayesian}
Radford~M Neal.
\newblock \emph{Bayesian learning for neural networks}.
\newblock Springer, 1996.

\bibitem[Pinkus(1999)]{pinkus1999approximation}
Allan Pinkus.
\newblock Approximation theory of the mlp model in neural networks.
\newblock \emph{Acta numerica}, 8:\penalty0 143--195, 1999.

\bibitem[Rahimi \& Recht(2007)Rahimi and Recht]{rahimi2007}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2007.

\bibitem[Rudi \& Rosasco(2017)Rudi and Rosasco]{rudi2017generalization}
Alessandro Rudi and Lorenzo Rosasco.
\newblock Generalization properties of learning with random features.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3215--3225, 2017.

\bibitem[Scetbon \& Harchaoui(2020)Scetbon and Harchaoui]{scetbon2020risk}
Meyer Scetbon and Zaid Harchaoui.
\newblock Risk bounds for multi-layer perceptrons through spectra of integral
  operators.
\newblock \emph{arXiv preprint arXiv:2002.12640}, 2020.

\bibitem[Schmidt-Hieber et~al.(2020)]{schmidt2020nonparametric}
Johannes Schmidt-Hieber et~al.
\newblock Nonparametric regression using deep neural networks with relu
  activation function.
\newblock \emph{Annals of Statistics}, 48\penalty0 (4):\penalty0 1875--1897,
  2020.

\bibitem[Smola et~al.(2001)Smola, Ovari, and
  Williamson]{smola2001regularization}
Alex~J Smola, Zoltan~L Ovari, and Robert~C Williamson.
\newblock Regularization with dot-product kernels.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2001.

\bibitem[Song \& Yang(2019)Song and Yang]{song2019quadratic}
Zhao Song and Xin Yang.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound.
\newblock \emph{arXiv preprint arXiv:1906.03593}, 2019.

\bibitem[Telgarsky(2016)]{telgarsky2016benefits}
Matus Telgarsky.
\newblock Benefits of depth in neural networks.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2016.

\bibitem[Xie et~al.(2017)Xie, Liang, and Song]{xie2017diverse}
Bo~Xie, Yingyu Liang, and Le~Song.
\newblock Diverse neural network learns true target functions.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2017.

\bibitem[Yang \& Salman(2019)Yang and Salman]{yang2019fine}
Greg Yang and Hadi Salman.
\newblock A fine-grained spectral perspective on neural networks.
\newblock \emph{arXiv preprint arXiv:1907.10599}, 2019.

\bibitem[Yarotsky(2017)]{yarotsky2017error}
Dmitry Yarotsky.
\newblock Error bounds for approximations with deep relu networks.
\newblock \emph{Neural Networks}, 94:\penalty0 103--114, 2017.

\bibitem[Zou et~al.(2019)Zou, Cao, Zhou, and Gu]{zou2019stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock \emph{Machine Learning}, 2019.

\end{thebibliography}
