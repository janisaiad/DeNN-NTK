\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adams and Fournier(2003)]{adams2003_SobolevSpaces}
Robert~A Adams and John~JF Fournier.
\newblock \emph{Sobolev Spaces}.
\newblock {Elsevier}, 2003.

\bibitem[{Allen-Zhu} et~al.(2019{\natexlab{a}}){Allen-Zhu}, Li, and Song]{allen-zhu2019_ConvergenceRate}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Zhao Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019{\natexlab{a}}.

\bibitem[{Allen-Zhu} et~al.(2019{\natexlab{b}}){Allen-Zhu}, Li, and Song]{allen-zhu2019_ConvergenceTheory}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization, June 2019{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1811.03962}.

\bibitem[Andreas~Christmann(2008)]{andreaschristmann2008_SupportVector}
Ingo Steinwart~(auth.) Andreas~Christmann.
\newblock \emph{Support Vector Machines}.
\newblock Information {{Science}} and {{Statistics}}. {Springer-Verlag New York}, {New York, NY}, 1 edition, 2008.
\newblock ISBN 0-387-77242-1 0-387-77241-3 978-0-387-77241-7 978-0-387-77242-4.
\newblock \doi{10.1007/978-0-387-77242-4}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and Wang]{arora2019_FinegrainedAnalysis}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, pages 322--332. {PMLR}, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and Wang]{arora2019_ExactComputation}
Sanjeev Arora, Simon~S. Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~32. {Curran Associates, Inc.}, 2019{\natexlab{b}}.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/dbc4d84bfcfe2284ba11beffb853a8c4-Abstract.html}.

\bibitem[Azevedo and Menegatto(2014)]{azevedo2014_SharpEstimates}
D.~Azevedo and V.A. Menegatto.
\newblock Sharp estimates for eigenvalues of integral operators generated by dot product kernels on the sphere.
\newblock \emph{Journal of Approximation Theory}, 177:\penalty0 57--68, January 2014.
\newblock ISSN 00219045.
\newblock \doi{10.1016/j.jat.2013.10.002}.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and Tsigler]{bartlett2020_BenignOverfittinga}
Peter~L. Bartlett, Philip~M. Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0 (48):\penalty0 30063--30070, 2020.

\bibitem[Bauer et~al.(2007)Bauer, Pereverzyev, and Rosasco]{bauer2007_RegularizationAlgorithms}
F.~Bauer, S.~Pereverzyev, and L.~Rosasco.
\newblock On regularization algorithms in learning theory.
\newblock \emph{Journal of complexity}, 23\penalty0 (1):\penalty0 52--72, 2007.
\newblock \doi{10.1016/j.jco.2006.07.001}.

\bibitem[Beaglehole et~al.(2022)Beaglehole, Belkin, and Pandit]{beaglehole2022_KernelRidgeless}
Daniel Beaglehole, Mikhail Belkin, and Parthe Pandit.
\newblock Kernel ridgeless regression is inconsistent in low dimensions, June 2022.

\bibitem[Bietti and Bach(2020)]{bietti2020_DeepEquals}
Alberto Bietti and Francis Bach.
\newblock Deep equals shallow for {{ReLU}} networks in kernel regimes.
\newblock \emph{arXiv preprint arXiv:2009.14397}, 2020.

\bibitem[Bietti and Mairal(2019)]{bietti2019_InductiveBias}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~32, 2019.

\bibitem[Caponnetto and Yao(2010)]{caponnetto2010_CrossvalidationBased}
A.~Caponnetto and Y.~Yao.
\newblock Cross-validation based adaptation for regularization operators in learning theory.
\newblock \emph{Analysis and Applications}, 08:\penalty0 161--183, 2010.
\newblock \doi{10.1142/S0219530510001564}.

\bibitem[Caponnetto and De~Vito(2007)]{caponnetto2007_OptimalRates}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0 (3):\penalty0 331--368, 2007.
\newblock \doi{10.1007/s10208-006-0196-8}.

\bibitem[Chen and Xu(2020)]{chen2020_DeepNeural}
Lin Chen and Sheng Xu.
\newblock Deep neural tangent kernel and laplace kernel have the same {{RKHS}}.
\newblock \emph{arXiv preprint arXiv:2009.10683}, 2020.

\bibitem[Cho and Saul(2009)]{cho2009_KernelMethods}
Youngmin Cho and Lawrence Saul.
\newblock Kernel methods for deep learning.
\newblock In Y.~Bengio, D.~Schuurmans, J.~Lafferty, C.~Williams, and A.~Culotta, editors, \emph{Advances in Neural Information Processing Systems}, volume~22. {Curran Associates, Inc.}, 2009.
\newblock URL \url{https://proceedings.neurips.cc/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf}.

\bibitem[Dai and Xu(2013)]{dai2013_ApproximationTheory}
Feng Dai and Yuan Xu.
\newblock \emph{Approximation Theory and Harmonic Analysis on Spheres and Balls}.
\newblock Springer {{Monographs}} in {{Mathematics}}. {Springer New York}, {New York, NY}, 2013.
\newblock ISBN 978-1-4614-6659-8 978-1-4614-6660-4.
\newblock \doi{10.1007/978-1-4614-6660-4}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019_BERTPretraining}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {{BERT}}: {{Pre-training}} of deep bidirectional transformers for language understanding, May 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018_GradientDescent}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural networks.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, September 2018.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019_GradientDescent}
Simon~S. Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{Proceedings of the 36th {{International Conference}} on {{Machine Learning}}}, pages 1675--1685. {PMLR}, May 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/du19c.html}.

\bibitem[Fan and Wang(2020)]{fan2020_SpectraConjugate}
Zhou Fan and Zhichao Wang.
\newblock Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 7710--7721, 2020.

\bibitem[Fischer and Steinwart(2020)]{fischer2020_SobolevNorm}
Simon-Raphael Fischer and Ingo Steinwart.
\newblock Sobolev norm learning rates for regularized least-squares algorithms.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 205:1--205:38, 2020.
\newblock URL \url{https://www.semanticscholar.org/paper/248fb62f75dac19f02f683cecc2bf4929f3fcf6d}.

\bibitem[Frei et~al.(2022)Frei, Chatterji, and Bartlett]{frei2022_BenignOverfitting}
Spencer Frei, Niladri~S. Chatterji, and Peter Bartlett.
\newblock Benign overfitting without linearity: {{Neural}} network classifiers trained by gradient descent for noisy linear data.
\newblock In \emph{Proceedings of {{Thirty Fifth Conference}} on {{Learning Theory}}}, pages 2668--2703. {PMLR}, June 2022.
\newblock URL \url{https://proceedings.mlr.press/v178/frei22a.html}.

\bibitem[Geifman et~al.(2020)Geifman, Yadav, Kasten, Galun, Jacobs, and Ronen]{geifman2020_SimilarityLaplace}
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen.
\newblock On the similarity between the {{Laplace}} and neural tangent kernels.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~33, pages 1451--1461, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition}, pages 770--778, 2016.

\bibitem[Hu et~al.(2021)Hu, Wang, Lin, and Cheng]{hu2021_RegularizationMatters}
Tianyang Hu, Wenjia Wang, Cong Lin, and Guang Cheng.
\newblock Regularization matters: {{A}} nonparametric perspective on overparametrized neural network.
\newblock In \emph{International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, pages 829--837. {PMLR}, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018_NeuralTangent}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: {{Convergence}} and generalization in neural networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~{Cesa-Bianchi}, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~31. {Curran Associates, Inc.}, 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019_StylebasedGenerator}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial networks.
\newblock In \emph{Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition}, pages 4401--4410, 2019.

\bibitem[Krizhevsky et~al.(2017)Krizhevsky, Sutskever, and Hinton]{krizhevsky2017_ImagenetClassification}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Communications of the ACM}, 60\penalty0 (6):\penalty0 84--90, 2017.

\bibitem[Lai et~al.(2023)Lai, Xu, Chen, and Lin]{lai2023_GeneralizationAbility}
Jianfa Lai, Manyun Xu, Rui Chen, and Qian Lin.
\newblock Generalization ability of wide neural networks on {{R}}, February 2023.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, {Sohl-Dickstein}, and Pennington]{lee2019_WideNeural}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha {Sohl-Dickstein}, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under gradient descent.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~32. {Curran Associates, Inc.}, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/0d1a9651497a38d8b1c3871c84528bd4-Abstract.html}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, and Lin]{li2023_KernelInterpolation}
Yicheng Li, Haobo Zhang, and Qian Lin.
\newblock Kernel interpolation generalizes poorly.
\newblock \emph{Biometrika}, page asad048, August 2023{\natexlab{a}}.
\newblock ISSN 0006-3444, 1464-3510.
\newblock \doi{10.1093/biomet/asad048}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Zhang, and Lin]{li2023_SaturationEffect}
Yicheng Li, Haobo Zhang, and Qian Lin.
\newblock On the saturation effect of kernel ridge regression.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, February 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=tFvr-kYWs_Y}.

\bibitem[Li and Liang(2018)]{li2018_LearningOverparameterized}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient descent on structured data.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Liang and Rakhlin(2020)]{liang2020_JustInterpolate}
Tengyuan Liang and Alexander Rakhlin.
\newblock Just interpolate: {{Kernel}} "ridgeless" regression can generalize.
\newblock \emph{The Annals of Statistics}, 48\penalty0 (3), June 2020.
\newblock ISSN 0090-5364.
\newblock \doi{10.1214/19-AOS1849}.

\bibitem[Lin et~al.(2018)Lin, Rudi, Rosasco, and Cevher]{lin2018_OptimalRates}
Junhong Lin, Alessandro Rudi, L.~Rosasco, and V.~Cevher.
\newblock Optimal rates for spectral algorithms with least-squares regression over {{Hilbert}} spaces.
\newblock \emph{Applied and Computational Harmonic Analysis}, 48:\penalty0 868--890, 2018.
\newblock \doi{10.1016/j.acha.2018.09.009}.

\bibitem[Montanari and Zhong(2022)]{montanari2022_InterpolationPhase}
Andrea Montanari and Yiqiao Zhong.
\newblock The interpolation phase transition in neural networks: {{Memorization}} and generalization under lazy training.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (5):\penalty0 2816--2847, 2022.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Bansal, Yang, Barak, and Sutskever]{nakkiran2019_DeepDouble}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
\newblock Deep double descent: {{Where}} bigger models and more data hurt.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, September 2019.
\newblock URL \url{https://openreview.net/forum?id=B1g5sA4twr}.

\bibitem[Nguyen et~al.(2021)Nguyen, Mondelli, and Montufar]{nguyen2021_TightBounds}
Quynh Nguyen, Marco Mondelli, and Guido~F. Montufar.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep {{ReLU}} networks.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, pages 8119--8129. {PMLR}, 2021.

\bibitem[Rakhlin and Zhai(2018)]{rakhlin2018_ConsistencyInterpolation}
Alexander Rakhlin and Xiyu Zhai.
\newblock Consistency of interpolation with {{Laplace}} kernels is a high-dimensional phenomenon, December 2018.
\newblock URL \url{http://arxiv.org/abs/1812.11167}.

\bibitem[Ronen et~al.(2019)Ronen, Jacobs, Kasten, and Kritchman]{ronen2019_ConvergenceRate}
Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman.
\newblock The convergence rate of neural networks for learned functions of different frequencies.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Simon(2015)]{simon2015_OperatorTheory}
Barry Simon.
\newblock \emph{Operator Theory}.
\newblock {American Mathematical Society}, {Providence, Rhode Island}, November 2015.
\newblock ISBN 978-1-4704-1103-9 978-1-4704-2763-4.
\newblock \doi{10.1090/simon/004}.

\bibitem[Steinwart and Scovel(2012)]{steinwart2012_MercerTheorem}
Ingo Steinwart and C.~Scovel.
\newblock Mercer's theorem on general domains: {{On}} the interaction between measures, kernels, and {{RKHSs}}.
\newblock \emph{Constructive Approximation}, 35\penalty0 (3):\penalty0 363--417, 2012.
\newblock \doi{10.1007/S00365-012-9153-3}.

\bibitem[Suh et~al.(2022)Suh, Ko, and Huo]{suh2022_NonparametricRegression}
Namjoon Suh, Hyunouk Ko, and Xiaoming Huo.
\newblock A non-parametric regression viewpoint: {{Generalization}} of overparametrized deep {{ReLU}} network under noisy observations.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, May 2022.
\newblock URL \url{https://openreview.net/forum?id=bZJbzaj_IlP}.

\bibitem[Vershynin(2010)]{vershynin2010_IntroductionNonasymptotic}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Vershynin(2018)]{vershynin2018_HighdimensionalProbability}
Roman Vershynin.
\newblock \emph{High-Dimensional Probability: {{An}} Introduction with Applications in Data Science}, volume~47.
\newblock {Cambridge university press}, 2018.
\newblock ISBN 1-108-24454-8.

\bibitem[Weyl(1939)]{weyl1939_VolumeTubes}
Hermann Weyl.
\newblock On the volume of tubes.
\newblock \emph{American Journal of Mathematics}, 61\penalty0 (2):\penalty0 461--472, 1939.

\bibitem[Widom(1963)]{widom1963_AsymptoticBehavior}
Harold Widom.
\newblock Asymptotic behavior of the eigenvalues of certain integral equations.
\newblock \emph{Transactions of the American Mathematical Society}, 109\penalty0 (2):\penalty0 278--295, 1963.
\newblock ISSN 0002-9947.
\newblock \doi{10.2307/1993907}.

\end{thebibliography}
