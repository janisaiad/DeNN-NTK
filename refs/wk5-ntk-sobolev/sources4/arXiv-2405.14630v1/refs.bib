
@InProceedings{pmlr-v97-arora19a,
  title = 	 {Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author =       {Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {322--332},
  year = 	 {2019},
  noeditor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/arora19a/arora19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/arora19a.html},
  abstract = 	 {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR’17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.}
}

@inproceedings{NEURIPS2018_54fe976b,
 author = {Li, Yuanzhi and Liang, Yingyu},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{bowman2022spectral,
title={Spectral Bias Outside the Training Set for Deep Networks in the Kernel Regime},
author={Benjamin Bowman and Guido Mont\'ufar},
booktitle={Advances in Neural Information Processing Systems},
noeditor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=a01PL2gb7W5}
}

@InProceedings{pmlr-v216-banerjee23a,
  title = 	 {Neural tangent kernel at initialization: linear width suffices},
  author = {Banerjee, Arindam and Cisneros-Velarde, Pedro and Zhu, Libin and Belkin, Mikhail},
  booktitle = {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {110--118},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/banerjee23a/banerjee23a.pdf},
  url = 	 {https://proceedings.mlr.press/v216/banerjee23a.html},
  abstract = 	 {In this paper we study the problem of lower bounding the minimum eigenvalue of the neural tangent kernel (NTK) at initialization, an important quantity for the theoretical analysis of training in neural networks. We consider feedforward neural networks with smooth activation functions. Without any distributional assumptions on the input, we present a novel result: we show that for suitable initialization variance, $\widetilde{\Omega}(n)$ width, where $n$ is the number of training samples, suffices to ensure that the NTK at initialization is positive definite, improving prior results for smooth activations under our setting. Prior to our work, the sufficiency of linear width has only been shown either for networks with ReLU activation functions, and sublinear width has been shown for smooth networks but with additional conditions on the distribution of the data. The technical challenge in the analysis stems from the layerwise inhomogeneity of smooth activation functions and we handle the challenge using {\em generalized} Hermite series expansion of such activations.}
}

@book{zhang11,
  added-at = {2017-02-13T08:20:55.000+0100},
  address = {New York, NY},
  author = {Zhang, Fuzhen},
  biburl = {https://www.bibsonomy.org/bibtex/251f01e25233b7192e9c950b9dd62a838/ytyoun},
  description = {Matrix Theory - Basic Results and Techniques | Fuzhen Zhang | Springer},
  doi = {10.1007/978-1-4614-1099-7},
  edition = {Second},
  interhash = {5adade2ce18d54618e29500be7b73951},
  intrahash = {51f01e25233b7192e9c950b9dd62a838},
  isbn = {9781461410997 1461410991 9781461410980 1461410983},
  keywords = {eigenvalues majorization matrix textbook},
  publisher = {Springer},
  refid = {929286773},
  timestamp = {2017-02-13T08:21:58.000+0100},
  title = {Matrix Theory: Basic Results and Techniques},
  year = 2011
}

@article{Weyl1912,
author = {Weyl, H.},
journal = {Mathematische Annalen},
pages = {441-479},
title = {Das asymptotische {V}erteilungsgesetz der {E}igenwerte linearer partieller {D}ifferentialgleichungen (mit einer {A}nwendung auf die {T}heorie der {H}ohlraumstrahlung)},
url = {http://eudml.org/doc/158545},
volume = {71},
year = {1912},
}

@article{THOMPSON1975251,
title = {Singular value inequalities for matrix sums and minors},
journal = {Linear Algebra and its Applications},
volume = {11},
number = {3},
pages = {251-269},
year = {1975},
noissn = {0024-3795},
nodoi = {https://doi.org/10.1016/0024-3795(75)90025-7},
url = {https://www.sciencedirect.com/science/article/pii/0024379575900257},
author = {R.C. Thompson},
abstract = {This paper gives new proofs for certain inequalities previously established by the author involving sums of singular values of matrices A, B, C = A + B, and also sums of singular values of A, B, and C when A, B are complementary submatrices of C. Some new facts concerning these inequalities are also included.}
}

@article{THOMPSON1971369,
title = {On the eigenvalues of sums of Hermitian matrices},
journal = {Linear Algebra and its Applications},
volume = {4},
number = {4},
pages = {369-376},
year = {1971},
noissn = {0024-3795},
nodoi = {https://doi.org/10.1016/0024-3795(71)90007-3},
url = {https://www.sciencedirect.com/science/article/pii/0024379571900073},
author = {Robert C. Thompson and Linda J. Freede}
}

@article{karhadkar2023mildly,
      title={Mildly Overparameterized {ReLU} Networks Have a Favorable Loss Landscape}, 
      author={Kedar Karhadkar and Michael Murray and Hanna Tseran and Guido Mont\'ufar},
      year={2023},
      journal={arXiv:2305.19510},
}

@inproceedings{vardi2022on,
title={On the Optimal Memorization Power of Re{LU} Neural Networks},
author={Gal Vardi and Gilad Yehudai and Ohad Shamir},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=MkTPtnjeYTV}
}

@inproceedings{song2021subquadratic,
title={Subquadratic Overparameterization for Shallow Neural Networks},
author={Chaehwan Song and Ali Ramezani-Kebrya and Thomas Pethick and Armin Eftekhari and Volkan Cevher},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=NhbFhfM960}
}

@ARTICLE{Oymak2019TowardMO,
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title={Toward Moderate Overparameterization: Global Convergence Guarantees for Training Shallow Neural Networks}, 
  year={2020},
  volume={1},
  number={1},
  pages={84-105},
  keywords={Training;Training data;Stochastic processes;Biological neural networks;Information theory;Convergence;Neural network training;overparameterization;nonconvex optimization;random matrix theory},
  url={https://doi.org/10.1109/JSAIT.2020.2991332}}


@article{doi:10.1137/21M1413699,
author = {Mont\'{u}far, Guido and Ren, Yue and Zhang, Leon},
title = {Sharp Bounds for the Number of Regions of Maxout Networks and Vertices of {M}inkowski Sums},
journal = {SIAM Journal on Applied Algebra and Geometry},
volume = {6},
number = {4},
pages = {618-649},
year = {2022},
nodoi = {10.1137/21M1413699},
URL = {https://doi.org/10.1137/21M1413699},
eprint = {https://doi.org/10.1137/21M1413699},
    abstract = { Abstract. We present results on the number of linear regions of the functions that can be represented by artificial feedforward neural networks with maxout units. A rank- \(k\) maxout unit is a function computing the maximum of \(k\) linear functions. For networks with a single layer of maxout units, the linear regions correspond to the upper vertices of a Minkowski sum of polytopes. We obtain face counting formulas in terms of the intersection posets of tropical hypersurfaces or the number of upper faces of partial Minkowski sums, along with explicit sharp upper bounds for the number of regions for any input dimension, any number of units, and any ranks, in cases with and without biases. Based on these results we also obtain asymptotically sharp upper bounds for networks with multiple layers.}
    }


@INPROCEEDINGS{Stanley04anintroduction,
    author = {Richard P. Stanley},
    title = {An Introduction to Hyperplane Arrangements},
    booktitle = {Lecture notes, IAS/Park City Mathematics Institute},
    year = {2004}
}

@book{ETC,
  author = {Joswig, Michael},
  title = {Essentials of tropical combinatorics},
  publisher = {American Mathematical Society},
  address = {Providence, RI},
  series = {Graduate Studies in Mathematics},
  volume = {219},
  year = {2021},
  url = {https://page.math.tu-berlin.de/~joswig/etc/index.html}
}

@InProceedings{zhang2018tropical,
  title = 	 {Tropical Geometry of Deep Neural Networks},
  author = 	 {Zhang, Liwen and Naitzat, Gregory and Lim, Lek-Heng},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5824--5832},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/zhang18i/zhang18i.pdf},
  url = 	 {http://proceedings.mlr.press/v80/zhang18i.html},
  abstract = 	 {We establish, for the first time, explicit connections between feedforward neural networks with ReLU activation and tropical geometry — we show that the family of such neural networks is equivalent to the family of tropical rational maps. Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotopes, which serve as building blocks for deeper networks; we relate decision boundaries of such neural networks to tropical hypersurfaces, a major object of study in tropical geometry; and we prove that linear regions of such neural networks correspond to vertices of polytopes associated with tropical rational functions. An insight from our tropical formulation is that a deeper network is exponentially more expressive than a shallow network.}
}

@inproceedings{NEURIPS2018_5a4be1fa,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{LIU2021106923,
title = {Understanding the loss landscape of one-hidden-layer ReLU networks},
journal = {Knowledge-Based Systems},
volume = {220},
pages = {106923},
year = {2021},
noissn = {0950-7051},
nodoi = {https://doi.org/10.1016/j.knosys.2021.106923},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001866},
author = {Bo Liu},
keywords = {Deep learning theory, ReLU networks, Loss landscape, Local minima, Saddle points},
abstract = {In this paper, it is proved that for one-hidden-layer ReLU networks all differentiable local minima are global inside each differentiable region. Necessary and sufficient conditions for the existences of differentiable local minima, saddle points and non-differentiable local minima are given, as well as their locations if they do exist. Building upon the theory, a linear programming based algorithm is designed to judge the existence of differentiable local minima, and is used to predict whether spurious local minima exist for the MNIST and CIFAR-10 datasets. Experimental results show that there are no spurious local minima for most typical weight vectors. These theoretical predictions are verified by demonstrating the consistency between them and the results of gradient descent search.}
}

@book{anthony_bartlett_1999, place={Cambridge}, 
title={Neural Network Learning: Theoretical Foundations}, 
noDOI={10.1017/CBO9780511624216}, 
publisher={Cambridge University Press}, 
author={Anthony, Martin and Bartlett, Peter L.}, 
year={1999},
url={https://doi.org/10.1017/CBO9780511624216}
}

@inproceedings{NIPS1998_f18a6d1c,
 author = {Sakurai, Akito},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 pages = {},
 publisher = {MIT Press},
 title = {Tight Bounds for the {VC}-Dimension of Piecewise Polynomial Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f18a6d1cde4b205199de8729a6637b42-Paper.pdf},
 volume = {11},
 year = {1998}
}



@inproceedings{pascanu2014number,
      title={On the number of response regions of deep feed forward networks with piece-wise linear activations}, 
      author={Razvan Pascanu and Guido Mont\'ufar and Yoshua Bengio},
      booktitle ={International Conference on Learning Representations}, 
      year={2014},
      url={https://openreview.net/forum?id=bSaT4mmQt84Lx}, 
}



@inproceedings{NEURIPS2021_04a1bf2d,
 author = {Loukas, Andreas and Poiitis, Marinos and Jegelka, Stefanie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {494--508},
 publisher = {Curran Associates, Inc.},
 title = {What training reveals about neural network complexity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/04a1bf2d968f1ce381cf1f9184a807a9-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{xu2022traversing, 
title={Traversing the Local Polytopes of {ReLU} Neural Networks},
author={Shaojie Xu and Joel Vaughan and Jie Chen and Aijun Zhang and Agus Sudjianto},
booktitle={The AAAI-22 Workshop on Adversarial Machine Learning and Beyond},
year={2022},
url={https://openreview.net/forum?id=EQjwT2-Vaba}
}

@misc{xu2022traversing-OLD,
      title={Traversing the Local Polytopes of ReLU Neural Networks: A Unified Approach for Network Verification}, 
      author={Shaojie Xu and Joel Vaughan and Jie Chen and Aijun Zhang and Agus Sudjianto},
      year={2022},
      eprint={2111.08922},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2023learning,
      title={Learning Narrow One-Hidden-Layer {ReLU} Networks}, 
      author={Sitan Chen and Zehao Dou and Surbhi Goel and Adam R Klivans and Raghu Meka},
      year={2023},
      eprint={2304.10524},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{gao2022a,
title={A global convergence theory for deep Re{LU} implicit networks via over-parameterization},
author={Tianxiang Gao and Hailiang Liu and Jia Liu and Hridesh Rajan and Hongyang Gao},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R332S76RjxS}
}



@inproceedings{NEURIPS2019_dbea3d0e,
 author = {Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Small {ReLU} networks are powerful memorizers: a tight analysis of memorization capacity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/dbea3d0e2a17c170c412c74273778159-Paper.pdf},
 volume = {32},
 year = {2019}
}

@Inbook{Schläfli1950,
    author="Schl{\"a}fli, Ludwig",
    title="Theorie der vielfachen Kontinuit{\"a}t",
    bookTitle="Gesammelte Mathematische Abhandlungen: Band I",
    year="1950",
    publisher="Springer Basel",
    address="Basel",
    pages="167--387",
    abstract="Die Abhandlung, die ich hier der Kaiserlichen Akademie der Wissenschaften vorzulegen die Ehre habe, enth{\"a}lt einen Versuch, einen neuen Zweig der Analysis zu begr{\"u}nden und zu bearbeiten, welcher, gleichsam eine analytische Geometrie von n Dimensionen, diejenigen der Ebene und des Raumes als spezielle F{\"a}lle f{\"u}r n = 2, 3 in sich enthielte. Ich nenne denselben Theorie der vielfachen Kontinuit{\"a}t {\"u}berhaupt in demselben Sinne, wie man zum Beispiel die Geometrie des Raumes eine Theorie der dreifachen Kontinuit{\"a}t nennen kann. Wie in dieser eine Gruppe von Werten der drei Koordinaten einen Punkt bestimmt, so soll in jener eine Gruppe gegebener Werte der n Variabeln x, y, {\ldots} eine L{\"o}sung bestimmen. Ich gebrauche diesen Ausdruck, weil man bei einer oder mehreren Gleichungen mit vielen Variabeln jede gen{\"u}gende Gruppe von Werten auch so nennt; das Ungew{\"o}hnliche der Benennung liegt nur darin, da{\ss} ich sie auch noch beibehalte, wenn gar keine Gleichung zwischen den Variabeln gegeben ist. In diesem Falle nenne ich die Gesamtheit aller L{\"o}sungen die n-fache Totalit{\"a}t; sind hingegen 1, 2, 3, {\ldots} Gleichungen gegeben, so hei{\ss}t bzw. die Gesamtheit ihrer L{\"o}sungen (n − 1)-faches, (n − 2)-faches, (n − 3)-faches, {\ldots} Kontinuum. Aus der Vorstellung der allseitigen Kontinuit{\"a}t der in einer Totalit{\"a}t enthaltenen L{\"o}sungen entwickelt sich diejenige der Unabh{\"a}ngigkeit ihrer gegenseitigen Lage von dem System der gebrauchten Variabeln, insofern durch Transformation neue Variabeln an ihre Stelle treten k{\"o}nnen.",
    noisbn="978-3-0348-4118-4",
    nodoi="10.1007/978-3-0348-4118-4_13",
    url="https://doi.org/10.1007/978-3-0348-4118-4_13"
}


@phdthesis{Cover1964, 
    author={Thomas M. Cover},  
    title = {Geometrical and Statistical Properties of Linear Threshold Devices},  
    school = {Stanford Electronics Laboratories Technical Report \#6107-1}, 
    year ={1964}, 
    month = {May}, 
    url={https://isl.stanford.edu/~cover/papers/paper1.pdf}
}

@ARTICLE{4038449,
  author={Cover, Thomas M.},
  journal={IEEE Transactions on Electronic Computers}, 
  title={Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition}, 
  year={1965},
  volume={EC-14},
  number={3},
  pages={326-334},
  nodoi={10.1109/PGEC.1965.264137}
  }



@article{noteson,
      title={Notes on the number of linear regions of deep neural networks}, 
      author={Guido Mont\'ufar},
      year={2017},
      journal={},
      url={https://www.researchgate.net/publication/322539221_Notes_on_the_number_of_linear_regions_of_deep_neural_networks}
}

@InProceedings{pmlr-v80-serra18b,
  title = 	 {Bounding and Counting Linear Regions of Deep Neural Networks},
  author =       {Serra, Thiago and Tjandraatmadja, Christian and Ramalingam, Srikumar},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4558--4566},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/serra18b/serra18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/serra18b.html},
  abstract = 	 {We investigate the complexity of deep neural networks (DNN) that represent piecewise linear (PWL) functions. In particular, we study the number of linear regions, i.e. pieces, that a PWL function represented by a DNN can attain, both theoretically and empirically. We present (i) tighter upper and lower bounds for the maximum number of linear regions on rectifier networks, which are exact for inputs of dimension one; (ii) a first upper bound for multi-layer maxout networks; and (iii) a first method to perform exact enumeration or counting of the number of regions by modeling the DNN with a mixed-integer linear formulation. These bounds come from leveraging the dimension of the space defining each linear region. The results also indicate that a deep rectifier network can only have more linear regions than every shallow counterpart with same number of neurons if that number exceeds the dimension of the input.}
}

@article{hinz2021using,
      title={Using activation histograms to bound the number of affine regions in ReLU feed-forward neural networks}, 
      author={Peter Hinz},
      year={2021},
      journal={arXiv preprint arXiv:2103.17174},
}

@inproceedings{NEURIPS2019_9766527f,
 author = {Hanin, Boris and Rolnick, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep ReLU Networks Have Surprisingly Few Activation Patterns},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NEURIPS2021_f2c3b258,
 author = {Tseran, Hanna and Mont\'ufar, Guido},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28995--29008},
 publisher = {Curran Associates, Inc.},
 title = {On the Expected Complexity of Maxout Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/f2c3b258e9cd8ba16e18f319b3c88c66-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{NIPS2014_109d2dd3,
 author = {Mont\'ufar, Guido and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the Number of Linear Regions of Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/109d2dd3608f669ca17920c511c2a41e-Paper.pdf},
 volume = {27},
 year = {2014}
}



@article{doi:10.1137/140957081,
author = {Mont\'{u}far, Guido F. and Morton, Jason},
title = {When Does a Mixture of Products Contain a Product of Mixtures?},
journal = {SIAM Journal on Discrete Mathematics},
volume = {29},
number = {1},
pages = {321-347},
year = {2015},
nodoi = {10.1137/140957081},
URL = {https://doi.org/10.1137/140957081},
eprint = {https://doi.org/10.1137/140957081},
    abstract = { We derive relations between theoretical properties of restricted Boltzmann machines (RBMs), popular machine learning models which form the building blocks of deep learning models, and several natural notions from discrete mathematics and convex geometry. We give implications and equivalences relating RBM-representable probability distributions, perfectly reconstructible inputs, Hamming modes, zonotopes and zonosets, point configurations in hyperplane arrangements, linear threshold codes, and multicovering numbers of hypercubes. As a motivating application, we prove results on the relative representational power of mixtures of product distributions and products of mixtures of pairs of product distributions (RBMs) that formally justify widely held intuitions about distributed representations. In particular, we show that a mixture of products requiring an exponentially larger number of parameters is needed to represent the probability distributions which can be obtained as products of mixtures. }
}


@book{orientedmatroids, 
place={Cambridge}, 
edition={2}, 
series={Encyclopedia of Mathematics and its Applications}, 
title={Oriented Matroids}, 
noDOI={10.1017/CBO9780511586507}, 
publisher={Cambridge University Press}, 
author={Björner, Anders and Las Vergnas, Michel and Sturmfels, Bernd and White, Neil and Ziegler, Gunter M.}, 
year={1999}, 
collection={Encyclopedia of Mathematics and its Applications},
url={https://www.cambridge.org/core/books/oriented-matroids/A34966F40E168883C68362886EF5D334}
}

@book{zaslavsky1975facing,
  title={Facing up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes},
  author={Zaslavsky, Thomas},
  noisbn={9780821818541},
  nolccn={lc75015678},
  series={American Mathematical Society: Memoirs of the American Mathematical Society},
  url={https://books.google.com/books?id=K-nTCQAAQBAJ},
  year={1975},
  publisher={American Mathematical Society}
}

@ARTICLE{6796044,
  author={Chen, An Mei and Lu, Haw-minn and Hecht-Nielsen, Robert},
  journal={Neural Computation}, 
  title={On the Geometry of Feedforward Neural Network Error Surfaces}, 
  year={1993},
  volume={5},
  number={6},
  pages={910-927},
  nodoi={10.1162/neco.1993.5.6.910},
  url={https://ieeexplore.ieee.org/document/6796044}
  }

@article{FUKUMIZU2000317,
title = {Local minima and plateaus in hierarchical structures of multilayer perceptrons},
journal = {Neural Networks},
volume = {13},
number = {3},
pages = {317-327},
year = {2000},
noissn = {0893-6080},
nodoi = {https://doi.org/10.1016/S0893-6080(00)00009-5},
url = {https://www.sciencedirect.com/science/article/pii/S0893608000000095},
author = {Fukumizu, Kenji and Amari, {Shun-ichi}},
keywords = {Multilayer perceptron, Local minima, Plateau, Error surface},
abstract = {Local minima and plateaus pose a serious problem in learning of neural networks. We investigate the hierarchical geometric structure of the parameter space of three-layer perceptrons in order to show the existence of local minima and plateaus. It is proved that a critical point of the model with H−1 hidden units always gives many critical points of the model with H hidden units. These critical points consist of many lines in the parameter space, which can cause plateaus in learning of neural networks. Based on this result, we prove that a point in the critical lines corresponding to the global minimum of the smaller model can be a local minimum or a saddle point of the larger model. We give a necessary and sufficient condition for this, and show that this kind of local minima exist as a line segment if any. The results are universal in the sense that they do not require special properties of the target, loss functions and activation functions, but only use the hierarchical structure of the model.}
}

@InProceedings{pmlr-v97-nguyen19a,
  title = 	 {On Connected Sublevel Sets in Deep Learning},
  author =       {Nguyen, Quynh},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4790--4799},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/nguyen19a/nguyen19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/nguyen19a.html},
  abstract = 	 {This paper shows that every sublevel set of the loss function of a class of deep over-parameterized neural nets with piecewise linear activation functions is connected and unbounded. This implies that the loss has no bad local valleys and all of its global minima are connected within a unique and potentially very large global valley.}
}

@InProceedings{Shen_2018_CVPR,
author = {Shen, Hao},
title = {Towards a Mathematical Understanding of the Difficulty in Learning With Feedforward Neural Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018},
url ={https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Towards_a_Mathematical_CVPR_2018_paper.pdf}
}

@inproceedings{NIPS2016_f2fc9902,
 author = {Kawaguchi, Kenji},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Learning without Poor Local Minima},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf},
 volume = {29},
 year = {2016}
}

@article{KAWAGUCHI2019167,
title = {Depth with nonlinearity creates no bad local minima in ResNets},
journal = {Neural Networks},
volume = {118},
pages = {167-174},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301820},
author = {Kenji Kawaguchi and Yoshua Bengio},
keywords = {Deep learning, Residual neural network, Non-convex optimization, Local minima},
abstract = {In this paper, we prove that depth with nonlinearity creates no bad local minima in a type of arbitrarily deep ResNets with arbitrary nonlinear activation functions, in the sense that the values of all local minima are no worse than the global minimum value of corresponding classical machine-learning models, and are guaranteed to further improve via residual representations. As a result, this paper provides an affirmative answer to an open question stated in a paper in the conference on Neural Information Processing Systems 2018. This paper advances the optimization theory of deep learning only for ResNets and not for other network architectures.}
}

@InProceedings{pmlr-v70-zhong17a,
  title = 	 {Recovery Guarantees for One-hidden-layer Neural Networks},
  author =       {Kai Zhong and Zhao Song and Prateek Jain and Peter L. Bartlett and Inderjit S. Dhillon},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {4140--4149},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/zhong17a/zhong17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/zhong17a.html},
  abstract = 	 {In this paper, we consider regression problems with one-hidden-layer neural networks (1NNs). We distill some properties of activation functions that lead to   <em>local strong convexity</em> in the neighborhood of the ground-truth parameters for the 1NN squared-loss objective and most popular nonlinear activation functions  satisfy the distilled properties, including rectified linear units (ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation functions that are also smooth, we show <em>local linear convergence</em> guarantees of gradient descent under a resampling rule. For homogeneous activations, we show tensor methods are able to initialize the parameters to fall into the local strong convexity region. As a result, tensor initialization followed by gradient descent is guaranteed to recover the ground truth with sample complexity $ d \cdot \log(1/\epsilon) \cdot \mathrm{poly}(k,\lambda )$ and computational complexity $n\cdot d \cdot \mathrm{poly}(k,\lambda) $ for smooth  homogeneous activations with high probability, where $d$ is the dimension of the input, $k$ ($k\leq d$) is the number of hidden nodes, $\lambda$ is a conditioning  property of the ground-truth parameter matrix between the input layer and the hidden layer, $\epsilon$ is the targeted precision and $n$ is the number of samples. To the best of our knowledge, this is the first work that provides recovery guarantees for 1NNs with both sample complexity and computational complexity <em>linear</em> in the input dimension and <em>logarithmic</em> in the precision.}
}

@ARTICLE{107014,
  author={Gori, Marco and Tesi, Alberto},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={On the problem of local minima in backpropagation}, 
  year={1992},
  volume={14},
  number={1},
  pages={76-86},
  nodoi={10.1109/34.107014},
  url={https://ieeexplore.ieee.org/document/107014}
  }

@INPROCEEDINGS{155333,
  author={Poston, T. and Lee, C.-N. and Choie, Y. and Kwon, Y.},
  booktitle={IJCNN-91-Seattle International Joint Conference on Neural Networks}, 
  title={Local minima and back propagation}, 
  year={1991},
  volume={ii},
  number={},
  pages={173-176 vol.2},
  nodoi={10.1109/IJCNN.1991.155333},
  url={https://ieeexplore.ieee.org/document/155333}
  }

@InProceedings{pmlr-v48-safran16,
  title = 	 {On the Quality of the Initial Basin in Overspecified Neural Networks},
  author = 	 {Safran, Itay and Shamir, Ohad},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {774--782},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/safran16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/safran16.html},
  abstract = 	 {Deep learning, in the form of artificial neural networks, has achieved remarkable practical success in recent years, for a variety of difficult machine learning applications. However, a theoretical explanation for this remains a major open problem, since training neural networks involves optimizing a highly non-convex objective function, and is known to be computationally hard in the worst case. In this work, we study the \emphgeometric structure of the associated non-convex objective function, in the context of ReLU networks and starting from a random initialization of the network parameters. We identify some conditions under which it becomes more favorable to optimization, in the sense of (i) High probability of initializing at a point from which there is a monotonically decreasing path to a global minimum; and (ii) High probability of initializing at a basin (suitably defined) with a small minimal objective value. A common theme in our results is that such properties are more likely to hold for larger (“overspecified”) networks, which accords with some recent empirical and theoretical observations.}
}

@ARTICLE{6795937,
  author={Kůrková, Věra and Kainen, Paul C.},
  journal={Neural Computation}, 
  title={Functionally Equivalent Feedforward Neural Networks}, 
  year={1994},
  volume={6},
  number={3},
  pages={543-558},
  nodoi={10.1162/neco.1994.6.3.543}
  }


@article{kohn2022geometry,
author = {Kohn, Kathl\'{e}n and Merkh, Thomas and Mont\'{u}far, Guido and Trager, Matthew},
title = {Geometry of Linear Convolutional Networks},
journal = {SIAM Journal on Applied Algebra and Geometry},
volume = {6},
number = {3},
pages = {368-406},
year = {2022},
nodoi = {10.1137/21M1441183},
URL = {https://doi.org/10.1137/21M1441183},
eprint = {https://doi.org/10.1137/21M1441183},
    abstract = { We study the family of functions that are represented by a linear convolutional network (LCN). These functions form a semi-algebraic subset of the set of linear maps from input space to output space. In contrast, the families of functions represented by fully connected linear networks form algebraic sets. We observe that the functions represented by LCNs can be identified with polynomials that admit certain factorizations, and we use this perspective to describe the impact of the network's architecture on the geometry of the resulting function space. We further study the optimization of an objective function over an LCN, analyzing critical points in function space and in parameter space and describing dynamical invariants for gradient descent. Overall, our theory predicts that the optimized parameters of an LCN will often correspond to repeated filters across layers, or filters that can be decomposed as repeated filters. We also conduct numerical and symbolic experiments that illustrate our results and present an in-depth analysis of the landscape for small architectures. }
}


@inproceedings{Sharifnassab2020Bounds,
title={Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow {ReLU} Networks},
author={Arsalan Sharifnassab and Saber Salehkaleybar and S. Jamaloddin Golestani},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BkgXHTNtvS}
}


@misc{swirszcz2017local,
title={Local minima in training of deep networks},
author={Grzegorz Swirszcz and Wojciech Marian Czarnecki and Razvan Pascanu},
year={2017},
url={https://openreview.net/forum?id=Syoiqwcxx}
}

@inproceedings{NIPS1995_3806734b,
 author = {Auer, Peter and Herbster, Mark and Warmuth, Manfred K. K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 pages = {},
 publisher = {MIT Press},
 title = {Exponentially many local minima for single neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3806734b256c27e41ec2c6bffa26d9e7-Paper.pdf},
 volume = {8},
 year = {1995}
}

@article{Sontag1989BackpropagationCG,
  title={Backpropagation Can Give Rise to Spurious Local Minima Even for Networks without Hidden Layers},
  author={Eduardo Sontag and H{\'e}ctor J. Sussmann},
  journal={Complex Syst.},
  year={1989},
  volume={3},
  url={https://www.complex-systems.com/abstracts/v03_i01_a07/}
}

@inproceedings{frankle2018the,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}

@article{Petzka2021nonattracting, 
author = {Petzka, Henning and Sminchisescu, Cristian}, 
title = {Non-Attracting Regions of Local Minima in Deep and Wide Neural Networks}, 
year = {2021}, 
issue_date = {January 2021}, 
publisher = {JMLR.org}, 
volume = {22}, 
number = {1}, 
noissn = {1532-4435}, 
abstract = {Understanding the loss surface of neural networks is essential for the design of models with predictable performance and their success in applications. Experimental results suggest that sufficiently deep and wide neural networks are not negatively impacted by suboptimal local minima. Despite recent progress, the reason for this outcome is not fully understood. Could deep networks have very few, if at all, suboptimal local optima? or could all of them be equally good? We provide a construction to show that suboptimal local minima (i.e., non-global ones), even though degenerate, exist for fully connected neural networks with sigmoid activation functions. The local minima obtained by our construction belong to a connected set of local solutions that can be escaped from via a non-increasing path on the loss curve. For extremely wide neural networks of decreasing width after the wide layer, we prove that every suboptimal local minimum belongs to such a connected set. This provides a partial explanation for the successful application of deep neural networks. In addition, we also characterize under what conditions the same construction leads to saddle points instead of local minima for deep neural networks.}, 
journal = {J. Mach. Learn. Res.}, 
month = {jan}, 
articleno = {143}, 
numpages = {34}, 
keywords = {local minima, deep learning, global minima, path, neural network} 
}

@article{grigsby2022functional,
      title={Functional dimension of feedforward {ReLU} neural networks}, 
      author={J. Elisenda Grigsby and Kathryn Lindsey and Robert Meyerhoff and Chenxi Wu},
      year={2022},
      journal={arXiv preprint arXiv:2209.04036},
      archivePrefix={arXiv},
      primaryClass={math.MG}
}


@InProceedings{pmlr-v119-rolnick20a,
  title = 	 {Reverse-engineering deep {R}e{LU} networks},
  author =       {Rolnick, David and Kording, Konrad},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8178--8187},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/rolnick20a/rolnick20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/rolnick20a.html},
  abstract = 	 {The output of a neural network depends on its architecture and weights in a highly nonlinear way, and it is often assumed that a network’s parameters cannot be recovered from its output. Here, we prove that, in fact, it is frequently possible to reconstruct the architecture, weights, and biases of a deep ReLU network by observing only its output. We leverage the fact that every ReLU network defines a piecewise linear function, where the boundaries between linear regions correspond to inputs for which some neuron in the network switches between inactive and active ReLU states. By dissecting the set of region boundaries into components associated with particular neurons, we show both theoretically and empirically that it is possible to recover the weights of neurons and their arrangement within the network, up to isomorphism.}
}

@inproceedings{phuong2020functional,
title={Functional vs. parametric equivalence of {ReLU} networks},
author={Mary Phuong and Christoph H. Lampert},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Bylx-TNKvH}
}

@InProceedings{pmlr-v99-savarese19a,
  title = 	 {How do infinite width bounded norm networks look in function space?},
  author =       {Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {2667--2690},
  year = 	 {2019},
  editor = 	 {Beygelzimer, Alina and Hsu, Daniel},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v99/savarese19a/savarese19a.pdf},
  url = 	 {https://proceedings.mlr.press/v99/savarese19a.html},
  abstract = 	 {We consider the question of what functions can be captured by ReLU networks with an unbounded number of units (infinite width), but where the overall network Euclidean norm (sum of squares of all weights in the system, except for an unregularized bias term for each unit) is bounded; or equivalently what is the minimal norm required to approximate a given function. For functions $f:\mathbb R \rightarrow\mathbb R$ and a single hidden layer, we show that the minimal network norm for representing $f$ is $\max(\int \lvert f”(x) \rvert \mathrm{d} x, \lvert  f’(-\infty) + f’(+\infty) \rvert)$, and hence the minimal norm fit for a sample is given by a linear spline interpolation.  }
}

@inproceedings{Ongie2020A,
title={A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case},
author={Greg Ongie and Rebecca Willett and Daniel Soudry and Nathan Srebro},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1lNPxHKDH}
}

@InProceedings{pmlr-v119-pilanci20a,
  title = 	 {Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-layer Networks},
  author =       {Pilanci, Mert and Ergen, Tolga},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {7695--7705},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/pilanci20a/pilanci20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/pilanci20a.html},
  abstract = 	 {We develop exact representations of training two-layer neural networks with rectified linear units (ReLUs) in terms of a single convex program with number of variables polynomial in the number of training samples and the number of hidden neurons. Our theory utilizes semi-infinite duality and minimum norm regularization. We show that ReLU networks trained with standard weight decay are equivalent to block $\ell_1$ penalized convex models. Moreover, we show that certain standard convolutional linear networks are equivalent semi-definite programs which can be simplified to $\ell_1$ regularized linear models in a polynomial sized discrete Fourier feature space}
}



@inproceedings{dai2021representation,
title={Representation Costs of Linear Neural Networks: Analysis and Design},
author={Zhen Dai and Mina Karzand and Nathan Srebro},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=3oQyjABdbC8}
}

@inproceedings{Lyu2020Gradient,
title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
author={Kaifeng Lyu and Jian Li},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJeLIgBKPS}
}

@inproceedings{Zhang2020Identity,
title={Identity Crisis: Memorization and Generalization Under Extreme Overparameterization},
author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Michael C. Mozer and Yoram Singer},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1l6y0VFPr}
}

@inproceedings{yun2021a,
title={A unifying view on implicit bias in training linear neural networks},
author={Chulhee Yun and Shankar Krishnan and Hossein Mobahi},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=ZsZM-4iMQkH}
}

@InProceedings{pmlr-v178-jagadeesan22a,
  title = 	 {Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm},
  author =       {Jagadeesan, Meena and Razenshteyn, Ilya and Gunasekar, Suriya},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {2276--2325},
  year = 	 {2022},
  editor = 	 {Loh, Po-Ling and Raginsky, Maxim},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--05 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v178/jagadeesan22a/jagadeesan22a.pdf},
  url = 	 {https://proceedings.mlr.press/v178/jagadeesan22a.html},
  abstract = 	 {We provide a function space characterization of the inductive bias resulting from minimizing the $\ell_2$ norm of the weights in multi-channel convolutional neural networks with linear activations and empirically test our resulting hypothesis on ReLU  networks trained using gradient descent. We define an \emph{induced regularizer} in the function space as the minimum $\ell_2$ norm of weights of a network required to realize a function.  For two layer linear convolutional networks with $C$ output channels and kernel size $K$, we show the following: (a) If the inputs to the network are single channeled, the induced regularizer for any $K$ is \emph{independent} of the number of output channels $C$. Furthermore, we derive the regularizer is a norm given by a semidefinite program (SDP). (b) In contrast, for multi-channel inputs, multiple output channels can be necessary to merely realize all matrix-valued linear functions and thus the inductive bias \emph{does} depend on $C$. However, for sufficiently large $C$, the induced regularizer is again given by an SDP that is independent of $C$. In particular, the induced regularizer for  $K=1$ and $K=D$ (input dimension) are given in closed form as the nuclear norm and the $\ell_{2,1}$ group-sparse norm, respectively, of the Fourier coefficients of the linear predictor. We investigate the broader applicability of our theoretical results to implicit regularization from gradient descent on linear and ReLU networks through experiments on MNIST and CIFAR-10 datasets.}
}

@InProceedings{pmlr-v80-laurent18b,
  title = 	 {The Multilinear Structure of {R}e{LU} Networks},
  author =       {Laurent, Thomas and von Brecht, James},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2908--2916},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/laurent18b/laurent18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/laurent18b.html},
  abstract = 	 {We study the loss surface of neural networks equipped with a hinge loss criterion and ReLU or leaky ReLU nonlinearities. Any such network defines a piecewise multilinear form in parameter space. By appealing to harmonic analysis we show that all local minima of such network are non-differentiable, except for those minima that occur in a region of parameter space where the loss surface is perfectly flat. Non-differentiable minima are therefore not technicalities or pathologies; they are heart of the problem when investigating the loss of ReLU networks. As a consequence, we must employ techniques from nonsmooth analysis to study these loss surfaces. We show how to apply these techniques in some illustrative cases.}
}



@misc{levin2022effect,
  doi = {10.48550/ARXIV.2207.03512},
  url = {https://arxiv.org/abs/2207.03512},
  author = {Levin, Eitan and Kileel, Joe and Boumal, Nicolas},
  keywords = {Optimization and Control (math.OC), Numerical Analysis (math.NA), FOS: Mathematics, FOS: Mathematics, 65K10, 49J53, 90C26, 90C46},
  title = {The effect of smooth parametrizations on nonconvex optimization landscapes},
  publisher = {arXiv},
  year = {2022},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{cooper2018loss, 
author = {Cooper, Yaim},
title = {Global Minima of Overparameterized Neural Networks},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {3},
number = {2},
pages = {676-691},
year = {2021},
nodoi = {10.1137/19M1308943},
URL = {https://doi.org/10.1137/19M1308943},
eprint = {https://doi.org/10.1137/19M1308943},
    abstract = { We explore some mathematical features of the loss landscape of overparameterized neural networks. A priori, one might imagine that the loss function looks like a typical function from \$\mathbb{R}^d\$ to \$\mathbb{R}\$, in particular, that it has discrete global minima. In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function. If a neural net has \$d\$ parameters and is trained on \$n\$ data points \$(x\_i, y\_i) \in \mathbb{R}^s \times \mathbb{R}^r\$, with \$d>r n\$, we show that the locus \$M\$ of global minima of \$L\$ is usually not discrete but rather an \$(d- rn)\$-dimensional submanifold of \$\mathbb{R}^d\$. In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that \$M\$ is typically a very-high-dimensional submanifold of \$\mathbb{R}^d\$. }
}

@article{cooper2018loss-OLD,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}



@book{harris2013algebraic,
  title={Algebraic geometry: A first course},
  author={Harris, Joe},
  volume={133},
  year={2013},
  publisher={Springer Science \& Business Media},
  url={https://link.springer.com/book/10.1007/978-1-4757-2189-8}
}

@article{soudry2016no,
  title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
  author={Soudry, Daniel and Carmon, Yair},
  journal={arXiv preprint arXiv:1605.08361},
  year={2016}
}

@article{cooper2020critical,
  title={The critical locus of overparameterized neural networks},
  author={Cooper, Y},
  journal={arXiv preprint arXiv:2005.04210},
  year={2020}
}

@article{kawaguchi2016deep,
  title={Deep learning without poor local minima},
  author={Kawaguchi, Kenji},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{bharadwaj2023complex,
  title={Complex Critical Points of Deep Linear Neural Networks},
  author={Bharadwaj, Ayush and Ho{\c{s}}ten, Serkan},
  journal={arXiv preprint arXiv:2301.12651},
  year={2023}
}

@article{mehta2021loss,
  title={The loss surface of deep linear networks viewed through the algebraic geometry lens},
  author={Mehta, Dhagash and Chen, Tianran and Tang, Tingting and Hauenstein, Jonathan D},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={9},
  pages={5664--5680},
  year={2021},
  publisher={IEEE}
}

@inproceedings{trager2019pure,
title={Pure and Spurious Critical Points: a Geometric Study of Linear Networks},
author={Matthew Trager and Kathlén Kohn and Joan Bruna},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgOlCVYvB}
}

@article{grigsby2022transversality,
  title={On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks},
  author={Grigsby, J Elisenda and Lindsey, Kathryn},
  journal={SIAM Journal on Applied Algebra and Geometry},
  volume={6},
  number={2},
  pages={216--242},
  year={2022},
  publisher={SIAM}
}

@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}

@inproceedings{nguyen2017loss,
  title={The loss surface of deep and wide neural networks},
  author={Nguyen, Quynh and Hein, Matthias},
  booktitle={International conference on machine learning},
  pages={2603--2612},
  year={2017},
  organization={PMLR}
}

.@inproceedings{nguyen2021tight-OLD,
  title={Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep {ReLU} networks},
  author={Nguyen, Quynh and Mondelli, Marco and Mont\'ufar, Guido},
  booktitle={International Conference on Machine Learning},
  pages={8119--8129},
  year={2021},
  organization={PMLR},
}

@InProceedings{nguyen2021tight,
  title = 	 {Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep {ReLU} Networks},
  author =       {Nguyen, Quynh and Mondelli, Marco and Mont\'ufar, Guido},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8119--8129},
  year = 	 {2021},
  noeditor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  nopdf = 	 {http://proceedings.mlr.press/v139/nguyen21g/nguyen21g.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nguyen21g.html},
  abstract = 	 {A recent line of work has analyzed the theoretical properties of deep neural networks via the Neural Tangent Kernel (NTK). In particular, the smallest eigenvalue of the NTK has been related to the memorization capacity, the global convergence of gradient descent algorithms and the generalization of deep nets. However, existing results either provide bounds in the two-layer setting or assume that the spectrum of the NTK matrices is bounded away from 0 for multi-layer networks. In this paper, we provide tight bounds on the smallest eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of infinite widths and for finite widths. In the finite-width setting, the network architectures we consider are fairly general: we require the existence of a wide layer with roughly order of $N$ neurons, $N$ being the number of data samples; and the scaling of the remaining layer widths is arbitrary (up to logarithmic factors). To obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of hidden feature matrices, and upper bounds on the Lipschitz constant of input-output feature maps.}
}



@inproceedings{du2018gradient,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}

@article{bourgain2010singularity,
  title={On the singularity probability of discrete random matrices},
  author={Bourgain, Jean and Vu, Van H and Wood, Philip Matchett},
  journal={Journal of Functional Analysis},
  volume={258},
  number={2},
  pages={559--603},
  year={2010},
  publisher={Elsevier}, 
  url = {https://www.sciencedirect.com/science/article/pii/S0022123609001955},
}


@InProceedings{safran2018spurious,
  title = 	 {Spurious Local Minima are Common in Two-Layer {R}e{LU} Neural Networks},
  author =       {Safran, Itay and Shamir, Ohad},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4433--4441},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/safran18a/safran18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/safran18a.html},
  abstract = 	 {We consider the optimization problem associated with training simple ReLU neural networks of the form $\mathbf{x}\mapsto \sum_{i=1}^{k}\max\{0,\mathbf{w}_i^\top \mathbf{x}\}$ with respect to the squared loss. We provide a computer-assisted proof that even if the input distribution is standard Gaussian, even if the dimension is arbitrarily large, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once $6\le k\le 20$. By a concentration of measure argument, this implies that in high input dimensions, <em>nearly all</em> target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an over-parameterization assumption is necessary to get a positive result in this setting.}
}

@inproceedings{
wang2022the,
title={The Hidden Convex Optimization Landscape of Regularized Two-Layer Re{LU} Networks: an Exact Characterization of Optimal Solutions},
author={Yifei Wang and Jonathan Lacotte and Mert Pilanci},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Z7Lk2cQEG8a}
}

@article{rolnick2020identifying,
  title={Identifying weights and architectures of unknown relu networks},
  author={Rolnick, David and Kording, Konrad P},
  year={2020}
}



@InProceedings{analytical_formula_tian,
  title = 	 {An Analytical Formula of Population Gradient for two-layered {R}e{LU} network and its Applications in Convergence and Critical Point Analysis},
  author =       {Yuandong Tian},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3404--3413},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/tian17a/tian17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/tian17a.html},
  abstract = 	 {In this paper, we explore theoretical properties of training a two-layered ReLU network $g(\mathbf{x}; \mathbf{w}) = \sum_{j=1}^K \sigma(\mathbf{w}_j^\top\mathbf{x})$ with centered $d$-dimensional spherical Gaussian input $\mathbf{x}$ ($\sigma$=ReLU). We train our network with gradient descent on $\mathbf{w}$ to mimic the output of a teacher network with the same architecture and fixed parameters $\mathbf{w}^*$. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters (“out-of-plane“) are not isolated and form manifolds, and characterize in-plane critical-point-free regions for two-ReLU case. On the other hand, convergence to $\mathbf{w}^*$ for one ReLU node is guaranteed with at least $(1-\epsilon)/2$ probability, if weights are initialized randomly with standard deviation upper-bounded by $O(\epsilon/\sqrt{d})$, in accordance with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards $\mathbf{w}^*$ (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings.}
}

@inproceedings{analytical_formula_tian-OLD, 
author = {Tian, Yuandong}, 
title = {An Analytical Formula of Population Gradient for Two-Layered ReLU Network and Its Applications in Convergence and Critical Point Analysis}, 
year = {2017}, 
publisher = {JMLR.org}, 
abstract = {In this paper, we explore theoretical properties of training a two-layered ReLU network g(x; w) = ΣKj=1 σ(WTjX) with centered d-dimensional spherical Gaussian input x (σ=ReLU). We train our network with gradient descent on w to mimic the output of a teacher network with the same architecture and fixed parameters w*. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters ("out-of-plane") are not isolated and form manifolds, and characterize in-plane critical-point-free regions for two ReLU case. On the other hand, convergence to w* for one ReLU node is guaranteed with at least (1 - ε)/2 probability, if weights are initialized randomly with standard deviation upper-bounded by O(ε/√d), consistent with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards w* (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings.}, 
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70}, 
pages = {3404–3413}, 
numpages = {10}, 
location = {Sydney, NSW, Australia}, series = {ICML'17} 
}


@InProceedings{pmlr-v139-simsek21a,
  title = 	 {Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances},
  author =       {Simsek, Berfin and Ged, Fran{\c{c}}ois and Jacot, Arthur and Spadaro, Francesco and Hongler, Clement and Gerstner, Wulfram and Brea, Johanni},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9722--9732},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/simsek21a/simsek21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/simsek21a.html},
  abstract = 	 {We study how permutation symmetries in overparameterized multi-layer neural networks generate ‘symmetry-induced’ critical points. Assuming a network with $ L $ layers of minimal widths $ r_1^*, \ldots, r_{L-1}^* $ reaches a zero-loss minimum at $ r_1^*! \cdots r_{L-1}^*! $ isolated points that are permutations of one another, we show that adding one extra neuron to each layer is sufficient to connect all these previously discrete minima into a single manifold. For a two-layer overparameterized network of width $ r^*+ h =: m $ we explicitly describe the manifold of global minima: it consists of $ T(r^*, m) $ affine subspaces of dimension at least $ h $ that are connected to one another. For a network of width $m$, we identify the number $G(r,m)$ of affine subspaces containing only symmetry-induced critical points that are related to the critical points of a smaller network of width }
  }

@inproceedings{
zhou2018critical,
title={Critical Points of Linear Neural Networks: Analytical Forms and Landscape Properties},
author={Yi Zhou and Yingbin Liang},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SysEexbRb},
}


@ARTICLE{8409482,
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D.},
  journal={IEEE Transactions on Information Theory}, 
  title={Theoretical Insights Into the Optimization Landscape of Over-Parameterized Shallow Neural Networks}, 
  year={2019},
  volume={65},
  number={2},
  pages={742--769},
  nodoi={10.1109/TIT.2018.2854560},
  url={https://ieeexplore.ieee.org/document/8409482}
  }

  @article{soltanolkotabi2018theoretical-OLD,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={2},
  pages={742--769},
  year={2018},
  publisher={IEEE}, 
  url={https://ieeexplore.ieee.org/document/8409482}
}


@inproceedings{chizat2019lazy,
 author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Lazy Training in Differentiable Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{chizat2019lazy-OLD,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{harris2020array,
  title={Array programming with NumPy},
  author={Harris, Charles R and Millman, K Jarrod and Van Der Walt, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and others},
  journal={Nature},
  volume={585},
  number={7825},
  pages={357--362},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{dalcin2011parallel,
  title={Parallel distributed computing using Python},
  author={Dalcin, Lisandro D and Paz, Rodrigo R and Kler, Pablo A and Cosimo, Alejandro},
  journal={Advances in Water Resources},
  volume={34},
  number={9},
  pages={1124--1139},
  year={2011},
  publisher={Elsevier}
}

@article{hunter_matplotlib_2007,
	title = {Matplotlib: {A} {2D} graphics environment},
	volume = {9},
	number = {03},
	journal = {Computing in science \& engineering},
	author = {Hunter, John D},
	year = {2007},
	nonote = {Publisher: IEEE Computer Society},
	pages = {90--95},
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}, 
  url={https://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html}
}

@article{ball1997elementary,
  title={An elementary introduction to modern convex geometry},
  author={Ball, Keith},
  journal={Flavors of geometry},
  volume={31},
  pages={1--58},
  nopages={26},
  year={1997}
}

@article{montanari2022interpolation,
  title={The interpolation phase transition in neural networks: Memorization and generalization under lazy training},
  author={Montanari, Andrea and Zhong, Yiqiao},
  journal={The Annals of Statistics},
  volume={50},
  number={5},
  pages={2816--2847},
  year={2022},
  publisher={Institute of Mathematical Statistics},
  URL = {https://doi.org/10.1214/22-AOS2211}
}

@book{axler2013harmonic,
  title={Harmonic function theory},
  author={Axler, Sheldon and Bourdon, Paul and Wade, Ramey},
  volume={137},
  year={2013},
  publisher={Springer Science \& Business Media},
  url = {https://doi.org/10.1007/978-1-4757-8137-3}
}

@article{rubin1999inversion,
  title={Inversion and characterization of the hemispherical transform},
  author={Rubin, Boris},
  journal={Journal d’Analyse Math{\'e}matique},
  volume={77},
  pages={105--128},
  year={1999},
  publisher={Springer},
  url={https://doi.org/10.1007/BF02791259}
}

@book{efthimiou2014spherical,
  title={Spherical harmonics in p dimensions},
  author={Efthimiou, Costas and Frye, Christopher},
  year={2014},
  publisher={World Scientific},
  nodoi = {10.1142/9134},
  URL = {https://doi.org/10.1142/9134},
}






@article{gonzalez2004dual,
  title={Dual Radon transforms on affine Grassmann manifolds},
  author={Gonzalez, Fulton and Kakehi, Tomoyuki},
  journal={Transactions of the American Mathematical Society},
  volume={356},
  number={10},
  pages={4161--4180},
  year={2004}
}

@book{gradshteyn2014table,
  title={Table of integrals, series, and products},
  author={Gradshteyn, Izrail Solomonovich and Ryzhik, Iosif Moiseevich},
  year={2014},
  publisher={Academic press},
  url={https://doi.org/10.1016/C2010-0-64839-5}
}

@article{ronen2019convergence,
  title={The convergence rate of neural networks for learned functions of different frequencies},
  author={Ronen, Basri and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

.@article{bombari2022memorization-OLD,
  title={Memorization and optimization in deep neural networks with minimum over-parameterization},
  author={Bombari, Simone and Amani, Mohammad Hossein and Mondelli, Marco},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7628--7640},
  year={2022}
}


@inproceedings{bombari2022memorization,
 author = {Bombari, Simone and Amani, Mohammad Hossein and Mondelli, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {7628--7640},
 publisher = {Curran Associates, Inc.},
 title = {Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/323746f0ae2fbd8b6f500dc2d5c5f898-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}



@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}, 
  url={https://doi.org/10.1017/9781108231596}
}

@book{Horn_Johnson_2012, 
place={Cambridge}, 
edition={2}, 
title={Matrix Analysis}, 
publisher={Cambridge University Press}, 
author={Horn, Roger A. and Johnson, Charles R.}, 
year={2012},
url={
https://doi.org/10.1017/CBO9780511810817}}

@article{Schur1911,
author = {Schur, J.},
journal = {Journal für die reine und angewandte Mathematik},
pages = {1-28},
title = {Bemerkungen zur {T}heorie der beschränkten {B}ilinearformen mit unendlich vielen {V}eränderlichen.},
url = {http://eudml.org/doc/149352},
volume = {140},
year = {1911},
}

@article{seeley1966spherical,
  title={Spherical harmonics},
  author={Seeley, Robert T},
  journal={The American Mathematical Monthly},
  volume={73},
  number={4P2},
  pages={115--121},
  year={1966},
  publisher={Taylor \& Francis},
  url={https://doi.org/10.1080/00029890.1966.11970927}
}

@article{carlitz1961some,
  title={Some integrals containing products of Legendre polynomials},
  author={Carlitz, L},
  journal={Archiv der Mathematik},
  volume={12},
  pages={334--340},
  year={1961},
  publisher={Springer}
}

@article{tropp2012user,
  title={User-friendly tail bounds for sums of random matrices},
  author={Tropp, Joel A.},
  journal={Foundations of computational mathematics},
  volume={12},
  pages={389--434},
  year={2012},
  publisher={Springer},
  url={https://doi.org/10.1007/s10208-011-9099-z}
}



@article{10.1007/s10107-020-01501-5,
author = {Bolte, J\'{e}r\^{o}me and Pauwels, Edouard},
title = {Conservative set valued fields, automatic differentiation, stochastic gradient methods and deep learning},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {188},
number = {1},
noissn = {0025-5610},
url = {https://doi.org/10.1007/s10107-020-01501-5},
nodoi = {10.1007/s10107-020-01501-5},
abstract = {Modern problems in AI or in numerical analysis require nonsmooth approaches with a flexible calculus. We introduce generalized derivatives called conservative fields for which we develop a calculus and provide representation formulas. Functions having a conservative field are called path differentiable: convex, concave, Clarke regular and any semialgebraic Lipschitz continuous functions are path differentiable. Using Whitney stratification techniques for semialgebraic and definable sets, our model provides variational formulas for nonsmooth automatic differentiation oracles, as for instance the famous backpropagation algorithm in deep learning. Our differential model is applied to establish the convergence in values of nonsmooth stochastic gradient methods as they are implemented in practice.},
journal = {Math. Program.},
month = {jul},
pages = {19–51},
numpages = {33},
keywords = {90C06 Large-scale problems, 49J53 Set-valued and variational analysis, 68T05 Learning and adaptive systems in artificial intelligence, 62M45 Neural nets and related approaches to inference from stochastic processes, 49M27 Decomposition methods, 65K10 Numerical optimization and variational techniques, First order methods, Clarke subdifferential, Stochastic gradient, o-Minimal structures, Definable sets, Nonsmooth stochastic optimization, Backpropagation algorithm, Automatic differentiation, Deep learning}
}


@inproceedings{10.5555/3495724.3496288,
 author = {Lee, Wonyeol and Yu, Hangyeol and Rival, Xavier and Yang, Hongseok},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6719--6730},
 publisher = {Curran Associates, Inc.},
 title = {On Correctness of Automatic Differentiation for Non-Differentiable Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4aaa76178f8567e05c8e8295c96171d8-Paper.pdf},
 volume = {33},
 year = {2020}
}



@article{10.1214/aos/1015957395,
author = {B. Laurent and P. Massart},
title = {{Adaptive estimation of a quadratic functional by model selection}},
volume = {28},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1302--1338},
keywords = {$l_p$-bodies, adaptive estimation, Besov bodies, efficient estimation, Gaussian sequence model, Model selection, quadratic functionals},
year = {2000},
nodoi = {10.1214/aos/1015957395},
URL = {https://doi.org/10.1214/aos/1015957395}
}
@inproceedings{cao2019towards,
	author = {Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
	booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
	month = {August},
	pages = {2205--2211},
	title = {Towards Understanding the Spectral Bias of Deep Learning},
	url = {https://doi.org/10.24963/ijcai.2021/304},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.24963/ijcai.2021/304}}



@InProceedings{basri2020frequency,
  title = 	 {Frequency Bias in Neural Networks for Input of Non-Uniform Density},
  author =       {Basri, Ronen and Galun, Meirav and Geifman, Amnon and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {685--694},
  year = 	 {2020},
  noeditor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/basri20a/basri20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/basri20a.html},
  abstract = 	 {Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias – networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency $\kappa$, convergence at a point $x \in \S^{d-1}$ occurs in time $O(\kappa^d/p(x))$ where $p(x)$ denotes the local density at $x$. Specifically, for data in $\S^1$ we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.}
}


@inproceedings{velikanov2021explicit,
 author = {Velikanov, Maksim and Yarotsky, Dmitry},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {2570--2582},
 publisher = {Curran Associates, Inc.},
 title = {Explicit loss asymptotics in the gradient descent training of neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/14faf969228fc18fcd4fcf59437b0c97-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{jacot2018neural,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
 volume = {31},
 year = {2018}
}



@inproceedings{xie2017diverse,
  title={Diverse neural network learns true target functions},
  author={Xie, Bo and Liang, Yingyu and Song, Le},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages={1216--1224},
  year={2017},
  publisher={PMLR}, 
  url = 	 {https://proceedings.mlr.press/v54/xie17a.html},
}




% TAKEN FROM OLD NTK PAPER
@ARTICLE{Sontag89backpropagationcan,
    author = {Eduardo D. Sontag and Héctor J. Sussmann},
    title = {Backpropagation Can Give Rise To Spurious Local Minima Even For Networks Without Hidden Layers},
    journal = {Complex Systems},
    year = {1989},
    volume = {3},
    pages = {91--106}
}


@book{DBLP:books/daglib/0025992,
  author    = {Martin Anthony and
               Peter L. Bartlett},
  title     = {Neural Network Learning - Theoretical Foundations},
  publisher = {Cambridge University Press},
  year      = {2002},
  url       = {http://www.cambridge.org/gb/knowledge/isbn/item1154061/?site\_locale=en\_GB},
  noisbn      = {978-0-521-57353-5},
  timestamp = {Thu, 05 May 2011 16:48:09 +0200},
  biburl    = {https://dblp.org/rec/books/daglib/0025992.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{du2019gradient,
  title = 	 {Gradient Descent Finds Global Minima of Deep Neural Networks},
  author =       {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1675--1685},
  year = 	 {2019},
  noeditor =	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/du19c/du19c.pdf},
  url = 	 {https://proceedings.mlr.press/v97/du19c.html},
  abstract = 	 {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.}
}


@inproceedings{Lee2019WideNN-SHORT,
 author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf},
 volume = {32},
 year = {2019}
}

@InProceedings{allenzhu2019convergence,
  title = 	 {A Convergence Theory for Deep Learning via Over-Parameterization},
  author =       {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {242--252},
  year = 	 {2019},
  noeditor =	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth =	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/allen-zhu19a/allen-zhu19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/allen-zhu19a.html},
  abstract = 	 {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps &nbsp; e^{-T}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).}
}


@inproceedings{zou2019improved,
 author = {Zou, Difan and Gu, Quanquan},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {An Improved Analysis of Training Over-parameterized Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep {ReLU} networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020},
  publisher={Springer},
  url={https://doi.org/10.1007/s10994-019-05839-6}
}



@InProceedings{nguyenrelu,
  title = 	 {On the Proof of Global Convergence of Gradient Descent for Deep {ReLU} Networks with Linear Widths},
  author =       {Nguyen, Quynh},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8056--8062},
  year = 	 {2021},
  noeditor =	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  nomonth =	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nguyen21a/nguyen21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nguyen21a.html},
  abstract = 	 {We give a simple proof for the global convergence of gradient descent in training deep ReLU networks with the standard square loss, and show some of its improvements over the state-of-the-art. In particular, while prior works require all the hidden layers to be wide with width at least $\Omega(N^8)$ ($N$ being the number of training samples), we require a single wide layer of linear, quadratic or cubic width depending on the type of initialization. Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof need not track the evolution of the entire NTK matrix, or more generally, any quantities related to the changes of activation patterns during training. Instead, we only need to track the evolution of the output at the last hidden layer, which can be done much more easily thanks to the Lipschitz property of ReLU. Some highlights of our setting: (i) all the layers are trained with standard gradient descent, (ii) the network has standard parameterization as opposed to the NTK one, and (iii) the network has a single wide layer as opposed to having all wide hidden layers as in most of NTK-related results.}
}

@inproceedings{marco,
 author = {Nguyen, Quynh and Mondelli, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11961--11972},
 publisher = {Curran Associates, Inc.},
 title = {Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology},
 url = {https://proceedings.neurips.cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{
murray2023characterizing,
title={Characterizing the spectrum of the {NTK} via a power series expansion},
author={Michael Murray and Hui Jin and Benjamin Bowman and Guido Mont\'ufar},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Tvms8xrZHyR}
}

@misc{yang2020finegrained,
      title={A Fine-Grained Spectral Perspective on Neural Networks}, 
      author={Greg Yang and Hadi Salman},
      year={2020},
      eprint={1907.10599},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cui2021generalization,
title={Generalization Error Rates in Kernel Regression: The Crossover from the Noiseless to Noisy Regime},
author={Hugo Cui and Bruno Loureiro and Florent Krzakala and Lenka Zdeborov{\'a}},
booktitle={Advances in Neural Information Processing Systems},
noeditor ={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=Da_EHrAcfwd}
}

@inproceedings{
jin2022learning,
title={Learning Curves for {G}aussian Process Regression with Power-Law Priors and Targets},
author={Hui Jin and Pradeep Kr. Banerjee and Guido Mont\'ufar},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=KeI9E-gsoB}
}

@inproceedings{NEURIPS2020_572201a4,
 author = {Fan, Zhou and Wang, Zhichao},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7710--7721},
 publisher = {Curran Associates, Inc.},
 title = {Spectra of the Conjugate Kernel and Neural Tangent Kernel for linear-width neural networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/572201a4497b0b9f02d4f279b09ec30d-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{arora_exact_comp, 
 author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Exact Computation with an Infinitely Wide Neural Net},
 url = {https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{uniform_sphere_data,
  author    = {Ronen Basri and
               David W. Jacobs and
               Yoni Kasten and
               Shira Kritchman},
  noeditor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {The Convergence Rate of Neural Networks for Learned Functions of Different
               Frequencies},
  booktitle = {Advances in Neural Information Processing Systems 32},
  nonote ={Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada}, 
  pages     = {4763--4772},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/5ac8bb8a7d745102a978c5f8ccdb61b8-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:19 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/BasriJKK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
bietti2021deep,
title={Deep Equals Shallow for {ReLU} Networks in Kernel Regimes},
author={Alberto Bietti and Francis Bach},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=aDjoksTpXOP}
}

@inproceedings{geifman2020similarity,
 author = {Geifman, Amnon and Yadav, Abhay and Kasten, Yoni and Galun, Meirav and Jacobs, David and Ronen, Basri},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1451--1461},
 publisher = {Curran Associates, Inc.},
 title = {On the Similarity between the {L}aplace and Neural Tangent Kernels},
 url = {https://proceedings.neurips.cc/paper/2020/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{banerjee2023neural,
title={Neural Tangent Kernel at Initialization: Linear Width Suffices},
author={Arindam Banerjee and Pedro Cisneros-Velarde and Libin Zhu and Mikhail Belkin},
booktitle={The 39th Conference on Uncertainty in Artificial Intelligence},
year={2023},
url={https://openreview.net/forum?id=VJaoe7Rp9tZ}
}

@article{LIU202285,
title = {Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
journal = {Applied and Computational Harmonic Analysis},
volume = {59},
pages = {85-116},
year = {2022},
note = {Special Issue on Harmonic Analysis and Machine Learning},
noissn = {1063-5203},
nodoi = {https://doi.org/10.1016/j.acha.2021.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S106352032100110X},
author = {Chaoyue Liu and Libin Zhu and Mikhail Belkin},
keywords = {Deep learning, Non-linear optimization, Over-parameterized models, PL condition},
abstract = {The success of deep learning is due, to a large extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. The purpose of this work is to propose a modern view and a general mathematical framework for loss landscapes and efficient optimization in over-parameterized machine learning models and systems of non-linear equations, a setting that includes over-parameterized deep neural networks. Our starting observation is that optimization landscapes corresponding to such systems are generally not convex, even locally around a global minimum, a condition we call essential non-convexity. We argue that instead they satisfy PL⁎, a variant of the Polyak-Łojasiewicz condition [32], [25] on most (but not all) of the parameter space, which guarantees both the existence of solutions and efficient optimization by (stochastic) gradient descent (SGD/GD). The PL⁎ condition of these systems is closely related to the condition number of the tangent kernel associated to a non-linear system showing how a PL⁎-based non-linear theory parallels classical analyses of over-parameterized linear equations. We show that wide neural networks satisfy the PL⁎ condition, which explains the (S)GD convergence to a global minimum. Finally we propose a relaxation of the PL⁎ condition applicable to “almost” over-parameterized systems.}
}

@inproceedings{NEURIPS2020_b7ae8fec,
 author = {Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15954--15964},
 publisher = {Curran Associates, Inc.},
 title = {On the linearity of large non-linear models: when and why the tangent kernel is constant},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{10.5555/3495724.3496995,
 author = {Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor ={H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15156--15172},
 publisher = {Curran Associates, Inc.},
 title = {Finite Versus Infinite Neural Networks: an Empirical Study},
 url = {https://proceedings.neurips.cc/paper/2020/file/ad086f59924fffe0773f8d0ca22ea712-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{lazy_training,
 author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
 booktitle = {Advances in Neural Information Processing Systems},
 noeditor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Lazy Training in Differentiable Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{nevai1994generalized,
  title={Generalized Jacobi weights, Christoffel functions, and Jacobi polynomials},
  author={Nevai, Paul and Erd{\'e}lyi, Tam{\'a}s and Magnus, Alphonse P},
  journal={SIAM Journal on Mathematical Analysis},
  volume={25},
  number={2},
  pages={602--614},
  year={1994},
  publisher={SIAM}
}

@article{xie2013exponential,
  title={On exponential convergence of {G}egenbauer interpolation and spectral differentiation},
  author={Xie, Ziqing and Wang, Li-Lian and Zhao, Xiaodan},
  journal={Mathematics of Computation},
  volume={82},
  number={282},
  pages={1017--1036},
  year={2013},
  url = {https://doi.org/10.1090/S0025-5718-2012-02645-7}
}

@article{li2010concise,
  title={Concise formulas for the area and volume of a hyperspherical cap},
  author={Li, Shengqiao},
  journal={Asian Journal of Mathematics \& Statistics},
  volume={4},
  number={1},
  pages={66--70},
  year={2010},
  publisher={Science Alert}
}