\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{proof}{Proof}
\newtheorem{theorem}{Theorem}

\title{The NTK and its First-Order Corrections}
\author{}
\date{}

\begin{document}

\maketitle

\begin{proposition}[Formula for $K_\theta(x_1,x_2)$]
Given $x_1,x_2 \in \mathbb{R}^{m_0}$, the entry $K_\theta(x_1,x_2) \in \mathbb{R}^{m_l \times m_l}$ equals:
\begin{equation}
\sigma^{-2} \sum_{k=1}^{l-1} m q_k X_k(x_1,x_2,\theta_{1:k-1})A_l B_{k+1,l}(x_1,\theta_{1:l-1})B_{k+1,l}(x_2,\theta_{1:l-1})^*A_l^* + X_l(x_1,x_2,\theta_{1:l-1})I_{m_l}
\end{equation}
\end{proposition}

\section{Corrections at First Order}

First-order corrections for wide neural networks can be obtained by considering terms of order $\mathcal{O}(n^{-1})$ in the asymptotic expansion. For the NTK (Neural Tangent Kernel), these corrections are particularly important as they describe how the kernel evolves during training.

\begin{remark}[Connection with the NTK]
The NTK formula given in Proposition 6 is the basis for deriving the corrections $O_3$ and $O_4$. Indeed, $O_3$ can be obtained by taking the gradient of this formula with respect to the network parameters, since the NTK is essentially the derivative of the network function with respect to its parameters.

More precisely, the first-order corrections $O_3$ are of order $\mathcal{O}(n^{-1})$ and describe how the kernel evolves during training. These corrections are crucial because:
\begin{itemize}
\item They represent the first deviation from infinite-width behavior
\item They allow precise quantification of the approximation error made when considering the kernel as constant
\item They show that kernel corrections during stochastic gradient descent are of order $\mathcal{O}(n^{-1})$, which improves the previous bound of $\mathcal{O}(n^{-1/2})$
\end{itemize}

To obtain $O_3$, we integrate the differential equations of the coupled system that describes the network evolution, keeping only terms of order $\mathcal{O}(n^{-1})$. This correction becomes constant at long times, indicating a stabilization of the network behavior.
\end{remark}

\section{Derivation of First-Order Corrections}

\begin{proof}[Detailed proof of the derivation of $O_3$]
Let $l \ge 2$ be the number of layers and $m_0,\dots ,m_l$ the widths. For each $k\in\{1,\dots ,l\}$ we denote $A_k\in \mathbb R^{m_k\times m_{k-1}}$ the weight matrix of layer $k$. For an input $x\in\mathbb R^{m_0}$ we define the activation of the first layer by $x_1(x)=x$ and for $k\ge 2$
\[
 x_k(x,\theta_{1:k-1})\;=\;m_{k-1}^{-1/2}\,\phi\bigl(N_{k-1}(x,\theta_{1:k-1})\bigr),\qquad N_{k-1}(x,\theta_{1:k-1})\;=\;m^{q_{k-1}/2}A_{k-1}x_{k-1}(x,\theta_{1:k-2}).
\]
The scalar derivative of the activation is denoted
\[
 x'_k(x,\theta_{1:k-1})\;=\;m_{k-1}^{-1/2}\,\phi'\bigl(N_{k-1}(x,\theta_{1:k-1})\bigr).
\]
We associate to this derivative the diagonal matrix $D_k(x,\theta_{1:k-1})=\operatorname{diag}(x'_k(x,\theta_{1:k-1}))\in \mathbb R^{m_{k-1}\times m_{k-1}}$. For $1\le k<l$ we finally define the backpropagation matrix
\[
 B_{k+1,l}(x,\theta_{1:l-1})\;=\;\prod_{s=k+1}^{l-1} m^{q_s/2}\,A_s\,D_{s}(x,\theta_{1:s-1})
\]
(the product is ordered from left to right by increasing indices).

The previous Proposition gives for all $x_1,x_2\in\mathbb R^{m_0}$
\[
 K_{\theta}(x_1,x_2)=\sigma^{-2}\sum_{k=1}^{l-1}m q_k X_k(x_1,x_2,\theta_{1:k-1})A_l B_{k+1,l}(x_1,\theta_{1:l-1})B_{k+1,l}(x_2,\theta_{1:l-1})^{*}A_l^{*}+X_l(x_1,x_2,\theta_{1:l-1})I_{m_l}.
\]
We now fix a parameter $\theta_{\mu}$ belonging to layer $p$ with $1\le p\le l-1$ and consider the derivative $\partial_{\theta_{\mu}}K_{\theta}(x_1,x_2)$. Terms with index $k<p$ do not depend on $\theta_{\mu}$, so their derivative is zero. The term $k=p$ explicitly contains $A_p$ and produces after differentiation the main contribution
\[
 \partial_{\theta_{\mu}}\Bigl(m q_p X_p A_l B_{p+1,l}(x_1)B_{p+1,l}(x_2)^{*}A_l^{*}\Bigr).
\]
For each $k>p$ the dependence on $\theta_{\mu}$ appears only through the factor $B_{k+1,l}$. In this product $B_{k+1,l}$ the matrix $A_p$ is separated from subsequent matrices by at least one diagonal matrix $D_{p+1}(x)$ which contains $\phi'$. The derivative of $D_{p+1}(x)$ with respect to a coefficient of $A_p$ is proportional to $\phi''$. When the activation is ReLU, we have $\phi''=0$ almost everywhere; the contribution of these derivatives is therefore zero for almost all realizations of $\theta$. Consequently all terms in the sum with index strictly greater than $p$ vanish after differentiation.

It follows that the derivative of $K_{\theta}(x_1,x_2)$ with respect to $\theta_{\mu}$ reduces to the single term arising from $k=p$. By introducing the operator
\[
 O_3(x_1,x_2,x_3)=\partial_{\theta_{\mu}}K_{\theta}(x_1,x_2)\quad(\text{$x_3$ implicitly denotes the dependence on }\theta_{\mu}),
\]
we obtain an object of order $\mathcal O\bigl(n^{-1}\bigr)$ when the width $n$ of each layer grows, which constitutes the desired first-order correction. The next term $O_4$ would arise from second-order derivatives but these would again require the appearance of $\phi''$ and are consequently zero for ReLU, so that $O_4$ does not contribute. The sum in the NTK formula can therefore be truncated at index $p$ for the study of this derivative.
\end{proof}

\section{Spectral Analysis and Finite-Width Corrections}

\begin{theorem}[Spectral decomposition of the NTK]
For a finite-width network $N$, the smallest eigenvalue of the NTK admits the following decomposition:
\[ \lambda_{\min}(K_\theta) \geq al + \frac{b}{N} + o(\frac{1}{N}) \]
where:
\begin{itemize}
\item $al$ is the asymptotic term corresponding to the infinite-width NTK ($N \to \infty$)
\item $\frac{b}{N}$ is the first-order correction arising from terms $O_3$ and $O_4$
\item $o(\frac{1}{N})$ represents negligible higher-order terms
\end{itemize}
\end{theorem}

\begin{proof}
This decomposition follows from the analysis of corrections $O_3$ and $O_4$:

1) The term $al$ comes from the infinite-width NTK formula. It grows linearly with the network depth $l$ because each layer contributes additively to the minimal eigenvalue.

2) The corrections $O_3$ and $O_4$ are of order $\frac{1}{N}$ because they arise from finite fluctuations in the network width. More precisely:
   \[ \Vert O_3 \Vert = \mathcal{O}(\frac{1}{N}) \quad \text{and} \quad \Vert O_4 \Vert = \mathcal{O}(\frac{1}{N}) \]

3) The sum of these corrections gives the term $\frac{b}{N}$, where $b$ is a constant that depends on the network architecture and the ReLU activation.

This decomposition is fundamental because it shows that:
\begin{enumerate}
\item The asymptotic behavior of the NTK ($al$) dominates for wide networks
\item The $\frac{1}{N}$ corrections become significant for networks of moderate width
\item The positivity of $\lambda_{\min}(K_\theta)$ is guaranteed for sufficiently large $N$
\end{enumerate}

This positivity guarantee is crucial as it ensures the convergence of gradient descent learning in the finite-width regime.
\end{proof}

\end{document}
