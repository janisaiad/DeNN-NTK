\begin{thebibliography}{10}

\bibitem{Neal1996}
Radford~M. Neal.
\newblock {\em Priors for Infinite Networks}, pages 29--53.
\newblock Springer New York, New York, NY, 1996.

\bibitem{lee2018deep}
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam
  Schoenholz, and Yasaman Bahri.
\newblock Deep neural networks as gaussian processes.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{daniely2017sgd}
Amit Daniely.
\newblock Sgd learns the conjugate kernel class of the network.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2422--2430, 2017.

\bibitem{ntk}
Arthur {Jacot}, Franck {Gabriel}, and Cl{\'e}ment {Hongler}.
\newblock {Neural Tangent Kernel: Convergence and Generalization in Neural
  Networks}.
\newblock {\em arXiv e-prints}, page arXiv:1806.07572, June 2018.

\bibitem{DBLP:journals/corr/abs-1711-04735}
Jeffrey Pennington, Samuel~S. Schoenholz, and Surya Ganguli.
\newblock Resurrecting the sigmoid in deep learning through dynamical isometry:
  theory and practice.
\newblock {\em CoRR}, abs/1711.04735, 2017.

\bibitem{NIPS2009_3628}
Youngmin Cho and Lawrence~K. Saul.
\newblock Kernel methods for deep learning.
\newblock pages 342--350, 2009.

\bibitem{Feynman:1949zx}
R.~P. Feynman.
\newblock {Space - time approach to quantum electrodynamics}.
\newblock {\em Phys. Rev.}, 76:769--789, 1949.
\newblock [,99(1949)].

\bibitem{tHooft:1973alw}
Gerard 't~Hooft.
\newblock {A Planar Diagram Theory for Strong Interactions}.
\newblock {\em Nucl. Phys.}, B72:461, 1974.
\newblock [,337(1973)].

\bibitem{g.2018gaussian}
Alexander~G. de~G.~Matthews, Jiri Hron, Mark Rowland, Richard~E. Turner, and
  Zoubin Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{novak2019bayesian}
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel~A.
  Abolafia, Jeffrey Pennington, and Jascha Sohl-dickstein.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{garriga-alonso2018deep}
Adria Garriga-Alonso, Carl~Edward Rasmussen, and Laurence Aitchison.
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock {\em arXiv preprint arXiv:1902.04760}, 2019.

\bibitem{pennington2017geometry}
Jeffrey Pennington and Yasaman Bahri.
\newblock Geometry of neural network loss surfaces via random matrix theory.
\newblock In {\em International Conference on Machine Learning}, pages
  2798--2806, 2017.

\bibitem{pennington2018spectrum}
Jeffrey Pennington and Pratik Worah.
\newblock The spectrum of the fisher information matrix of a
  single-hidden-layer neural network.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5410--5419, 2018.

\bibitem{schoenholz2017correspondence}
Samuel~S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein.
\newblock A correspondence between random neural networks and statistical field
  theory.
\newblock {\em arXiv preprint arXiv:1710.06570}, 2017.

\bibitem{xiao2018dynamical}
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel~S Schoenholz, and
  Jeffrey Pennington.
\newblock Dynamical isometry and a mean field theory of cnns: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock {\em arXiv preprint arXiv:1806.05393}, 2018.

\bibitem{chen2018dynamical}
Minmin Chen, Jeffrey Pennington, and Samuel~S Schoenholz.
\newblock Dynamical isometry and a mean field theory of rnns: Gating enables
  signal propagation in recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1806.05394}, 2018.

\bibitem{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In {\em Advances In Neural Information Processing Systems}, pages
  2253--2261, 2016.

\bibitem{2019arXiv190206720L}
Jaehoon {Lee}, Lechao {Xiao}, Samuel~S. {Schoenholz}, Yasaman {Bahri}, Jascha
  {Sohl-Dickstein}, and Jeffrey {Pennington}.
\newblock {Wide Neural Networks of Any Depth Evolve as Linear Models Under
  Gradient Descent}.
\newblock {\em arXiv e-prints}, page arXiv:1902.06720, Feb 2019.

\bibitem{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.02054}, 2018.

\bibitem{du2018gradient2}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em arXiv preprint arXiv:1811.03804}, 2018.

\bibitem{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock {\em arXiv preprint arXiv:1811.03962}, 2018.

\bibitem{geiger2019scaling}
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun,
  St{\'e}phane d'Ascoli, Giulio Biroli, Cl{\'e}ment Hongler, and Matthieu
  Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock {\em arXiv preprint arXiv:1901.01608}, 2019.

\bibitem{arora2019fine}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock {\em arXiv preprint arXiv:1901.08584}, 2019.

\bibitem{NIPS2017_6857}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 30}, pages 2637--2646. Curran Associates, Inc., 2017.

\bibitem{huang2019dynamics}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy, 2019.

\bibitem{fdworkshop}
Ethan Dyer and Guy Gur-Ari.
\newblock Asymptotics of wide networks from feynman diagrams.
\newblock In {\em Theoretical Physics for Deep Learning, ICML Workshop}. 2019.

\bibitem{2018arXiv181207956C}
Lenaic {Chizat}, Edouard {Oyallon}, and Francis {Bach}.
\newblock {On Lazy Training in Differentiable Programming}.
\newblock {\em arXiv e-prints}, page arXiv:1812.07956, Dec 2018.

\bibitem{2019arXiv190608899G}
Behrooz {Ghorbani}, Song {Mei}, Theodor {Misiakiewicz}, and Andrea {Montanari}.
\newblock {Limitations of Lazy Training of Two-layers Neural Networks}.
\newblock {\em arXiv e-prints}, page arXiv:1906.08899, Jun 2019.

\bibitem{DBLP:journals/corr/abs-1906-08034}
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart.
\newblock Disentangling feature and lazy learning in deep neural networks: an
  empirical study.
\newblock {\em CoRR}, abs/1906.08034, 2019.

\bibitem{hanin2018products}
Boris Hanin and Mihai Nica.
\newblock Products of many large random matrices and gradients in deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1812.05994}, 2018.

\bibitem{trefethen97}
Lloyd~N. Trefethen and David Bau.
\newblock {\em Numerical Linear Algebra}.
\newblock SIAM, 1997.

\bibitem{sagun2016eigenvalues}
Levent Sagun, Leon Bottou, and Yann LeCun.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock {\em arXiv preprint arXiv:1611.07476}, 2016.

\bibitem{sagun2017empirical}
Levent Sagun, Utku Evci, V~Ugur Guney, Yann Dauphin, and Leon Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock {\em arXiv preprint arXiv:1706.04454}, 2017.

\bibitem{gur2018gradient}
Guy Gur-Ari, Daniel~A Roberts, and Ethan Dyer.
\newblock Gradient descent happens in a tiny subspace.
\newblock {\em arXiv preprint arXiv:1812.04754}, 2018.

\bibitem{ghorbani2019investigation}
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao.
\newblock An investigation into neural net optimization via hessian eigenvalue
  density.
\newblock {\em arXiv preprint arXiv:1901.10159}, 2019.

\bibitem{papyan2019measurements}
Vardan Papyan.
\newblock Measurements of three-level hierarchical structure in the outliers in
  the spectrum of deepnet hessians.
\newblock {\em arXiv preprint arXiv:1901.08244}, 2019.

\end{thebibliography}
