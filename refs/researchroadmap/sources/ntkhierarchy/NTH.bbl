\newcommand{\noopsort}[1]{} \newcommand{\printfirst}[2]{#1}
  \newcommand{\singleletter}[1]{#1} \newcommand{\switchargs}[2]{#2#1}
\begin{thebibliography}{10}

\bibitem{allen2019can}
Z.~Allen-Zhu and Y.~Li.
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock {\em arXiv preprint arXiv:1905.10337}, 2019.

\bibitem{allen2018learning}
Z.~Allen-Zhu, Y.~Li, and Y.~Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock {\em arXiv preprint arXiv:1811.04918}, 2018.

\bibitem{allen2018convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock {\em In ICML, arXiv:1811.03962}, 2018.

\bibitem{araujo2019mean}
D.~Ara{\'u}jo, R.~I. Oliveira, and D.~Yukimura.
\newblock A mean-field limit for certain deep neural networks.
\newblock {\em arXiv preprint arXiv:1906.00193}, 2019.

\bibitem{arora2019exact}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock {\em arXiv preprint arXiv:1904.11955}, 2019.

\bibitem{arora2019fine}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, and R.~Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock {\em arXiv preprint arXiv:1901.08584}, 2019.

\bibitem{bhojanapalli2016global}
S.~Bhojanapalli, B.~Neyshabur, and N.~Srebro.
\newblock Global optimality of local search for low rank matrix recovery.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3873--3881, 2016.

\bibitem{chizat2018global}
L.~Chizat and F.~Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In {\em Advances in neural information processing systems}, pages
  3036--3046, 2018.

\bibitem{choromanska2015loss}
A.~Choromanska, M.~Henaff, M.~Mathieu, G.~Ben~Arous, and Y.~LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock In {\em Proceedings of the Eighteenth International Conference on
  Artificial Intelligence and Statistics}, pages 192--204, 2015.

\bibitem{collobert2011natural}
R.~Collobert, J.~Weston, L.~Bottou, M.~Karlen, K.~Kavukcuoglu, and P.~Kuksa.
\newblock Natural language processing (almost) from scratch.
\newblock {\em Journal of machine learning research}, 12(Aug):2493--2537, 2011.

\bibitem{dauphin2014identifying}
Y.~N. Dauphin, R.~Pascanu, C.~Gulcehre, K.~Cho, S.~Ganguli, and Y.~Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2933--2941, 2014.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{du2018gradient2}
S.~S. Du, J.~D. Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em ICML, arXiv:1811.03804}, 2018.

\bibitem{du2018gradient1}
S.~S. Du, X.~Zhai, B.~Poczos, and A.~Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em In ICLR, arXiv:1810.02054}, 2018.

\bibitem{ge2015escaping}
R.~Ge, F.~Huang, C.~Jin, and Y.~Yuan.
\newblock Escaping from saddle points---online stochastic gradient for tensor
  decomposition.
\newblock In {\em Proceedings of The 28th Conference on Learning Theory}, pages
  797--842, 2015.

\bibitem{ge2016matrix}
R.~Ge, J.~D. Lee, and T.~Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2973--2981, 2016.

\bibitem{ghorbani2019linearized}
B.~Ghorbani, S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock {\em arXiv preprint arXiv:1904.12191}, 2019.

\bibitem{glorot2010understanding}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem{hinton2012deep}
G.~Hinton, L.~Deng, D.~Yu, G.~Dahl, A.-r. Mohamed, N.~Jaitly, A.~Senior,
  V.~Vanhoucke, P.~Nguyen, B.~Kingsbury, et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition.
\newblock {\em IEEE Signal processing magazine}, 29, 2012.

\bibitem{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{jin2017escape}
C.~Jin, R.~Ge, P.~Netrapalli, S.~M. Kakade, and M.~I. Jordan.
\newblock How to escape saddle points efficiently.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1724--1732. JMLR. org, 2017.

\bibitem{kawaguchi2016deep}
K.~Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  586--594, 2016.

\bibitem{kawaguchi2019gradient}
K.~Kawaguchi and J.~Huang.
\newblock Gradient descent finds global minima for generalizable deep neural
  networks of practical sizes.
\newblock {\em arXiv preprint arXiv:1908.02419}, 2019.

\bibitem{kawaguchi2019elimination}
K.~Kawaguchi and L.~P. Kaelbling.
\newblock Elimination of all bad local minima in deep learning.
\newblock {\em arXiv preprint arXiv:1901.00279}, 2019.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lee2019wide}
J.~Lee, L.~Xiao, S.~S. Schoenholz, Y.~Bahri, J.~Sohl-Dickstein, and
  J.~Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock {\em arXiv preprint arXiv:1902.06720}, 2019.

\bibitem{lee2016gradient}
J.~D. Lee, M.~Simchowitz, M.~I. Jordan, and B.~Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In {\em Conference on learning theory}, pages 1246--1257, 2016.

\bibitem{li2018learning}
Y.~Li and Y.~Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8157--8166, 2018.

\bibitem{liang2018adding}
S.~Liang, R.~Sun, J.~D. Lee, and R.~Srikant.
\newblock Adding one neuron can eliminate all bad local minima.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{mei2019mean}
S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock {\em arXiv preprint arXiv:1902.06015}, 2019.

\bibitem{nguyen2019mean}
P.-M. Nguyen.
\newblock Mean field limit of the learning dynamics of multilayer neural
  networks.
\newblock {\em arXiv preprint arXiv:1902.02880}, 2019.

\bibitem{park2016non}
D.~Park, A.~Kyrillidis, C.~Caramanis, and S.~Sanghavi.
\newblock Non-square matrix sensing without spurious local minima via the
  burer-monteiro approach.
\newblock {\em arXiv preprint arXiv:1609.03240}, 2016.

\bibitem{sainath2013deep}
T.~N. Sainath, A.-r. Mohamed, B.~Kingsbury, and B.~Ramabhadran.
\newblock Deep convolutional neural networks for lvcsr.
\newblock In {\em 2013 IEEE international conference on acoustics, speech and
  signal processing}, pages 8614--8618. IEEE, 2013.

\bibitem{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em Nature}, 529(7587):484--489, 2016.

\bibitem{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nature}, 550(7676):354, 2017.

\bibitem{sirignano2019mean}
J.~Sirignano and K.~Spiliopoulos.
\newblock Mean field analysis of deep neural networks.
\newblock {\em arXiv preprint arXiv:1903.04440}, 2019.

\bibitem{song2018mean}
M.~Song, A.~Montanari, and P.~Nguyen.
\newblock A mean field view of the landscape of two-layers neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  115:E7665--E7671, 2018.

\bibitem{song2019quadratic}
Z.~Song and X.~Yang.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound.
\newblock {\em arXiv preprint arXiv:1906.03593}, 2019.

\bibitem{sun2016complete}
J.~Sun, Q.~Qu, and J.~Wright.
\newblock Complete dictionary recovery over the sphere i: Overview and the
  geometric picture.
\newblock {\em IEEE Transactions on Information Theory}, 63(2):853--884, 2016.

\bibitem{sun2018geometric}
J.~Sun, Q.~Qu, and J.~Wright.
\newblock A geometric analysis of phase retrieval.
\newblock {\em Foundations of Computational Mathematics}, 18(5):1131--1198,
  2018.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1--9, 2015.

\bibitem{MR2963170}
R.~Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock In {\em Compressed sensing}, pages 210--268. Cambridge Univ. Press,
  Cambridge, 2012.

\bibitem{wu2016google}
Y.~Wu, M.~Schuster, Z.~Chen, Q.~V. Le, M.~Norouzi, W.~Macherey, M.~Krikun,
  Y.~Cao, Q.~Gao, K.~Macherey, et~al.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock {\em arXiv preprint arXiv:1609.08144}, 2016.

\bibitem{DBLP:journals/corr/abs-1902-04760}
G.~Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock {\em CoRR}, abs/1902.04760, 2019.

\bibitem{yehudai2019power}
G.~Yehudai and O.~Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock {\em arXiv preprint arXiv:1904.00687}, 2019.

\bibitem{zou2018stochastic}
D.~Zou, Y.~Cao, D.~Zhou, and Q.~Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock {\em arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
