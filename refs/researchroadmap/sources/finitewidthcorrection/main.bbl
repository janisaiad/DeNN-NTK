\begin{thebibliography}{10}

\bibitem{belkin2018reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine learning and the bias-variance trade-off.
\newblock {\em arXiv preprint arXiv:1812.11118}, 2018.

\bibitem{belkin2019two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock {\em arXiv preprint arXiv:1903.07571}, 2019.

\bibitem{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock {\em arXiv preprint arXiv:1905.12173}, 2019.

\bibitem{chizat2018note}
Lenaic Chizat and Francis Bach.
\newblock A note on lazy training in supervised differentiable programming.
\newblock {\em arXiv preprint arXiv:1812.07956}, 2018.

\bibitem{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In {\em Advances in neural information processing systems}, pages
  3036--3046, 2018.

\bibitem{wei2018regularization}
Qiang~Liu Colin~Wei, Jason D.~Lee and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets v.s. their induced kernel.
\newblock {\em arXiv preprint arXiv:1810.05369}, 2018.

\bibitem{cybenko1989approximation}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of Control, Signals, and Systems (MCSS)},
  2(4):303--314, 1989.

\bibitem{dyer2018asymptotics}
Ethan Dyer and Guy Gur-Ari.
\newblock Asymptotics of wide networks from feynman diagrams.
\newblock In {\em ICML Workshop on Physics for Deep Learning}, 2018.

\bibitem{hanin2018neural}
Boris Hanin.
\newblock Which neural net architectures give rise to exploding and vanishing
  gradients?
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{hanin2018products}
Boris Hanin and Mihai Nica.
\newblock Products of many large random matrices and gradients in deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1812.05994}, 2018.

\bibitem{hanin2018start}
Boris Hanin and David Rolnick.
\newblock How to start training: The effect of initialization and architecture.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  571--581, 2018.

\bibitem{hanin2019deep}
Boris Hanin and David Rolnick.
\newblock Deep relu networks have surprisingly few activation patterns.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{hornik1989multilayer}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock {\em Neural networks}, 2(5):359--366, 1989.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel~S Schoenholz, Yasaman Bahri, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock {\em arXiv preprint arXiv:1902.06720}, 2019.

\bibitem{li2019towards}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock {\em arXiv preprint arXiv:1907.04595}, 2019.

\bibitem{mei2019mean}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock {\em arXiv preprint arXiv:1902.06015}, 2019.

\bibitem{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layers neural networks.
\newblock {\em arXiv preprint arXiv:1804.06561}, 2018.

\bibitem{rotskoff2018parameters}
Grant Rotskoff and Eric Vanden-Eijnden.
\newblock Parameters as interacting particles: long time convergence and
  asymptotic error scaling of neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  7146--7155, 2018.

\bibitem{rotskoff2018neural}
Grant~M Rotskoff and Eric Vanden-Eijnden.
\newblock Neural networks as interacting particle systems: Asymptotic convexity
  of the loss landscape and universal scaling of the approximation error.
\newblock {\em arXiv preprint arXiv:1805.00915}, 2018.

\bibitem{spigler2018jamming}
Stefano Spigler, Mario Geiger, St{\'e}phane d'Ascoli, Levent Sagun, Giulio
  Biroli, and Matthieu Wyart.
\newblock A jamming transition from under-to over-parametrization affects loss
  landscape and generalization.
\newblock {\em arXiv preprint arXiv:1810.09665}, 2018.

\bibitem{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock {\em arXiv preprint arXiv:1902.04760}, 2019.

\end{thebibliography}
